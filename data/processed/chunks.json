[
  {
    "text": "S A N J I V R A N JA N DA S\nD ATA S C I E N C E :\nT H E O R I E S ,\nM O D E L S ,\nA L G O R I T H M S , A N D\nA N A LY T I C S\nS. R. DAS",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 1,
      "chunk_index": 0
    }
  },
  {
    "text": "Copyright©2013,2014,2016SanjivRanjanDas\npublished by s. r. das\nhttp://algo.scu.edu/ sanjivdas/\n∼\nLicensedundertheApacheLicense,Version2.0(the“License”);youmaynotusethisbookexceptincompliance\nwiththeLicense. YoumayobtainacopyoftheLicenseathttp://www.apache.org/licenses/LICENSE-2.0. Unless\nrequiredbyapplicablelaworagreedtoinwriting,softwaredistributedundertheLicenseisdistributedonan “as\nis” basis, without warranties or conditions of any kind,eitherexpressorimplied. SeetheLicenseforthe",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 2,
      "chunk_index": 0
    }
  },
  {
    "text": "specificlanguagegoverningpermissionsandlimitationsundertheLicense.\nThisprinting,July2016",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 2,
      "chunk_index": 1
    }
  },
  {
    "text": "THE FUTURE IS ALREADY HERE; IT’S JUST NOT VERY EVENLY DISTRIBUTED.\n– WILLIAM GIBSON\nTHE PUBLIC IS MORE FAMILIAR WITH BAD DESIGN THAN GOOD DESIGN. IT IS, IN\nEFFECT, CONDITIONED TO PREFER BAD DESIGN, BECAUSE THAT IS WHAT IT LIVES\nWITH. THE NEW BECOMES THREATENING, THE OLD REASSURING.\n– PAUL RAND\nIT SEEMS THAT PERFECTION IS ATTAINED NOT WHEN THERE IS NOTHING LEFT TO\nADD, BUT WHEN THERE IS NOTHING MORE TO REMOVE.\n– ANTOINE DE SAINT-EXUPÉRY\n. . . IN GOD WE TRUST, ALL OTHERS BRING DATA.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 3,
      "chunk_index": 0
    }
  },
  {
    "text": ". . . IN GOD WE TRUST, ALL OTHERS BRING DATA.\n– WILLIAM EDWARDS DEMING",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 3,
      "chunk_index": 1
    }
  },
  {
    "text": "Acknowledgements: I am extremely grateful to the following friends, stu-\ndents, and readers (mutually non-exclusive) who offered me feedback\non these chapters. I am most grateful to John Heineke for his constant\nfeedback and continuous encouragement. All the following students\nmade helpful suggestions on the manuscript: Sumit Agarwal, Kevin\nAguilar, Sankalp Bansal, Sivan Bershan, Ali Burney, Monalisa Chati, Jian-\nWei Cheng, Chris Gadek, Karl Hennig, Pochang Hsu, Justin Ishikawa,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 4,
      "chunk_index": 0
    }
  },
  {
    "text": "Ravi Jagannathan, Alice Yehjin Jun, Seoyoung Kim, Ram Kumar, Fed-\nerico Morales, Antonio Piccolboni, Shaharyar Shaikh, Jean-Marc Soumet,\nRakesh Sountharajan, Greg Tseng, Dan Wong, Jeffrey Woo.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 4,
      "chunk_index": 1
    }
  },
  {
    "text": "Contents\n25\n1 The Art of Data Science\n27\n1.1 Volume, Velocity, Variety\n29\n1.2 Machine Learning\n30\n1.3 Supervised and Unsupervised Learning\n30\n1.4 Predictions and Forecasts\n31\n1.5 Innovation and Experimentation\n31\n1.6 The Dark Side\n1.6.1 Big Errors 31\n1.6.2 Privacy 32\n37\n1.7 Theories, Models, Intuition, Causality, Prediction, Correlation\n41\n2 The Very Beginning: Got Math?\n41\n2.1 Exponentials, Logarithms, and Compounding\n43\n2.2 Normal Distribution\n43\n2.3 Poisson Distribution\n44",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 5,
      "chunk_index": 0
    }
  },
  {
    "text": "43\n2.3 Poisson Distribution\n44\n2.4 Moments of a continuous random variable\n45\n2.5 Combining random variables\n45\n2.6 Vector Algebra\n48\n2.7 Statistical Regression\n49\n2.8 Diversification\n50\n2.9 Matrix Calculus\n52\n2.10Matrix Equations",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 5,
      "chunk_index": 1
    }
  },
  {
    "text": "6\n55\n3 Open Source: Modeling in R\n55\n3.1 System Commands\n56\n3.2 Loading Data\n58\n3.3 Matrices\n59\n3.4 Descriptive Statistics\n61\n3.5 Higher-Order Moments\n61\n3.6 Quick Introduction to Brownian Motions with R\n62\n3.7 Estimation using maximum-likelihood\n64\n3.8 GARCH/ARCH Models\n66\n3.9 Introduction to Monte Carlo\n71\n3.10Portfolio Computations in R\n72\n3.11Finding the Optimal Portfolio\n75\n3.12Root Solving\n77\n3.13Regression\n81\n3.14Heteroskedasticity\n83\n3.15Auto-regressive models\n86",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 6,
      "chunk_index": 0
    }
  },
  {
    "text": "83\n3.15Auto-regressive models\n86\n3.16Vector Auto-Regression\n90\n3.17Logit\n94\n3.18Probit\n95\n3.19Solving Non-Linear Equations\n97\n3.20Web-Enabling R Functions\n103\n4 MoRe: Data Handling and Other Useful Things\n103\n4.1 Data Extraction of stocks using quantmod\n109\n4.2 Using the merge function\n114\n4.3 Using the apply class of functions\n114\n4.4 Getting interest rate data from FRED\n117\n4.5 Cross-Sectional Data (an example)\n121\n4.6 Handling dates with lubridate\n124\n4.7 Using the data.table package\n128",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 6,
      "chunk_index": 1
    }
  },
  {
    "text": "124\n4.7 Using the data.table package\n128\n4.8 Another data set: Bay Area Bike Share data\n130\n4.9 Using the plyr package family",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 6,
      "chunk_index": 2
    }
  },
  {
    "text": "7\n135\n5 Being Mean with Variance: Markowitz Optimization\n135\n5.1 Quadratic (Markowitz) Problem\n5.1.1 Solution in R 137\n138\n5.2 Solving the problem with the quadprog package\n140\n5.3 Tracing out the Efficient Frontier\n141\n5.4 Covariances of frontier portfolios: r ,r\np q\n142\n5.5 Combinations\n143\n5.6 Zero Covariance Portfolio\n143\n5.7 Portfolio Problems with Riskless Assets\n145\n5.8 Risk Budgeting\n149\n6 Learning from Experience: Bayes Theorem\n149\n6.1 Introduction\n151",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 7,
      "chunk_index": 0
    }
  },
  {
    "text": "149\n6.1 Introduction\n151\n6.2 Bayes and Joint Probability Distributions\n152\n6.3 Correlated default (conditional default)\n153\n6.4 Continuous and More Formal Exposition\n156\n6.5 Bayes Nets\n159\n6.6 Bayes Rule in Marketing\n162\n6.7 Other Applications\n6.7.1 Bayes Models in Credit Rating Transitions 162\n6.7.2 Accounting Fraud 162\n6.7.3 Bayes was a Reverend after all... 162\n165\n7 More than Words: Extracting Information from News\n165\n7.1 Prologue\n167\n7.2 Framework\n169\n7.3 Algorithms",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 7,
      "chunk_index": 1
    }
  },
  {
    "text": "7.1 Prologue\n167\n7.2 Framework\n169\n7.3 Algorithms\n7.3.1 Crawlers and Scrapers 169\n7.3.2 Text Pre-processing 172\n7.3.3 The tm package 175\n7.3.4 Term Frequency - Inverse Document Frequency (TF-IDF) 178\n7.3.5 Wordclouds 180\n7.3.6 Regular Expressions 181",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 7,
      "chunk_index": 2
    }
  },
  {
    "text": "8\n184\n7.4 Extracting Data from Web Sources using APIs\n7.4.1 Using Twitter 184\n7.4.2 Using Facebook 187\n7.4.3 Text processing, plain and simple 190\n7.4.4 A Multipurpose Function to Extract Text 191\n193\n7.5 Text Classification\n7.5.1 Bayes Classifier 193\n7.5.2 Support Vector Machines 198\n7.5.3 Word Count Classifiers 200\n7.5.4 Vector Distance Classifier 201\n7.5.5 Discriminant-Based Classifier 202\n7.5.6 Adjective-Adverb Classifier 204\n7.5.7 Scoring Optimism and Pessimism 205",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 8,
      "chunk_index": 0
    }
  },
  {
    "text": "7.5.7 Scoring Optimism and Pessimism 205\n7.5.8 Voting among Classifiers 206\n7.5.9 Ambiguity Filters 206\n207\n7.6 Metrics\n7.6.1 Confusion Matrix 207\n7.6.2 Precision and Recall 208\n7.6.3 Accuracy 209\n7.6.4 False Positives 209\n7.6.5 Sentiment Error 210\n7.6.6 Disagreement 210\n7.6.7 Correlations 210\n7.6.8 Aggregation Performance 211\n7.6.9 Phase-Lag Metrics 213\n7.6.10 Economic Significance 215\n215\n7.7 Grading Text\n216\n7.8 Text Summarization\n219\n7.9 Discussion\n221",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 8,
      "chunk_index": 1
    }
  },
  {
    "text": "216\n7.8 Text Summarization\n219\n7.9 Discussion\n221\n7.10Appendix: Sample text from Bloomberg for summarization",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 8,
      "chunk_index": 2
    }
  },
  {
    "text": "9\n227\n8 Virulent Products: The Bass Model\n227\n8.1 Introduction\n227\n8.2 Historical Examples\n228\n8.3 The Basic Idea\n229\n8.4 Solving the Model\n8.4.1 Symbolic math in R 231\n233\n8.5 Software\n234\n8.6 Calibration\n236\n8.7 Sales Peak\n238\n8.8 Notes\n241\n9 Extracting Dimensions: Discriminant and Factor Analysis\n241\n9.1 Overview\n241\n9.2 Discriminant Analysis\n9.2.1 Notation and assumptions 242\n9.2.2 Discriminant Function 242\n9.2.3 How good is the discriminant function? 243\n9.2.4 Caveats 244",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 9,
      "chunk_index": 0
    }
  },
  {
    "text": "9.2.4 Caveats 244\n9.2.5 Implementation using R 244\n9.2.6 Confusion Matrix 248\n9.2.7 Multiple groups 249\n250\n9.3 Eigen Systems\n252\n9.4 Factor Analysis\n9.4.1 Notation 252\n9.4.2 The Idea 253\n9.4.3 Principal Components Analysis (PCA) 253\n9.4.4 Application to Treasury Yield Curves 257\n9.4.5 Application: Risk Parity and Risk Disparity 260\n9.4.6 Difference between PCA and FA 260\n9.4.7 Factor Rotation 260\n9.4.8 Using the factor analysis function 261",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 9,
      "chunk_index": 1
    }
  },
  {
    "text": "10\n265\n10 Bidding it Up: Auctions\n265\n10.1Theory\n10.1.1 Overview 265\n10.1.2 Auction types 266\n10.1.3 Value Determination 266\n10.1.4 Bidder Types 267\n10.1.5 Benchmark Model (BM) 267\n268\n10.2Auction Math\n10.2.1 Optimization by bidders 269\n10.2.2 Example 270\n272\n10.3Treasury Auctions\n10.3.1 DPA or UPA? 272\n274\n10.4Mechanism Design\n10.4.1 Collusion 275\n10.4.2 Clicks (Advertising Auctions) 276\n10.4.3 Next Price Auctions 278\n10.4.4 Laddered Auction 279\n283",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 10,
      "chunk_index": 0
    }
  },
  {
    "text": "10.4.4 Laddered Auction 279\n283\n11 Truncate and Estimate: Limited Dependent Variables\n283\n11.1Introduction\n284\n11.2Logit\n287\n11.3Probit\n288\n11.4Analysis\n11.4.1 Slopes 288\n11.4.2 Maximum-Likelihood Estimation (MLE) 292\n293\n11.5Multinomial Logit\n297\n11.6Truncated Variables\n11.6.1 Endogeneity 299\n11.6.2 Example: Women in the Labor Market 301\n11.6.3 Endogeity – Some Theory to Wrap Up 303",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 10,
      "chunk_index": 1
    }
  },
  {
    "text": "11\n305\n12 Riding the Wave: Fourier Analysis\n305\n12.1Introduction\n305\n12.2Fourier Series\n12.2.1 Basic stuff 305\n12.2.2 The unit circle 305\n12.2.3 Angular velocity 306\n12.2.4 Fourier series 307\n12.2.5 Radians 307\n12.2.6 Solving for the coefficients 308\n309\n12.3Complex Algebra\n12.3.1 From Trig to Complex 310\n12.3.2 Getting rid of a 311\n0\n12.3.3 Collapsing and Simplifying 311\n312\n12.4Fourier Transform\n12.4.1 Empirical Example 314\n315\n12.5Application to Binomial Option Pricing\n316",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 11,
      "chunk_index": 0
    }
  },
  {
    "text": "12.5Application to Binomial Option Pricing\n316\n12.6Application to probability functions\n12.6.1 Characteristic functions 316\n12.6.2 Finance application 316\n12.6.3 Solving for the characteristic function 317\n12.6.4 Computing the moments 318\n12.6.5 Probability density function 318\n321\n13 Making Connections: Network Theory\n321\n13.1Overview\n322\n13.2Graph Theory\n323\n13.3Features of Graphs\n325\n13.4Searching Graphs\n13.4.1 Depth First Search 325\n13.4.2 Breadth-first-search 329",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 11,
      "chunk_index": 1
    }
  },
  {
    "text": "12\n331\n13.5Strongly Connected Components\n333\n13.6Dijkstra’s Shortest Path Algorithm\n13.6.1 Plotting the network 337\n338\n13.7Degree Distribution\n340\n13.8Diameter\n341\n13.9Fragility\n341\n13.10Centrality\n346\n13.11Communities\n13.11.1 Modularity 348\n354\n13.12Word of Mouth\n355\n13.13Network Models of Systemic Risk\n13.13.1 Systemic Score, Fragility, Centrality, Diameter 355\n13.13.2 Risk Decomposition 359\n13.13.3 Normalized Risk Score 360\n13.13.4 Risk Increments 361\n13.13.5 Criticality 362",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 12,
      "chunk_index": 0
    }
  },
  {
    "text": "13.13.5 Criticality 362\n13.13.6 Cross Risk 364\n13.13.7 Risk Scaling 365\n13.13.8 Too Big To Fail? 367\n13.13.9 Application of the model to the banking network in India 369\n371\n13.14Map of Science\n377\n14 Statistical Brains: Neural Networks\n377\n14.1Overview\n378\n14.2Nonlinear Regression\n379\n14.3Perceptrons\n381\n14.4Squashing Functions\n381\n14.5How does the NN work?\n14.5.1 Logit/Probit Model 382\n14.5.2 Connection to hyperplanes 382\n382\n14.6Feedback/Backpropagation",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 12,
      "chunk_index": 1
    }
  },
  {
    "text": "382\n14.6Feedback/Backpropagation\n14.6.1 Extension to many perceptrons 384",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 12,
      "chunk_index": 2
    }
  },
  {
    "text": "13\n384\n14.7Research Applications\n14.7.1 Discovering Black-Scholes 384\n14.7.2 Forecasting 384\n384\n14.8Package neuralnet in R\n390\n14.9Package nnet in R\n393\n15 Zero or One: Optimal Digital Portfolios\n394\n15.1Modeling Digital Portfolios\n398\n15.2Implementation in R\n15.2.1 Basic recursion 398\n15.2.2 Combining conditional distributions 401\n404\n15.3Stochastic Dominance (SD)\n407\n15.4Portfolio Characteristics\n15.4.1 How many assets? 407\n15.4.2 The impact of correlation 409\n15.4.3 Uneven bets? 410",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 13,
      "chunk_index": 0
    }
  },
  {
    "text": "15.4.3 Uneven bets? 410\n15.4.4 Mixing safe and risky assets 411\n412\n15.5Conclusions\n415\n16 Against the Odds: Mathematics of Gambling\n415\n16.1Introduction\n16.1.1 Odds 415\n16.1.2 Edge 415\n16.1.3 Bookmakers 416\n416\n16.2Kelly Criterion\n16.2.1 Example 416\n16.2.2 Deriving the Kelly Criterion 418\n421\n16.3Entropy\n16.3.1 Linking the Kelly Criterion to Entropy 421\n16.3.2 Linking the Kelly criterion to portfolio optimization 422\n16.3.3 Implementing day trading 422",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 13,
      "chunk_index": 1
    }
  },
  {
    "text": "14\n423\n16.4Casino Games\n427\n17 In the Same Boat: Cluster Analysis and Prediction Trees\n427\n17.1Introduction\n427\n17.2Clustering using k-means\n17.2.1 Example: Randomly generated data in kmeans 430\n17.2.2 Example: Clustering of VC financing rounds 432\n17.2.3 NCAA teams 434\n436\n17.3Hierarchical Clustering\n436\n17.4Prediction Trees\n17.4.1 Classification Trees 440\n17.4.2 The C4.5 Classifier 442\n445\n17.5Regression Trees\n17.5.1 Example: Califonia Home Data 447\n451\n18 Bibliography",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 14,
      "chunk_index": 0
    }
  },
  {
    "text": "List of Figures\n11 27\n. TheFourVsofBigData.\n12\n. GoogleFluTrends. Thefigureshowsthehighcorrelationbetweenfluincidenceandsearches\nabout“flu”onGoogle. TheorangelineisactualUSfluactivity,andthebluelineisthe\n28\nGoogleFluTrendsestimate.\n13 33\n. Profilingcanconvertmassmediaintopersonalmedia.\n14 34\n. Ifit’sfree,youmaybetheproduct.\n15 36\n. Extractingconsumer’ssurplusthroughprofiling.\n31 67\n. SinglestockpathplotsimulatedfromaBrownianmotion.\n32 68\n. MultiplestockpathplotsimulatedfromaBrownianmotion.\n33 73",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 15,
      "chunk_index": 0
    }
  },
  {
    "text": "33 73\n. Systematicriskasthenumberofstocksintheportfolioincreases.\n34 98\n. HTMLcodefortheRcgiapplication.\n35 101\n. RcodefortheRcgiapplication.\n41 105\n. Plotsofthesixstockseriesextractedfromtheweb.\n42 108\n. Plotsofthecorrelationmatrixofsixstockseriesextractedfromtheweb.\n43 109\n. Regressionofstockaveragereturnsagainstsystematicrisk(β).\n44\n. GoogleFinance: theAAPLwebpageshowingtheURLwhichisneededtodownload\n113\nthepage.\n45 122\n. Failedbanktotalsbyyear.\n46 126\n. Rapetotalsbyyear.\n47 129",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 15,
      "chunk_index": 1
    }
  },
  {
    "text": "46 126\n. Rapetotalsbyyear.\n47 129\n. Rapetotalsbycounty.\n51 141\n. TheEfficientFrontier\n61\n. Bayesnetshowingthepathwaysofeconomicdistress. Therearethreechannels: aisthe\ninducementofindustrydistressfromeconomydistress;bistheinducementoffirmdis-\ntressdirectlyfromeconomydistress;cistheinducementoffirmdistressdirectlyfrom\n157\nindustrydistress.\n62 163\n. Article from the Scientific American on Bayes’ Theorem.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 15,
      "chunk_index": 2
    }
  },
  {
    "text": "16\n71\n. Thedataandalgorithmspyramids. Depictstheinverserelationshipbetweendatavol-\n169\numeandalgorithmiccomplexity.\n72\n. Quantityofhourlypostingsonmessageboardsafterselectednewsreleases. Source:\nDas,Martinez-JerezandTufano(2005). 171\n73\n. Subjectiveevaluationofcontentofpost-newsreleasepostingsonmessageboards. The\ncontentisdividedintoopinions,facts,andquestions. Source: Das,Martinez-Jerezand\nTufano(2005). 172\n74 173\n. Frequencyofpostingbymessageboardparticipants.\n75 173",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 16,
      "chunk_index": 0
    }
  },
  {
    "text": "75 173\n. Frequencyofpostingbydayofweekbymessageboardparticipants.\n76\n. Frequencyofpostingbysegmentofdaybymessageboardparticipants. Weshowthe\naveragenumberofmessagesperdayinthetoppanelandtheaveragenumberofchar-\n174\nacterspermessageinthebottompanel.\n77\n. Exampleofapplicationofwordcloudtothebiodataextractedfromthewebandstored\n181\ninaCorpus.\n78\n. Plotofstockseries(uppergraph)versussentimentseries(lowergraph). Thecorrela-\ntionbetweentheseriesishigh. TheplotisbasedonmessagesfromYahoo! Financeand\n211",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 16,
      "chunk_index": 1
    }
  },
  {
    "text": "211\nisforasingletwenty-fourhourperiod.\n79\n. Phase-laganalysis. Theleft-sideshowstheeightcanonicalgraphpatternsthatarede-\nrivedfromarrangementsofthestart,end,high,andlowpointsofatimeseries. Theright-\nsideshowstheleadsandlagsofpatternsofthestockseriesversusthesentimentseries.\n214\nApositivevaluemeansthatthestockseriesleadsthesentimentseries.\n81 228\n. Actual versus Bass model predictions for VCRs.\n82 229\n. Actual versus Bass model predictions for answering machines.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 16,
      "chunk_index": 2
    }
  },
  {
    "text": "8 . 3 Example of the adoption rate: m = 100,000, p = 0.01 and q = 0.2. 231\n8 . 4 Example of the adoption rate: m = 100,000, p = 0.01 and q = 0.2. 232\n85 234\n. ComputingtheBassmodelintegralusingWolframAlpha.\n86\n. Bass model forecast of Apple Inc’s quarterly sales. The current sales are\n237\nalso overlaid in the plot.\n87 237\n. Empirical adoption rates and parameters from the Bass paper.\n8 . 8 Increase in peak time with q 240\n↑\n101 271\n. ProbabilitydensityfunctionfortheBeta(a=2,b=4)distribution.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 16,
      "chunk_index": 3
    }
  },
  {
    "text": "102 273\n. RevenueintheDPAandUPAauctions.\n103 274\n. Treasuryauctionmarkups.\n104 275\n. Bid-Ask Spread in the Auction.\n131\n. Comparisonofrandomandscale-freegraphs. FromBarabasi,Albert-Laszlo.,andEric\nBonabeau(2003). “Scale-FreeNetworks,”ScientificAmericanMay,50–59. 323",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 16,
      "chunk_index": 4
    }
  },
  {
    "text": "17\n132\n. Microsoftacademicsearchtoolforco-authorshipnetworks. See: http://academic.research.microsoft.com/.\nThetopchartshowsco-authors,themiddleoneshowscitations,andthelastoneshows\nmyErdosnumber,i.e.,thenumberofhopsneededtobeconnectedtoPaulErdosvia\nmyco-authors. MyErdosnumberis3. Interestingly,IamaFinanceacademic,butmy\nshortestpathtoErdosisthroughComputerScienceco-authors,anotherfieldinwhich\n326\nIdabble.\n133 327\n. Depth-first-search.\n134 329",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 17,
      "chunk_index": 0
    }
  },
  {
    "text": "Idabble.\n133 327\n. Depth-first-search.\n134 329\n. Depth-firstsearchonasimplegraphgeneratedfromapairednodelist.\n135 330\n. Breadth-first-search.\n136\n. Stronglyconnectedcomponents. Theuppergraphshowstheoriginalnetworkandthe\nloweroneshowsthecompressednetworkcomprisingonlytheSCCs. Thealgorithmto\ndetermineSCCsreliesontwoDFSs. CanyouseeafurtherSCCinthesecondgraph?\n332\nThereshouldnotbeone.\n137 334\n. Findingconnectedcomponentsonagraph.\n138 335\n. Dijkstra’salgorithm.\n139 336",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 17,
      "chunk_index": 1
    }
  },
  {
    "text": "138 335\n. Dijkstra’salgorithm.\n139 336\n. Networkforcomputationofshortestpathalgorithm\n1310 338\n. PlotusingtheFruchterman-RheingoldandCirclelayouts\n1311 339\n. PlotoftheErdos-Renyirandomgraph\n1312 340\n. PlotofthedegreedistributionoftheErdos-Renyirandomgraph\n13 . 13 Interbanklendingnetworksbyyear. Thetoppanelshows2005,andthebottompanel\nisfortheyears2006-2009. 345\n1314 354\n. Communityversuscentrality\n1315 357\n. Bankingnetworkadjacencymatrixandplot\n13 . 16 Centralityforthe15banks. 359",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 17,
      "chunk_index": 2
    }
  },
  {
    "text": "13 . 16 Centralityforthe15banks. 359\n13 . 17 RiskDecompositionsforthe15banks. 361\n13 . 18 RiskIncrementsforthe15banks. 363\n13 . 19 Criticalityforthe15banks. 364\n1320 366\n. Spillovereffects.\n1321 368\n. Howriskincreaseswithconnectivityofthenetwork.\n1322 370\n. Howriskincreaseswithconnectivityofthenetwork.\n1323 372\n. ScreensforselectingtherelevantsetofIndianFIstoconstructthebankingnetwork.\n1324\n. ScreensfortheIndianFIsbankingnetwork. Theupperplotshowstheentirenetwork.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 17,
      "chunk_index": 3
    }
  },
  {
    "text": "Thelowerplotshowsthenetworkwhenwemouseoverthebankinthemiddleofthe\nplot. Redlinesshowthatthebankisimpactedbytheotherbanks,andbluelinesde-\n373\npictthatthebankimpactstheothers,inaGrangercausalmanner.\n1325\n. ScreensforsystemicriskmetricsoftheIndianFIsbankingnetwork. Thetopplotshows\nthecurrentriskmetrics,andthebottomplotshowsthehistoryfrom2008. 374\n1326 375\n. TheMapofScience.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 17,
      "chunk_index": 4
    }
  },
  {
    "text": "18\n141 380\n. Afeed-forwardmultilayerneuralnetwork.\n142 387\n. Theneuralnetfortheinfertdatasetwithtwoperceptronsinasinglehiddenlayer.\n151\n. Plotofthefinaloutcomedistributionforadigitalportfoliowithfiveassetsofoutcomes\n400\n5,8,4,2,1 allofequalprobability.\n{ }\n152\n. Plotofthefinaloutcomedistributionforadigitalportfoliowithfiveassetsofoutcomes\n5,8,4,2,1 withunconditionalprobabilityofsuccessof 0.1,0.2,0.1,0.05,0.15 ,respecitvely.\n{ } { }\n403\n153",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 18,
      "chunk_index": 0
    }
  },
  {
    "text": "{ } { }\n403\n153\n. Plotofthedifferenceindistributionforadigitalportfoliowithfiveassetswhenρ =\n0.75minusthatwhenρ=0.25. Weuseoutcomes 5,8,4,2,1 withunconditionalprob-\n{ }\n405\nabilityofsuccessof 0.1,0.2,0.1,0.05,0.15 ,respecitvely.\n{ }\n154\n. DistributionfunctionsforreturnsfromBernoulliinvestmentsasthenumberofinvest-\nments(n)increases. Usingtherecursiontechniquewecomputedtheprobabilitydis-\ntributionoftheportfoliopayoffforfourvaluesofn= 25,50,75,100 . Thedistribu-\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 18,
      "chunk_index": 1
    }
  },
  {
    "text": "{ }\ntionfunctionisplottedintheleftpanel. Thereare4plots,oneforeachn,andifwelook\natthebottomleftoftheplot,theleftmostlineisforn=100. Thenextlinetotheright\nisforn = 75,andsoon. Therightpanelplotsthevalueof\n(cid:82)u[G\n(x) G (x)] dx\n0 100 − 25\nforallu (0,1),andconfirmsthatitisalwaysnegative. Thecorrelationparameter\n∈\n408\nisρ=0.25.\n155\n. DistributionfunctionsforreturnsfromBernoulliinvestmentsasthecorrelationparam-\neter(ρ2)increases. Usingtherecursiontechniquewecomputedtheprobabilitydistri-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 18,
      "chunk_index": 2
    }
  },
  {
    "text": "butionoftheportfoliopayoffforfourvaluesofρ = 0.09,0.25,0.49,0.81 shownby\n{ }\ntheblack,red,greenandbluelinesrespectively. Thedistributionfunctionisplottedin\ntheleftpanel. Therightpanelplotsthevalueof (cid:82) 0 u[G ρ=0.09 (x) − G ρ=0.81 (x)]dxforall\n410\nu (0,1),andconfirmsthatitisalwaysnegative.\n∈\n161\n. BankrollevolutionundertheKellyrule. ThetopplotfollowstheKellycriterion,but\ntheothertwodeviatefromit,byoverbettingorunderbettingthefractiongivenbyKelly.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 18,
      "chunk_index": 3
    }
  },
  {
    "text": "Thevariablesare: oddsare4to1,implyingahouseprobabilityof p=0.2,ownprob-\n419\nabilityofwinningis p ∗ =0.25.\n162\n. Seehttp://wizardofodds.com/gambling/house-edge/. TheHouseEdgeforvariousgames.\nTheedgeisthesameas f inournotation. Thestandarddeviationisthatofthebankroll\n−\nof$1foronebet. 424\n171 430\n. VCStyleClusters.\n172 432\n. Twoclusterexample.\n173 433\n. Fiveclusterexample.\n174 435\n. NCAAclusterexample.\n175 437\n. NCAAdata,hierarchicalclusterexample.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 18,
      "chunk_index": 4
    }
  },
  {
    "text": "19\n176\n. NCAAdata,hierarchicalclusterexamplewithclustersonthetoptwoprincipalcom-\n438\nponents.\n177 443\n. Classificationtreeforthekyphosisdataset.\n178 448\n. Predictiontreeforcarsmileage.\n179 448\n. Californiahomepricespredictiontree.\n1710 449\n. Californiahomepricespartitiondiagram.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 19,
      "chunk_index": 0
    }
  },
  {
    "text": "List of Tables\n31\n. Autocorrelationindaily,weekly,andmonthlystockindexreturns. FromLo-Mackinlay,\n87\n“ANon-RandomWalkDownWallStreet”.\n32\n. CrossautocorrelationsinUSstocks. FromLo-Macklinlay,“ANon-RandomWalkDown\n91\nWallStreet.”\n7 . 1 CorrelationsofSentimentandStockReturnsfortheMSH35stocksandtheaggregated\nMSH35index. Stockreturns(STKRET)arecomputedfromclose-to-close. Wecompute\ncorrelationsusingdatafor88daysinthemonthsofJune,JulyandAugust2001. Re-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 21,
      "chunk_index": 0
    }
  },
  {
    "text": "turndataovertheweekendislinearlyinterpolated,asmessagescontinuetobeposted\noverweekends. Dailysentimentiscomputedfrommidnighttocloseoftradingat4pm\n(SENTY4pm). 212\n13 . 1 Summarystatisticsandthetop25banksorderedoneigenvaluecentralityfor2005. The\nR-metricisameasureofwhetherfailurecanspreadquickly,andthisissowhenR\n≥\n2. Thediameterofthenetworkisthelengthofthelongestgeodesic. Alsopresentedin\nthesecondpanelofthetablearethecentralityscoresfor2005correspondingtoFigure\n13.13. 344\n151",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 21,
      "chunk_index": 1
    }
  },
  {
    "text": "13.13. 344\n151\n. ExpectedutilityforBernoulliportfoliosasthenumberofinvestments(n)increases. The\ntablereportstheportfoliostatisticsforn= 25,50,75,100 . Expectedutilityisgiven\n{ }\ninthelastcolumn. Thecorrelationparameterisρ=0.25. TheutilityfunctionisU(C)=\n(0.1+C)1\n−\nγ/(1 γ),γ=3. 409\n−\n152\n. ExpectedutilityforBernoulliportfoliosasthecorrelation(ρ)increases. Thetablere-\nportstheportfoliostatisticsforρ= 0.09,0.25,0.49,0.81 . Expectedutilityisgivenin\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 21,
      "chunk_index": 2
    }
  },
  {
    "text": "{ }\nthelastcolumn. TheutilityfunctionisU(C)=(0.1+C)1 − γ/(1 γ),γ=3. 411\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 21,
      "chunk_index": 3
    }
  },
  {
    "text": "22\n153\n. ExpectedutilityforBernoulliportfolioswhentheportfoliocomprisesbalancedinvest-\ninginassetsversusimbalancedweights. Boththebalancedandimbalancedportfolio\nhaven=25assetswithinthem,eachwithasuccessprobabilityofq =0.05. Thefirst\ni\nhasequalpayoffs,i.e. 1/25each. Thesecondportfoliohaspayoffsthatmonotonically\nincrease,i.e. thepayoffsareequalto j/325,j = 1,2,...,25. Wenotethatthesumof\nthepayoffsinbothcasesis1. Thecorrelationparameterisρ=0.55. Theutilityfunc-\ntionisU(C)=(0.1+C)1\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 22,
      "chunk_index": 0
    }
  },
  {
    "text": "tionisU(C)=(0.1+C)1\n−\nγ/(1 γ),γ=3. 411\n−\n154\n. ExpectedutilityforBernoulliportfolioswhentheportfoliocomprisesbalancedinvest-\ninginassetswithidenticalsuccessprobabilitiesversusinvestinginassetswithmixed\nsuccessprobabilities. Boththeuniformandmixedportfolioshaven=26assetswithin\nthem. Inthefirstportfolio,alltheassetshaveaprobabilityofsuccessequaltoq =0.10.\ni\nInthesecondportfolio,halfthefirmshaveasuccessprobabilityof0.05andtheother",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 22,
      "chunk_index": 1
    }
  },
  {
    "text": "halfhaveaprobabilityof0.15. Thepayoffofallinvestmentsis1/26. Thecorrelationpa-\nrameterisρ = 0.55. TheutilityfunctionisU(C) = (0.1+C)1 − γ/(1 γ),γ = 3.\n−\n412",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 22,
      "chunk_index": 2
    }
  },
  {
    "text": "23\nDedicated to Geetu, for decades of fun and friendship",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 23,
      "chunk_index": 0
    }
  },
  {
    "text": "1\nThe Art of Data Science\n— “All models are wrong, but some are useful.”\nGeorgeE.P.BoxandN.R.Draperin“EmpiricalModelBuildingand\nResponseSurfaces,”JohnWiley&Sons,NewYork,1987.\nSo you want to be a “data scientist”? There is no widely accepted\n1\ndefinition of who a data scientist is. Several books now attempt to 1Theterm“datascientist”wascoined\nbyD.J.Patil.HewastheChiefScientist\ndefine what data science is and who a data scientist may be, see Patil\nforLinkedIn.In2011Forbesplacedhim\n2011 2012 2012",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 25,
      "chunk_index": 0
    }
  },
  {
    "text": "forLinkedIn.In2011Forbesplacedhim\n2011 2012 2012\n( ), Patil ( ), and Loukides ( ). This book’s viewpoint is that secondintheirDataScientistList,just\nbehindLarryPageofGoogle.\na data scientist is someone who asks unique, interesting questions of\ndata based on formal or informal theory, to generate rigorous and useful\n2\ninsights. It is likely to be an individual with multi-disciplinary train- 2ToquoteGeorgCantor-“Inmathe-\nmaticstheartofproposingaquestion",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 25,
      "chunk_index": 1
    }
  },
  {
    "text": "maticstheartofproposingaquestion\ning in computer science, business, economics, statistics, and armed with\nmustbeheldofhighervaluethan\nthe necessary quantity of domain knowledge relevant to the question at solvingit.”\nhand. The potential of the field is enormous for just a few well-trained\ndata scientists armed with big data have the potential to transform orga-\nnizations and societies. In the narrower domain of business life, the role",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 25,
      "chunk_index": 2
    }
  },
  {
    "text": "of the data scientist is to generate applicable business intelligence.\nAmong all the new buzzwords in business – and there are many –\n“Big Data” is one of the most often heard. The burgeoning social web,\nand the growing role of the internet as the primary information channel\nof business, has generated more data than we might imagine. Users up-\nload an hour of video data to YouTube every second. 3 87 % of the U.S. 3Mayer-SchönbergerandCukier(2013),\n7 4 p8.TheyreportthatUSC’sMartin",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 25,
      "chunk_index": 3
    }
  },
  {
    "text": "7 4 p8.TheyreportthatUSC’sMartin\npopulation has heard of Twitter, and % use it. Forty-nine percent of\nHilbertcalculatedthatmorethan300\nTwitter users follow some brand or the other, hence the reach is enor- exabytesofdatastoragewasbeingused\n2014 500\nin2007,anexabytebeingonebillion\nmous, and, as of , there are more then million tweets a day. But gigabytes,i.e.,1018bytes,and260of\ndata is not information, and until we add analytics, it is just noise. And binaryusage.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 25,
      "chunk_index": 4
    }
  },
  {
    "text": "4Incontrast,88%ofthepopulationhas\nmore, bigger, data may mean more noise and does not mean better data. heardofFacebook,and41%useit.See\nIn many cases, less is more, and we need models as well. That is what www.convinceandconvert.com/\n7-surprising-statistics-about\nthis book is about, it’s about theories and models, with or without data, -twitter-in-america/.Halfof\nTwitterusersarewhite,andofthe\nremaininghalf,halfareblack.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 25,
      "chunk_index": 5
    }
  },
  {
    "text": "26 data science: theories, models, algorithms, and analytics\nbig or small. It’s about analytics and applications, and a scientific ap-\nproach to using data based on well-founded theory and sound business\njudgment. This book is about the science and art of data analytics.\nData science is transforming business. Companies are using medical\ndata and claims data to offer incentivized health programs to employ-\n65000\nees. Caesar’s Entertainment Corp. analyzed data for , employees",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 26,
      "chunk_index": 0
    }
  },
  {
    "text": "and found substantial cost savings. Zynga Inc, famous for its game Far-\n25\nmville, accumulates terabytes of data every day and analyzes it to\nmake choices about new game features. UPS installed sensors to collect\ndata on speed and location of its vans, which combined with GPS infor-\n2011 84\nmation, reduced fuel usage in by . million gallons, and shaved\n85 5\nmillion miles off its routes. McKinsey argues that a successful data 5“HowBigDataisChangingtheWhole\nEquationforBusiness,”WallStreet",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 26,
      "chunk_index": 1
    }
  },
  {
    "text": "EquationforBusiness,”WallStreet\nanalytics plan contains three elements: interlinked data inputs, analytics\nJournalMarch8,2013.\n6\nmodels, and decision-support tools. In a seminal paper, Halevy, Norvig 6“BigData:What’sYourPlan?”McKin-\n2009\nseyQuarterly,March2013.\nand Pereira ( ), argue that even simple theories and models, with big\ndata, have the potential to do better than complex models with less data.\n7",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 26,
      "chunk_index": 2
    }
  },
  {
    "text": "7\nIn a recent talk well-regarded data scientist Hilary Mason empha- 7Attheh2oworldconferenceintheBay\nArea,on11thNovember2015.\nsized that the creation of “data products” requires three components:\ndata (of course) plus technical expertise (machine-learning) plus people\nand process (talent). Google Maps is a great example of a data product\nthat epitomizes all these three qualities. She mentioned three skills that\ngood data scientists need to cultivate: (a) in math and stats, (b) coding,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 26,
      "chunk_index": 3
    }
  },
  {
    "text": "(c) communication. I would add that preceding all these is the ability to\nask relevant questions, the answers to which unlock value for compa-\nnies, consumers, and society. Everything in data analytics begins with a\nclear problem statement, and needs to be judged with clear metrics.\nBeing a data scientist is inherently interdisciplinary. Good questions\ncome from many disciplines, and the best answers are likely to come\nfrom people who are interested in multiple fields, or at least from teams",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 26,
      "chunk_index": 4
    }
  },
  {
    "text": "that co-mingle varied skill sets. Josh Wills of Cloudera stated it well -\n“A data scientist is a person who is better at statistics than any software\nengineer and better at software engineering than any statistician.” In\ncontrast, complementing data scientists are business analytics people,\nwho are more familiar with business models and paradigms and can ask\ngood questions of the data.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 26,
      "chunk_index": 5
    }
  },
  {
    "text": "the art of data science 27\n1.1 Volume, Velocity, Variety\nThere are several \"V\"s of big data: three of these are volume, velocity,\n8\nvariety. Big data exceeds the storage capacity of conventional databases. 8Thisnomenclaturewasoriginatedby\ntheGartnergroupin2001,andhasbeen\nThis is it’s volume aspect. The scale of data generation is mind-boggling.\ninplacemorethanadecade.\n2003\nGoogle’s Eric Schmidt pointed out that until , all of human kind had",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 27,
      "chunk_index": 0
    }
  },
  {
    "text": "generated just 5 exabytes of data (an exabyte is 10006 bytes or a billion-\n5\nbillion bytes). Today we generate exabytes of data every two days.\nThe main reason for this is the explosion of “interaction” data, a new\nphenomenon in contrast to mere “transaction” data. Interaction data\ncomes from recording activities in our day-to-day ever more digital lives,\nsuch as browser activity, geo-location data, RFID data, sensors, personal",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 27,
      "chunk_index": 1
    }
  },
  {
    "text": "digital recorders such as the fitbit and phones, satellites, etc. We now live\nin the “internet of things” (or iOT), and it’s producing a wild quantity of\ndata, all of which we seem to have an endless need to analyze. In some\n4 11\nquarters it is better to speak of Vs of big data, as shown in Figure . .\nFigure1.1: TheFourVsofBigData.\nA good data scientist will be adept at managing volume not just tech-\nnically in a database sense, but by building algorithms to make intelli-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 27,
      "chunk_index": 2
    }
  },
  {
    "text": "28 data science: theories, models, algorithms, and analytics\ngent use of the size of the data as efficiently as possible. Things change\nwhen you have gargantuan data because almost all correlations become\nsignificant, and one might be tempted to draw spurious conclusions\nabout causality. For many modern business applications today extraction\nof correlation is sufficient, but good data science involves techniques that\nextract causality from these correlations as well.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 28,
      "chunk_index": 0
    }
  },
  {
    "text": "In many cases, detecting correlations is useful as is. For example, con-\n12\nsider the classic case of Google Flu Trends, see Figure . . The figure\nshows the high correlation between flu incidence and searches about\n2009\n“flu” on Google, see Ginsberg et. al. ( ). Obviously searches on the\nkey word “flu” do not result in the flu itself! Of course, the incidence of\nsearches on this key word is influenced by flu outbreaks. The interesting",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 28,
      "chunk_index": 1
    }
  },
  {
    "text": "point here is that even though searches about flu do not cause flu, they\ncorrelate with it, and may at times even be predictive of it, simply because\nsearches lead the actual reported levels of flu, as those may occur concur-\nrently but take time to be reported. And whereas searches may be pre-\ndictive, the cause of searches is the flu itself, one variable feeding on the\n9\nother, in a repeat cycle. Hence, prediction is a major outcome of corre- 9Interwoventimeseriessuchasthese",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 28,
      "chunk_index": 2
    }
  },
  {
    "text": "maybemodeledusingVectorAuto-\nlation, and has led to the recent buzz around the subfield of “predictive\nRegressions,atechniquewewillen-\nanalytics.” There are entire conventions devoted to this facet of corre- counterlaterinthisbook.\n10\nlation, such as the wildly popular PAW (Predictive Analytics World). 10Maybeafutilecollectionofpeople,\nwithnon-workingcrystalballs,as\nPattern recognition is in, passe causality is out.\nWilliamGibsonsaid-“Thefutureisnot\ngoogle-able.”\nFigure1.2: GoogleFluTrends. The",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 28,
      "chunk_index": 3
    }
  },
  {
    "text": "google-able.”\nFigure1.2: GoogleFluTrends. The\nfigureshowsthehighcorrelation\nbetweenfluincidenceandsearches\nabout“flu”onGoogle. Theorange\nlineisactualUSfluactivity,and\nthebluelineistheGoogleFlu\nTrendsestimate.\nData velocity is accelerating. Streams of tweets, Facebook entries, fi-\nnancial information, etc., are being generated by more users at an ever\nincreasing pace. Whereas velocity increases data volume, often exponen-\ntially, it might shorten the window of data retention or application. For",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 28,
      "chunk_index": 4
    }
  },
  {
    "text": "example, high-frequency trading relies on micro-second information and\nstreams of data, but the relevance of the data rapidly decays.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 28,
      "chunk_index": 5
    }
  },
  {
    "text": "the art of data science 29\nFinally, data variety is much greater than ever before. Models that\nrelied on just a handful of variables can now avail of hundreds of vari-\nables, as computing power has increased. The scale of change in volume,\nvelocity, and variety of the data that is now available calls for new econo-\nmetrics, and a range of tools for even single questions. This book aims to\nintroduce the reader to a variety of modeling concepts and econometric",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 29,
      "chunk_index": 0
    }
  },
  {
    "text": "techniques that are essential for a well-rounded data scientist.\nData science is more than the mere analysis of large data sets. It is\nalso about the creation of data. The field of “text-mining” expands avail-\nable data enormously, since there is so much more text being generated\nthan numbers. The creation of data from varied sources, and its quantifi-\ncation into information is known as “datafication.”\n1.2 Machine Learning\nData science is also more than “machine learning,” which is about how",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 29,
      "chunk_index": 1
    }
  },
  {
    "text": "systems learn from data. Systems may be trained on data to make deci-\nsions, and training is a continuous process, where the system updates its\nlearning and (hopefully) improves its decision-making ability with more\ndata. A spam filter is a good example of machine learning. As we feed\nit more data it keeps changing its decision rules, using a Bayesian filter,\nthereby remaining ahead of the spammers. It is this ability to adaptively",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 29,
      "chunk_index": 2
    }
  },
  {
    "text": "learn that prevents spammers from gaming the filter, as highlighted in\n11\nPaul Graham’s interesting essay titled “A Plan for Spam”. Credit card 11http://www.paulgraham.com/spam.html.\napprovals are also based on neural-nets, another popular machine learn-\ning technique. However, machine-learning techniques favor data over\njudgment, and good data science requires a healthy mix of both. Judg-\nment is needed to accurately contextualize the setting for analysis and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 29,
      "chunk_index": 3
    }
  },
  {
    "text": "to construct effective models. A case in point is Vinny Bruzzese, known\nas the “mad scientist of Hollywood” who uses machine learning to pre-\n12\ndict movie revenues. He asserts that mere machine learning would be 12“SolvingEquationofaHitFilm\nScript,WithData,”NewYorkTimes,May\ninsufficient to generate accurate predictions. He complements machine\n5,2013.\nlearning with judgment generated from interviews with screenwriters,\nsurveys, etc., “to hear and understand the creative vision, so our analysis",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 29,
      "chunk_index": 4
    }
  },
  {
    "text": "can be contextualized.”\nMachine intelligence is re-emerging as the new incarnation of AI (a\nfield that many feel has not lived up to its promise). Machine learning\npromises and has delivered on many questions of interest, and is also",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 29,
      "chunk_index": 5
    }
  },
  {
    "text": "30 data science: theories, models, algorithms, and analytics\nproving to be quite a game-changer, as we will see later on in this chap-\nter, and also as discussed in many preceding examples. What makes it\nso appealing? Hilary Mason suggests four characteristics of machine in-\ntelligence that make it interesting: (i) It is usually based on a theoretical\nbreakthrough and is therefore well grounded in science. (ii) It changes\nthe existing economic paradigm. (iii) The result is commoditization (e.g.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 30,
      "chunk_index": 0
    }
  },
  {
    "text": "Hadoop), and (iv) it makes available new data that leads to further data\nscience.\n1.3 Supervised and Unsupervised Learning\nSystems may learn in two broad ways, through “supervised” and “unsu-\npervised” learning. In supervised learning, a system produces decisions\n(outputs) based on input data. Both spam filters and automated credit\ncard approval systems are examples of this type of learning. So is lin-\near discriminant analysis (LDA). The system is given a historical data",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 30,
      "chunk_index": 1
    }
  },
  {
    "text": "sample of inputs and known outputs, and it “learns” the relationship\nbetween the two using machine learning techniques, of which there are\nseveral. Judgment is needed to decide which technique is most appropri-\nate for the task at hand.\nUnsupervised learning is a process of reorganizing and enhancing the\ninputs in order to place structure on unlabeled data. A good example is\ncluster analysis, which takes a collection of entities, each with a number",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 30,
      "chunk_index": 2
    }
  },
  {
    "text": "of attributes, and partitions the entity space into sets or groups based on\ncloseness of the attributes of all entities. What this does is reorganizes\nthe data, but it also enhances the data through a process of labeling the\ndata with additional tags (in this case a cluster number/name). Factor\nanalysis is also an unsupervised learning technique. The origin of this\nterminology is unclear, but it presumably arises from the fact that there",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 30,
      "chunk_index": 3
    }
  },
  {
    "text": "is no clear objective function that is maximized or minimized in unsu-\npervised learning, so that no “supervision” to reach an optimal is called\nfor. However, this is not necessarily true in general, and we will see ex-\namples of unsupervised learning (such as community detection in the\nsocial web) where the outcome depends on measurable objective criteria.\n1.4 Predictions and Forecasts\nData science is about making predictions and forecasts. There is a dif-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 30,
      "chunk_index": 4
    }
  },
  {
    "text": "ference between the two. The statistician-economist Paul Saffo has sug-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 30,
      "chunk_index": 5
    }
  },
  {
    "text": "the art of data science 31\ngested that predictions aim to identify one outcome, whereas forecasts\nencompass a range of outcomes. To say that “it will rain tomorrow” is\n40\nto make a prediction, but to say that “the chance of rain is %” (im-\n60\nplying that the chance of no rain is %) is to make a forecast, as it lays\nout the range of possible outcomes with probabilities. We make weather\nforecasts, not predictions. Predictions are statements of great certainty,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 31,
      "chunk_index": 0
    }
  },
  {
    "text": "whereas forecasts exemplify the range of uncertainty. In the context of\nthese definitions, the term predictive analytics is a misnomer for it’s goal\nis to make forecasts, not mere predictions.\n1.5 Innovation and Experimentation\nData science is about new ideas and approaches. It merges new concepts\nwith fresh algorithms. Take for example the A/B test, which is nothing\nbut the online implementation of a real-time focus group. Different sub-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 31,
      "chunk_index": 1
    }
  },
  {
    "text": "sets of users are exposed to A and B stimuli respectively, and responses\nare measured and analyzed. It is widely used for web site design. This\n2011\napproach has been in place for more than a decade, and in Google\n7000\nran more than , A/B tests. Facebook, Amazon, Netflix, and sev-\n13\neral others firms use A/B testing widely. The social web has become 13“TheA/BTest:InsidetheTechnology\nthat’sChangingtheRulesofBusiness,”\na teeming ecosystem for running social science experiments. The poten-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 31,
      "chunk_index": 2
    }
  },
  {
    "text": "byBrianChristian,Wired,April2012.\ntial to learn about human behavior using innovative methods is much\ngreater now than ever before.\n1.6 The Dark Side\n1.6.1 Big Errors\nThe good data scientist will take care to not over-reach in drawing con-\nclusions from big data. Because there are so many variables available,\nand plentiful observations, correlations are often statistically significant,\nbut devoid of basis. In the immortal words of the bard, empirical results",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 31,
      "chunk_index": 3
    }
  },
  {
    "text": "from big data may be - “A tale told by an idiot, full of sound and fury,\n14\nsignifying nothing.” One must be careful not to read too much in the 14WilliamShakespeareinMacbeth,Act\nV,SceneV.\ndata. More data does not guarantee less noise, and signal extraction may\nbe no easier than with less data.\nAdding more columns (variables in the cross section) to the data set,\nbut not more rows (time dimension) is also fraught with danger. As",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 31,
      "chunk_index": 4
    }
  },
  {
    "text": "the number of variables increases, more characteristics are likely to be",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 31,
      "chunk_index": 5
    }
  },
  {
    "text": "32 data science: theories, models, algorithms, and analytics\nrelated statistically. Over fitting models in-sample is much more likely\nwith big data, leading to poor performance out-of-sample.\nResearchers have also to be careful to explore the data fully, and not\nterminate their research the moment a viable result, especially one that\nthe researcher is looking for, is attained. With big data, the chances of\nstopping at a suboptimal, or worse, intuitively appealing albeit wrong",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 32,
      "chunk_index": 0
    }
  },
  {
    "text": "result become very high. It is like asking a question to a class of stu-\ndents. In a very large college class, the chance that someone will provide\na plausible yet off-base answer quickly is very high, which often short\ncircuits the opportunity for others in class to think more deeply about\nthe question and provide a much better answer.\n15\nNassim Taleb describes these issues elegantly - “I am not saying 15“BewaretheBigErrorsofBigData”\nWired,February2013.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 32,
      "chunk_index": 1
    }
  },
  {
    "text": "Wired,February2013.\nthere is no information in big data. There is plenty of information. The\nproblem – the central issue – is that the needle comes in an increasingly\nlarger haystack.” The fact is, one is not always looking for needles or\nTaleb’s black swans, and there are plenty of normal phenomena about\nwhich robust forecasts are made possible by the presence of big data.\n1.6.2 Privacy\nThe emergence of big data coincides with a gigantic erosion of privacy.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 32,
      "chunk_index": 2
    }
  },
  {
    "text": "Human kind has always been torn between the need for social interac-\ntion, and the urge for solitude and privacy. One trades off against the\nother. Technology has simply sharpened the divide and made the slope\nof this trade off steeper. It has provided tools of social interaction that\nsteal privacy much faster than in the days before the social web.\nRumors and gossip are now old world. They required bilateral trans-\nmission. The social web provides multilateral revelation, where privacy",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 32,
      "chunk_index": 3
    }
  },
  {
    "text": "no longer capitulates a battle at a time, but the entire war is lost at one\ngo. And data science is the tool that enables firms, governments, indi-\nviduals, benefactors and predators, et al, en masse, to feed on privacy’s\n13\ncarcass. The cartoon in Figure . parodies the kind of information spe-\ncialization that comes with the loss of privacy!\nThe loss of privacy is manifested in the practice of human profiling\nthrough data science. Our web presence increases entropically as we",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 32,
      "chunk_index": 4
    }
  },
  {
    "text": "move more of our life’s interactions to the web, be they financial, emo-\ntional, organizational, or merely social. And as we live more and more\nof our lives in this new social melieu, data mining and analytics enables\ncompanies to construct very accurate profiles of who we are, often better",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 32,
      "chunk_index": 5
    }
  },
  {
    "text": "the art of data science 33\nFigure1.3: Profilingcanconvert\nmassmediaintopersonalmedia.\nthan what we might do ourselves. We are moving from \"know thyself\" to\nknowing everything about almost everyone.\nIf you have a Facebook or Twitter presence, rest assured you have\nbeen profiled. For instance, let’s say you tweeted that you were taking\nyour dog for a walk. Profiling software now increments your profile\nwith an additional tag - pet owner. An hour later you tweet that you are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 33,
      "chunk_index": 0
    }
  },
  {
    "text": "returning home to cook dinner for your kids. You profile is now further\ntagged as a parent. As you might imagine, even a small Twitter presence\nends up being dramatically revealing about who you are. Information\nthat you provide on Facebook and Twitter, your credit card spending\npattern, and your blog, allows the creation of a profile that is accurate\nand comprehensive, and probably more objective than the subjective\nand biased opinion that you have of yourself. A machine knows thyself\n14",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 33,
      "chunk_index": 1
    }
  },
  {
    "text": "14\nbetter. And you are the product! (See Figure . .)\nHumankind leaves an incredible trail of “digital exhaust” comprising\nphone calls, emails, tweets, GPS information, etc., that companies use for\n1 3\nprofiling. It is said that / of people have a digital identity before being\nborn, initiated with the first sonogram from a routine hospital visit by\nan expectant mother. The half life of non-digital identity, or the average",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 33,
      "chunk_index": 2
    }
  },
  {
    "text": "34 data science: theories, models, algorithms, and analytics\nFigure1.4: Ifit’sfree,youmaybe\ntheproduct.\n92\nage of digital birth is six months, and within two years % of the US\n16\npopulation has a digital identity. Those of us who claim to be safe from 16See“TheHumanFaceofBigData”by\nRickSmolanandJenniferErwitt.\nrevealing their privacy by avoiding all forms of social media are simply\nprofiled as agents with a “low digital presence.” It might be interesting",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 34,
      "chunk_index": 0
    }
  },
  {
    "text": "to ask such people whether they would like to reside in a profile bucket\nthat is more likely to attract government interest than a profile bucket\nwith more average digital presence. In this age of profiling, the best\nway to remain inconspicuous is not to hide, but to remain as average as\npossible, so as to be mostly lost within a large herd.\nPrivacy is intricately and intrinsically connected to security and effi-\nciency. The increase in transacting on the web, and the confluence of pro-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 34,
      "chunk_index": 1
    }
  },
  {
    "text": "filing, has led to massive identity theft. Just as in the old days, when a\nthief picked your lock and entered your home, most of your possessions\nwere at risk. It is the same with electronic break ins, except that there are\nmany more doors to break in from and so many more windows through\nwhich an intruder can unearth revealing information. And unlike a thief\nwho breaks into your home, a hacker can reside in your electronic abode",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 34,
      "chunk_index": 2
    }
  },
  {
    "text": "for quite some time without being detected, an invisible parasite slowly\ndoing damage. While you are blind, you are being robbed blind. And\nunlike stealing your worldly possessions, stealing your very persona and\nidentity is the cruelest cut of them all.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 34,
      "chunk_index": 3
    }
  },
  {
    "text": "the art of data science 35\nAn increase in efficiency in the web ecosystem comes too at some\nretrenchment of privacy. Who does not shop on the internet? Each trans-\naction resides in a separate web account. These add up at an astonishing\npace. I have no idea of the exact number of web accounts in my name,\nbut I am pretty sure it is over a hundred, many of them used maybe just\nonce. I have unconsciously, yet quite willingly, marked my territory all",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 35,
      "chunk_index": 0
    }
  },
  {
    "text": "over the e-commerce landscape. I rationalize away this loss of privacy in\nthe name of efficiency, which undoubtedly exists. Every now and then I\nam reminded of this loss of privacy as my plane touches down in New\nYork city, and like clockwork, within an hour or two, I receive a discount\ncoupon in my email from Barnes & Noble bookstores. You see, whenever\nI am in Manhattan, I frequent the B&N store on the upper west side, and\nmy credit card company and/or Google knows this, as well as my air",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 35,
      "chunk_index": 1
    }
  },
  {
    "text": "travel schedule, since I buy both tickets and books on the same card and\nin the same browser. So when I want to buy books at a store discount,\nI fly to New York. That’s how rational I am, or how rational my profile\nsays I am! Humor aside, such profiling seems scary, though the thought\nquickly passes. I like the dopamine rush I get from my discount coupon\n17\nand I love buying books. 17Ialsolikewritingbooks,butIam\nmuchbetteratbuyingthem,andsome",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 35,
      "chunk_index": 2
    }
  },
  {
    "text": "muchbetteratbuyingthem,andsome\nProfiling implies a partitioning of the social space into targeted groups,\nwhatlessbetteratreadingthem!\nso that focused attention may be paid to specific groups, or various\ngroups may be treated differently through price discrimination. If my\nprofile shows me to be an affluent person who likes fine wine (both\nfacts untrue in my case, but hope springs eternal), then internet sales\npitches (via Groupon, Living Social, etc.) will be priced higher to me by",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 35,
      "chunk_index": 3
    }
  },
  {
    "text": "an online retailer than to someone whose profile indicates a low spend.\nProfiling enables retailers to maximize revenues by eating away the con-\nsumer’s surplus by better setting of prices to each buyer’s individual\n15\nwillingness to pay. This is depicted in Figure . .\n15\nIn Figure . the demand curve is represented by the line segment\nABC representing price-quantity combinations (more is demanded at\nlower prices). In a competitive market without price segmentation, let’s",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 35,
      "chunk_index": 4
    }
  },
  {
    "text": "assume that the equilibrium price is P and equilibrium quantity is Q\nas shown by the point B on the demand curve. (The upward sloping\nsupply curve is not shown but it must intersect the demand curve at\npoint B, of course.) Total revenue to the seller is the area OPBQ, i.e.,\nP Q.\n×\nNow assume that the seller is able to profile buyers so that price dis-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 35,
      "chunk_index": 5
    }
  },
  {
    "text": "36 data science: theories, models, algorithms, and analytics\nFigure1.5: Extractingconsumer’s\nsurplusthroughprofiling.\nPrice\nA\nConsumer’s\nsurplus\nP B\nMissed sales\nO\nQ C\nQuan’ty (Q)\ncrimination is possible. Based on buyers’ profiles, the seller will offer\neach buyer the price he is willing to pay on the demand curve, thereby\npicking off each price in the segment AB. This enables the seller to cap-\nture the additional region ABP, which is the area of consumer’s surplus,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 36,
      "chunk_index": 0
    }
  },
  {
    "text": "i.e., the difference between the price that buyers pay versus the price\nthey were actually willing to pay. The seller may also choose to offer\nsome consumers lower prices in the region BC of the demand curve so\nas to bring in additional buyers whose threshold price lies below the\ncompetitive market price P. Thus, profiling helps sellers capture con-\nsumer’s surplus and eat into the region of missed sales. Targeting brings",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 36,
      "chunk_index": 1
    }
  },
  {
    "text": "benefits to sellers and they actively pursue it. The benefits outweigh the\ncosts of profiling, and the practice is widespread as a result. Profiling\nalso makes price segmentation fine-tuned, and rather than break buy-\ners into a few segments, usually two, each profile becomes a separate\nsegment, and the granularity of price segmentation is modulated by the\nnumber of profiling groups the seller chooses to model.\nOf course, there is an insidious aspect to profiling, which has existed",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 36,
      "chunk_index": 2
    }
  },
  {
    "text": "for quite some time, such as targeting conducted by tax authorities. I",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 36,
      "chunk_index": 3
    }
  },
  {
    "text": "the art of data science 37\ndon’t believe we will take kindly to insurance companies profiling us\nany more than they already do. Profiling is also undertaken to snare ter-\nrorists. However, there is a danger in excessive profiling. A very specific\nprofile for a terrorist makes it easier for their ilk to game detection as\nfollows. Send several possible suicide bombers through airport security\nand see who is repeatedly pulled aside for screening and who is not.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 37,
      "chunk_index": 0
    }
  },
  {
    "text": "Repeating this exercise enables a terrorist cell to learn which candidates\ndo not fall into the profile. They may then use them for the execution of\na terror act, as they are unlikely to be picked up for special screening.\nThe antidote? Randomization of people picked for special screening in\nsearches at airports, which makes it hard for a terrorist to always assume\n18\nno likelihood of detection through screening. 18See\nhttp://acfnewsource.org.s60463.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 37,
      "chunk_index": 1
    }
  },
  {
    "text": "http://acfnewsource.org.s60463.\nAutomated invasions of privacy naturally lead to a human response,\ngridserver.com/science/random\nnot always rational or predictable. This is articulated in Campbell’s Law: security.html,alsoairedonKRON-\nTV,SanFrancisco,2/3/2003.\n“The more any quantitative social indicator (or even some qualitative\nindicator) is used for social decision-making, the more subject it will\nbe to corruption pressures and the more apt it will be to distort and\n19",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 37,
      "chunk_index": 2
    }
  },
  {
    "text": "19\ncorrupt the social processes it is intended to monitor.” We are in for 19See:http://en.wikipedia.org/wiki/\nCampbell’s law.\nan interesting period of interaction between man and machine, where\nthe battle for privacy will take center stage.\n1.7 Theories, Models, Intuition, Causality, Prediction, Corre-\nlation\nMy view of data science is one where theories are implemented using\ndata, some of it big data. This is embodied in an inference stack com-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 37,
      "chunk_index": 3
    }
  },
  {
    "text": "prising (in sequence): theories, models, intuition, causality, prediction,\nand correlation. The first three constructs in this chain are from Emanuel\n20\nDerman’s wonderful book on the pitfalls of models. 20“Models.Behaving.Badly.”Emanuel\nDerman,FreePress,NewYork,2011.\nTheories are statements of how the world should be or is, and are\nderived from axioms that are assumptions about the world, or precedent\ntheories. Models are implementations of theory, and in data science are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 37,
      "chunk_index": 4
    }
  },
  {
    "text": "often algorithms based on theories that are run on data. The results of\nrunning a model lead to intuition, i.e., a deeper understanding of the\nworld based on theory, model, and data. Whereas there are schools of\nthought that suggest data is all we need, and theory is obsolete, this\nauthor disagrees. Still the unreasonable proven effectiveness of big data\ncannot be denied. Chris Anderson argues in his Wired magazine article",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 37,
      "chunk_index": 5
    }
  },
  {
    "text": "38 data science: theories, models, algorithms, and analytics\n21\nthus:” 21“TheEndofTheory:TheDataDeluge\nMakestheScientificMethodObsolete.”\nSensors everywhere. Infinite storage. Clouds of processors. Our\nWired,v16(7),23rdJune,2008.\nability to capture, warehouse, and understand massive amounts\nof data is changing science, medicine, business, and technology.\nAs our collection of facts and figures grows, so will the oppor-\ntunity to find answers to fundamental questions. Because in the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 38,
      "chunk_index": 0
    }
  },
  {
    "text": "era of big data, more isn’t just more. More is different.\nIn contrast, the academic Thomas Davenport writes in his foreword\n2013\nto Seigel ( ) that models are key, and should not be increasingly es-\nchewed with increasing data:\nBut the point of predictive analytics is not the relative size or\nunruliness of your data, but what you do with it. I have found\nthat “big data often means small math,” and many big data\npractitioners are content just to use their data to create some",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 38,
      "chunk_index": 1
    }
  },
  {
    "text": "appealing visual analytics. That’s not nearly as valuable as\ncreating a predictive model.\nOnce we have established intuition for the results of a model, it re-\nmains to be seen whether the relationships we observe are causal, pre-\ndictive, or merely correlational. Theory may be causal and tested as\n1969\nsuch. Granger ( ) causality is often stated in mathematical form\n22\nfor two stationary time series of data as follows. X is said to Granger 22Aseriesisstationaryiftheprobability",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 38,
      "chunk_index": 2
    }
  },
  {
    "text": "distributionfromwhichtheobserva-\ncause Y if in the following equation system,\ntionsaredrawnisthesameatallpoints\nintime.\nY(t) = a +b Y(t 1)+c X(t 1)+e\n1 1 1 1\n− −\nX(t) = a +b Y(t 1)+c X(t 1)+e\n2 2 2 2\n− −\nthe coefficient c is significant and b is not significant. Hence, X causes\n1 2\nY, but not vice versa. Causality is a hard property to establish, even with\ntheoretical foundation, as the causal effect has to be well-entrenched in\nthe data.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 38,
      "chunk_index": 3
    }
  },
  {
    "text": "the data.\nWe have to be careful to impose judgment as much as possible since\nstatistical relationships may not always be what they seem. A variable\nmay satisfy the Granger causality regressions above but may not be\ncausal. For example, we earlier encountered the flu example in Google\nTrends. If we denote searches for flu as X, and the outbreak of flu as\nY, we may see a Granger cause relation between flu and searches for\nit. This does not mean that searching for flu causes flu, yet searches are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 38,
      "chunk_index": 4
    }
  },
  {
    "text": "predictive of flu. This is the essential difference between prediction and\ncausality.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 38,
      "chunk_index": 5
    }
  },
  {
    "text": "the art of data science 39\nAnd then there is correlation, at the end of the data science inference\nchain. Contemporaneous movement between two variables is quanti-\nfied using correlation. In many cases, we uncover correlation, but no\nprediction or causality. Correlation has great value to firms attempting\nto tease out beneficial information from big data. And even though it is\na linear relationship between variables, it lays the groundwork for un-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 39,
      "chunk_index": 0
    }
  },
  {
    "text": "covering nonlinear relationships, which are becoming easier to detect\nwith more data. The surprising parable about Walmart finding that pur-\nchases of beer and diapers seem to be highly correlated resulted in these\ntwo somewhat oddly-paired items being displayed on the same aisle in\n23\nsupermarkets. Unearthing correlations of sales items across the popu- 23http://www.theregister.co.uk/\n2006/08/15/beer diapers/.\nlation quickly lead to different business models aimed at exploiting these",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 39,
      "chunk_index": 1
    }
  },
  {
    "text": "correlations, such as my book buying inducement from Barnes & Noble,\nwhere my “fly and buy” predilection is easily exploited. Correlation is\noften all we need, eschewing human cravings for causality. As Mayer-\n2013\nSchönberger and Cukier ( ) so aptly put it, we are satisfied “... not\nknowing why but only what.”\nIn the data scientist mode of thought, relationships are multifaceted\ncorrelations amongst people. Facebook, Twitter, and many other plat-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 39,
      "chunk_index": 2
    }
  },
  {
    "text": "forms are datafying human relationships using graph theory, exploiting\nthe social web in an attempt to understand better how people relate to\neach other, with the goal of profiting from it. We use correlations on\nnetworks to mine the social graph, understanding better how different\nsocial structures may be exploited. We answer questions such as where\nto seed a new marketing campaign, which members of a network are\nmore important than the others, how quickly will information spread on",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 39,
      "chunk_index": 3
    }
  },
  {
    "text": "the network, i.e., how strong is the “network effect”?\nData science is about the quantization and understanding of human\nbehavior, the holy grail of social science. In the following chapters we\nwill explore a wide range of theories, techniques, data, and applications\nof a multi-faceted paradigm. We will also review the new technologies\ndeveloped for big data and data science, such as distributed computing\n2004\nusing the Dean and Ghemawat ( ) MapReduce paradigm developed\n24",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 39,
      "chunk_index": 4
    }
  },
  {
    "text": "24\nat Google, and implemented as the open source project Hadoop at Ya- 24http://research.google.com/\n25 archive/mapreduce.html\nhoo!. When data gets super sized, it is better to move algorithms to the\n25http://hadoop.apache.org/\ndata than the other way around. Just as big data has inverted database\nparadigms, so is big data changing the nature of inference in the study\nof human behavior. Ultimately, data science is a way of thinking, for",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 39,
      "chunk_index": 5
    }
  },
  {
    "text": "40 data science: theories, models, algorithms, and analytics\nsocial scientists, using computer science.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 40,
      "chunk_index": 0
    }
  },
  {
    "text": "2\nThe Very Beginning: Got Math?\nBusiness analytics requires the use of various quantitative tools, from\nalgebra and calculus, to statistics and econometrics, with implementa-\ntions in various programming languages and software. It calls for tech-\nnical expertise as well as good judgment, and the ability to ask insightful\nquestions and to deploy data towards answering the questions.\nThe presence of the web as the primary platform for business and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 41,
      "chunk_index": 0
    }
  },
  {
    "text": "marketing has spawned huge quantities of data, driving firms to attempt\nto exploit vast stores of information in honing their competitive edge. As\na consequence, firms in Silicon Valley (and elsewhere) are hiring a new\nbreed of employee known as “data scientist” whose role is to analyze\n“Big Data” using tools such as the ones you will learn in this course.\nThis chapter will review some of the mathematics, statistics, linear al-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 41,
      "chunk_index": 1
    }
  },
  {
    "text": "gebra, and calculus you might have not used in many years. It is more\nfun than it looks. We will also learn to use some mathematical packages\nalong the way. We’ll revisit some standard calculations and analyses that\nyou will have encountered in previous courses you might have taken.\nYou will refresh some old concepts, learn new ones, and become techni-\ncally adept with the tools of the trade.\n2.1 Exponentials, Logarithms, and Compounding",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 41,
      "chunk_index": 2
    }
  },
  {
    "text": "2.1 Exponentials, Logarithms, and Compounding\nIt is fitting to begin with the fundamental mathematical constant, “e =\n2.718281828...”, which is also the function “exp( )”. We often write this\n·\nfunction as ex, where x can be a real or complex variable. It shows up\nin many places, especially in Finance, where it is used for continuous\ncompounding and discounting of money at a given interest rate r over\nsome time horizon t.\nGiven y = ex, a fixed change in x results in the same continuous",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 41,
      "chunk_index": 3
    }
  },
  {
    "text": "42 data science: theories, models, algorithms, and analytics\npercentage change in y. This is because ln(y) = x, where ln( ) is the\n·\nnatural logarithm function, and is the inverse function of the exponential\nfunction. Recall also that the first derivative of this function is dy = ex,\ndx\ni.e., the function itself.\nThe constant e is defined as the limit of a specific function:\n(cid:18) (cid:19)n\n1\ne = lim 1+\nn ∞ n\n→\nExponential compounding is the limit of successively shorter intervals",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 42,
      "chunk_index": 0
    }
  },
  {
    "text": "over discrete compounding. Given a horizon t divided into n inter-\nvals per year, one dollar compounded from time zero to time t years\nover these n intervals at per annum rate r may be written as\n(cid:0)\n1+\nr(cid:1)nt\n.\nn\nContinuous-compounding is the limit of this equation when the number\nof periods n goes to infinity:\n(cid:16) r(cid:17)nt\n(cid:34)\n(cid:18)\n1\n(cid:19)n/r\n(cid:35)tr\nlim 1+ = lim 1+ = ert\nn ∞ n n ∞ n/r\n→ →",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 42,
      "chunk_index": 1
    }
  },
  {
    "text": "lim 1+ = lim 1+ = ert\nn ∞ n n ∞ n/r\n→ →\nThis is the forward value of one dollar. Present value is just the reverse.\nTherefore, the price today of a dollar received t years from today is P =\ne rt. The yield of a bond is:\n−\n1\nr = ln(P)\n−t\nIn bond mathematics, the negative of the percentage price sensitivity\nof a bond to changes in interest rates is known as “Duration”:\n(cid:18) (cid:19)\ndP 1 1 1\n= te\n−\nrt = tP = t.\n−dr P − − P P",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 42,
      "chunk_index": 2
    }
  },
  {
    "text": "dP 1 1 1\n= te\n−\nrt = tP = t.\n−dr P − − P P\nThe derivative dP is the price sensitivity of the bond to changes in inter-\ndr\nest rates, and is negative. Further dividing this by P gives the percentage\nprice sensitivity. The minus sign in front of the definition of duration is\napplied to convert the negative number to a positive one.\nThe “Convexity” of a bond is its percentage price sensitivity relative to\nthe second derivative, i.e.,\nd2P 1 1\n= t2P = t2.\ndr2 P P",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 42,
      "chunk_index": 3
    }
  },
  {
    "text": "d2P 1 1\n= t2P = t2.\ndr2 P P\nBecause the second derivative is positive, we know that the bond pricing\nfunction is convex.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 42,
      "chunk_index": 4
    }
  },
  {
    "text": "the very beginning: got math? 43\n2.2 Normal Distribution\nThis distribution is the workhorse of many models in the social sciences,\nand is assumed to generate much of the data that comprises the Big Data\nuniverse. Interestingly, most phenomena (variables) in the real world\nare not normally distributed. They tend to be “power law” distributed,\ni.e., many observations of low value, and very few of high value. The\nprobability distribution declines from left to right and does not have the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 43,
      "chunk_index": 0
    }
  },
  {
    "text": "characteristic hump shape of the normal distribution. An example of\ndata that is distributed thus is income distribution (many people with\nlow income, very few with high income). Other examples are word fre-\nquencies in languages, population sizes of cities, number of connections\nof people in a social network, etc.\nStill, we do need to learn about the normal distribution because it is\nimportant in statistics, and the central limit theorem does govern much",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 43,
      "chunk_index": 1
    }
  },
  {
    "text": "of the data we look at. Examples of approximately normally distributed\ndata are stock returns, and human heights.\nIf x N(µ,σ2), that is, x is normally distributed with mean µ and\n∼\nvariance σ2, then the probability “density” function for x is:\n1\n(cid:20)\n1(x\nµ)2(cid:21)\nf(x) = exp −\n√2πσ2 −2 σ2\nThe cumulative probability is given by the “distribution” function\n(cid:90) x\nF(x) = f(u)du\n∞\n−\nand\nF(x) = 1 F( x)\n− −\nbecause the normal distribution is symmetric. We often also use the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 43,
      "chunk_index": 2
    }
  },
  {
    "text": "notation N( ) or Φ( ) instead of F( ).\n· · ·\nThe “standard normal” distribution is: x N(0,1). For the standard\n∼\nnormal distribution: F(0) = 1. The normal distribution has continuous\n2\nsupport, i.e., a range of values of x that goes continuously from ∞ to\n−\n+∞\n.\n2.3 Poisson Distribution\nThe Poisson is also known as the rare-event distribution. Its density\nfunction is:\ne λλn\n−\nf(n;λ) =\nn!",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 43,
      "chunk_index": 3
    }
  },
  {
    "text": "44 data science: theories, models, algorithms, and analytics\nwhere there is only one parameter, i.e., the mean λ. The density function\nis over discrete values of n, the number of occurrences given the mean\nnumber of outcomes λ. The mean and variance of the Poisson distribu-\ntion are both λ. The Poisson is a discrete-support distribution, with a\nrange of values n = 0,1,2,... .\n{ }\n2.4 Moments of a continuous random variable\nThe following formulae are useful to review because any analysis of data",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 44,
      "chunk_index": 0
    }
  },
  {
    "text": "begins with descriptive statistics, and the following statistical “moments”\nare computed in order to get a first handle on the data. Given a random\nvariable x with probability density function f(x), then the following are\nthe first four moments.\n(cid:90)\nMean (first moment or average) = E(x) = xf(x)dx\nIn like fashion, powers of the variable result in higher (n-th order) mo-\nments. These are “non-central” moments, i.e., they are moments of the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 44,
      "chunk_index": 1
    }
  },
  {
    "text": "raw random variable x, not its deviation from its mean, i.e., [x E(x)].\n−\n(cid:90)\nnth moment = E(xn) = xnf(x)dx\nCentral moments are moments of de-meaned random variables. The\nsecond central moment is the variance:\nVariance = Var(x) = E[x E(x)]2 = E(x2) [E(x)]2\n− −\nThe standard deviation is the square-root of the variance, i.e., σ =\n(cid:112)\nVar(x). The third central moment, normalized by the standard de-\nviation to a suitable power is the skewness:\nE[x E(x)]3\nSkewness = −\nVar(x)3/2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 44,
      "chunk_index": 2
    }
  },
  {
    "text": "E[x E(x)]3\nSkewness = −\nVar(x)3/2\nThe absolute value of skewness relates to the degree of asymmetry in\nthe probability density. If more extreme values occur to the left than the\nright, the distribution is left-skewed. And vice-versa, the distribution is\nright-skewed.\nCorrespondingly, the fourth central, normalized moment is kurtosis.\nE[x E(x)]4\nKurtosis = −\n[Var(x)]2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 44,
      "chunk_index": 3
    }
  },
  {
    "text": "the very beginning: got math? 45\nKurtosis in the normal distribution has value 3. We define “Excess Kur-\n3\ntosis” to be Kurtosis minus . When a probability distribution has posi-\ntive excess kurtosis we call it “leptokurtic”. Such distributions have fatter\ntails (either or both sides) than a normal distribution.\n2.5 Combining random variables\nSince we often have to deal with composites of random variables, i.e.,\nmore than one random variable, we review here some simple rules for",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 45,
      "chunk_index": 0
    }
  },
  {
    "text": "moments of combinations of random variables. There are several other\nexpressions for the same equations, but we examine just a few here, as\nthese are the ones we will use more frequently.\nFirst, we see that means are additive and scalable, i.e.,\nE(ax+by) = aE(x)+bE(y)\nwhere x,y are random variables, and a,b are scalar constants. The vari-\nance of scaled, summed random variables is as follows:\nVar(ax+by) = a2Var(x)+b2Var(y)+2abCov(x,y) ( 2 . 1 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 45,
      "chunk_index": 1
    }
  },
  {
    "text": "And the covariance and correlation between two random variables is\nCov(x,y) = E(xy) E(x)E(y)\n−\nCov(x,y)\nCorr(x,y) =\n(cid:112)\nVar(x)Var(y)\nStudents of finance will be well-versed with these expressions. They are\nfacile and easy to implement.\n2.6 Vector Algebra\nWe will be using linear algebra in many of the models that we explore\nin this book. Linear algebra requires the manipulation of vectors and\nmatrices. We will also use vector calculus. Vector algebra and calculus",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 45,
      "chunk_index": 2
    }
  },
  {
    "text": "are very powerful methods for tackling problems that involve solutions\nin spaces of several variables, i.e., in high dimension. The parsimony of\nusing vector notation will become apparent as we proceed. This intro-\nduction is very light and meant for the reader who is mostly uninitiated\nin linear algebra.\nRather than work with an abstract exposition, it is better to introduce\nideas using an example. We’ll examine the use of vectors in the context",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 45,
      "chunk_index": 3
    }
  },
  {
    "text": "46 data science: theories, models, algorithms, and analytics\nof stock portfolios. We define the returns for each stock in a portfolio as:\n \nR\n1\n \nR\n 2 \n \nR =  : \n \n : \n \nR\nN\nThis is a random vector, because each return R ,i = 1,2,...,N comes\ni\nfrom its own distribution, and the returns of all these stocks are corre-\nlated. This random vector’s probability is represented as a joint or mul-\ntivariate probability distribution. Note that we use a bold font to denote\nthe vector R.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 46,
      "chunk_index": 0
    }
  },
  {
    "text": "the vector R.\nWe also define a Unit vector:\n \n1\n \n1\n \n \n1 =  : \n \n : \n \n1\nThe use of this unit vector will become apparent shortly, but it will be\nused in myriad ways and is a useful analytical object.\nA portfolio vector is defined as a set of portfolio weights, i.e., the frac-\ntion of the portfolio that is invested in each stock:\n \nw\n1\n \nw\n 2 \n \nw =  : \n \n : \n \nw\nN\n1\nThe total of portfolio weights must add up to .\nN\n∑\nw i = 1, w (cid:48) 1 = 1\ni=1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 46,
      "chunk_index": 1
    }
  },
  {
    "text": "N\n∑\nw i = 1, w (cid:48) 1 = 1\ni=1\nPay special attention to the line above. In it, there are two ways in which\nto describe the sum of portfolio weights. The first one uses summation\nnotation, and the second one uses a simple vector algebraic statement,\ni.e., that the transpose of w, denoted w\n(cid:48)\ntimes the unit vector 1 equals 1 . 1 1Often,thenotationfortransposeis\ntosuperscriptT,andinthiscasewe\nThe two elements on the left-hand-side of the equation are vectors, and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 46,
      "chunk_index": 2
    }
  },
  {
    "text": "wouldwritethisasw(cid:62).Wemayuse\nthe 1 on the right hand side is a scalar. The dimension of w is (1 N) eithernotationintherestofthebook.\n(cid:48)\n×",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 46,
      "chunk_index": 3
    }
  },
  {
    "text": "the very beginning: got math? 47\nand the dimension of 1 is (N 1). And a (1 N) vector multiplied by a\n× ×\n(N 1) results in a (1 1) vector, i.e., a scalar.\n× ×\nWe may also invest in a risk free asset (denoted as asset zero, i = 0),\nwith return R = r . In this case, the total portfolio weights including\n0 f\nthat of the risk free asset must sum to 1 , and the weight w is:\n0\nN\n∑\nw 0 = 1 w i = 1 w (cid:62) 1\n− −\ni=1\nNow we can use vector notation to compute statistics and quantities of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 47,
      "chunk_index": 0
    }
  },
  {
    "text": "the portfolio. The portfolio return is\nN\n∑\nR = w R = w R\np i i (cid:48)\ni=1\nAgain, note that the left-hand-side quantity is a scalar, and the two right-\nhand-side quantities are vectors. Since R is a random vector, R is a\np\nrandom (scalar, i.e., not a vector, of dimension 1 1) variable. Such a\n×\nproduct is called a scalar product of two vectors. In order for the calcula-\ntion to work, the two vectors or matrices must be “conformable” i.e., the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 47,
      "chunk_index": 1
    }
  },
  {
    "text": "inner dimensions of the matrices must be the same. In this case we are\nmultiplying w of dimension 1 N with R of dimension N 1 and since\n(cid:48)\n× ×\nthe two “inside” dimensions are both n, the calculation is proper as the\nmatrices are conformable. The result of the calculation will take the size\nof the “outer” dimensions, i.e., in this case 1 1. Now, suppose\n×\nR MVN[µ; Σ]\n∼\nThat is, returns are multivariate normally distributed with mean vector",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 47,
      "chunk_index": 2
    }
  },
  {
    "text": "E[R] = µ = [µ ,µ ,...,µ ] RN and covariance matrix Σ RN N. The\n1 2 N (cid:48) ×\n∈ ∈\nnotation RN stands for a “real-valued matrix of dimension N.” If it’s just\nN, then it means a vector of dimension N. If it’s written as N M, then\n×\nit’s a matrix of that dimension, i.e., N rows and M columns.\nWe can write the portfolio’s mean return as:\nN\n∑\nE[w R] = w E[R] = w µ = w µ\n(cid:48) (cid:48) (cid:48) i i\ni=1\nThe portfolio’s return variance is\nVar(R ) = Var(w R) = w Σw\np (cid:48) (cid:48)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 47,
      "chunk_index": 3
    }
  },
  {
    "text": "Var(R ) = Var(w R) = w Σw\np (cid:48) (cid:48)\nShowing why this is true is left as an exercise to the reader. Take a case\nwhere N = 2 and write out the expression for the variance of the port-\n21\nfolio using equation . . Then also undertake the same calculation using",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 47,
      "chunk_index": 4
    }
  },
  {
    "text": "48 data science: theories, models, algorithms, and analytics\nthe variance formula w Σw and see the equivalence. Also note carefully\n(cid:48)\nΣ\nthat this expression works because is a symmetric matrix. The multi-\nvariate normal density function is:\n(cid:20) (cid:21)\n1 1\nf(R) = exp (R−µ)(cid:48)Σ−1(R−µ)\n2πN/2 (cid:112) Σ −2\n| |\nNow, we take a look at some simple applications expressed in terms of\nvector notation.\n2.7 Statistical Regression",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 48,
      "chunk_index": 0
    }
  },
  {
    "text": "vector notation.\n2.7 Statistical Regression\nConsider a multivariate regression where a stock’s returns R are re-\ni\ngressed on several market factors R .\nk\nk\n∑\nR = β R +e , i.\nit ij jt it\n∀\nj=0\nwhere t = 1,2,...,T (i.e., there are T items in the time series), and\n{ }\nthere are k independent variables, and usually k = 0 is for the intercept.\nWe could write this also as\nk\n∑\nR = β + β R +e , i.\nit 0 ij jt it\n∀\nj=1\nCompactly, using vector notation, the same regression may be written as:\nR = R β +e",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 48,
      "chunk_index": 1
    }
  },
  {
    "text": "R = R β +e\ni k i i\nwhere R ,e RT, R RT (k+1), and β Rk+1. If there is an intercept\ni i k × i\n∈ ∈ ∈\nin the regression then the first column of R is 1, the unit vector. Without\nk\nproviding a derivation, you should know that each regression coefficient\nis:\nCov(R ,R )\nβ = i k\nik Var(R )\nk\nIn vector form, all coefficients may be calculated at once:\nβ = (R R ) 1(R R )\ni (cid:48)k k − (cid:48)k i\nwhere the superscript ( 1) stands for the inverse of the matrix (R R )\n−\n(cid:48)k k",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 48,
      "chunk_index": 2
    }
  },
  {
    "text": "−\n(cid:48)k k\nwhich is of dimension (k +1) (k +1). Convince yourself that the\n×\ndimension of the expression (R R ) is (k+1) 1, i.e., it is a vector. This\n(cid:48)k i\n×\nresults in the vector β R(k+1). This result comes from minimizing the\ni\n∈\nsummed squared mean residual error in the regression i.e.,\nmine\ni(cid:48)\ne\ni\nβi\nThis will be examined in full detail later in this book.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 48,
      "chunk_index": 3
    }
  },
  {
    "text": "the very beginning: got math? 49\n2.8 Diversification\nIt is useful to examine the power of using vector algebra with an ap-\nplication. Here we use vector and summation math to understand how\ndiversification in stock portfolios works. Diversification occurs when\nwe increase the number of non-perfectly correlated stocks in a portfolio,\nthereby reducing portfolio variance. In order to compute the variance of\nthe portfolio we need to use the portfolio weights w and the covariance",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 49,
      "chunk_index": 0
    }
  },
  {
    "text": "matrix of stock returns R, denoted Σ . We first write down the formula\nfor a portfolio’s return variance:\nn n n\nVar(w (cid:48) R) = w (cid:48)Σw = ∑ w2σ2+ ∑ ∑ w w σ\ni i i j ij\ni=1 i=1j=1,i=j\n(cid:54)\nReaders are strongly encouraged to implement this by hand for n = 2 to\nconvince themselves that the vector form of the expression for variance\nw(cid:48)Σw\nis the same thing as the long form on the right-hand side of the\nequation above. If returns are independent, then the formula collapses\nto:\nn",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 49,
      "chunk_index": 1
    }
  },
  {
    "text": "to:\nn\nVar(w R) = w Σw = ∑ w2σ2\n(cid:48) (cid:48) i i\ni=1\nIf returns are dependent, and equal amounts are invested in each asset\n(w = 1/n, i):\ni\n∀\nVar(w R) = 1 ∑ n σ i 2 + n − 1 ∑ n ∑ n σ ij\n(cid:48)\nn n n n(n 1)\ni=1 i=1j=1,i=j −\n(cid:54)\n1 n 1\n= σ¯2+ − σ¯\ni ij\nn n\n(cid:18) (cid:19)\n1 1\n= σ¯2+ 1 σ¯\ni ij\nn − n\nThe first term is the average variance, denoted σ¯ 2 divided by n, and\n1\nthe second is the average covariance, denoted σ¯ multiplied by factor\nij\n(n 1)/n. As n ∞ ,\n− →",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 49,
      "chunk_index": 2
    }
  },
  {
    "text": "ij\n(n 1)/n. As n ∞ ,\n− →\nVar(w (cid:48) R) = σ¯ ij .\nThis produces the remarkable result that in a well diversified portfolio,\nthe variances of each stock’s return does not matter at all for portfolio\nrisk! Further the risk of the portfolio, i.e., its variance, is nothing but the\naverage of off-diagonal terms in the covariance matrix.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 49,
      "chunk_index": 3
    }
  },
  {
    "text": "50 data science: theories, models, algorithms, and analytics\nDiversification exercise\nImplement the math above using R to compute the standard deviation\nof a portfolio of n identical securities with variance 0 . 04 , and pairwise\ncovariances equal to 0 . 01 . Keep increasing n and report the value of the\nstandard deviation. What do you see? Why would this be easier to do in\nR versus Excel?\nMatrix algebra exercise\nThe following brief notes will introduce you to everything you need to",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 50,
      "chunk_index": 0
    }
  },
  {
    "text": "know about the vocabulary of vectors and matrices in a \"DIY\" (do-it-\nyourself) mode. Define\n(cid:34) (cid:35)\nw\nw = [w w ] = 1\n1 2 (cid:48)\nw\n2\n(cid:34) (cid:35)\n1 0\nI =\n0 1\n(cid:34) (cid:35)\nσ2 σ\nΣ = 1 12\nσ σ2\n12 2\nDo the following exercises in long hand:\n• Show that\nI w = w.\n• Show that the dimensions make sense at all steps of your calculations.\n• Show that\nw Σ w = w2σ2+2w w σ +w2σ2.\n(cid:48) 1 1 1 2 12 2 2\n2.9 Matrix Calculus",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 50,
      "chunk_index": 1
    }
  },
  {
    "text": "(cid:48) 1 1 1 2 12 2 2\n2.9 Matrix Calculus\nIt is simple to undertake calculus when working with matrices. Cal-\nculations using matrices are mere functions of many variables. These\nfunctions are amenable to applying calculus, just as you would do in\nmultivariate calculus. However, using vectors and matrices makes things\nsimpler in fact, because we end up taking derivatives of these multivari-\nate functions in one fell swoop rather than one-by-one for each variable.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 50,
      "chunk_index": 2
    }
  },
  {
    "text": "An example will make this clear. Suppose\n(cid:34) (cid:35)\nw\nw = 1\nw\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 50,
      "chunk_index": 3
    }
  },
  {
    "text": "the very beginning: got math? 51\nand\n(cid:34) (cid:35)\n3\nB =\n4\nLet f(w) = w B. This is a function of two variables w ,w . If we write\n(cid:48) 1 2\nout f(w) in long form, we get 3w +4w . The derivative of f(w) with\n1 2\n∂f\nrespect to w is = 3. The derivative of f(w) with respect to w is\n1 ∂w 2\n1\n∂f\n= 4. Compare these answers to vector B. What do you see? What is\n∂w\n2\ndf\n? It’s B.\ndw\nThe insight here is that if we simply treat the vectors as regular scalars",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 51,
      "chunk_index": 0
    }
  },
  {
    "text": "and conduct calculus accordingly, we will end up with vector deriva-\ntives. Hence, the derivative of w B with respect to w is just B. Of course,\n(cid:48)\nw B is an entire function and B is a vector. But the beauty of this is that\n(cid:48)\nwe can take all derivatives of function w B at one time!\n(cid:48)\nThese ideas can also be extended to higher-order matrix functions.\nSuppose\n(cid:34) (cid:35)\n3 2\nA =\n2 4\nand\n(cid:34) (cid:35)\nw\nw = 1\nw\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 51,
      "chunk_index": 1
    }
  },
  {
    "text": "3 2\nA =\n2 4\nand\n(cid:34) (cid:35)\nw\nw = 1\nw\n2\nLet f(w) = w Aw. If we write out f(w) in long form, we get\n(cid:48)\nw\n(cid:48)\nAw = 3w\n1\n2+4w\n2\n2+2(2)w\n1\nw\n2\nTake the derivative of f(w) with respect to w , and this is\n1\ndf\n= 6w +4w\n1 2\ndw\n1\nTake the derivative of f(w) with respect to w , and this is\n2\ndf\n= 8w +4w\n2 1\ndw\n2\nNow, we write out the following calculation in long form:\n(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35)\n3 2 w 6w +4w\n2 A w = 2 1 = 1 2\n2 4 w 8w +4w\n2 2 1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 51,
      "chunk_index": 2
    }
  },
  {
    "text": "3 2 w 6w +4w\n2 A w = 2 1 = 1 2\n2 4 w 8w +4w\n2 2 1\nWhat do you notice about this solution when compared to the previous\ntwo answers? It is nothing but df . Since w R2, i.e., is of dimension 2 ,\ndw ∈\ndf\nthe derivative will also be of that dimension.\ndw",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 51,
      "chunk_index": 3
    }
  },
  {
    "text": "52 data science: theories, models, algorithms, and analytics\nTo see how this corresponds to scalar calculus, think of the function\nf(w) = w Aw as simply Aw2, where w is scalar. The derivative of this\n(cid:48)\nfunction with respect to w would be 2Aw. And, this is the same as what\nwe get when we look at a function of vectors, but with the caveat below.\nNote: This computation only works out because A is symmetric. What\nshould the expression be for the derivative of this vector function if A is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 52,
      "chunk_index": 0
    }
  },
  {
    "text": "not symmetric but is a square matrix? It turns out that this is\n∂f\n= A\n(cid:48)\nw+Aw = 2Aw\n∂w (cid:54)\nLet’s try this and see. Suppose\n(cid:34) (cid:35)\n3 2\nA =\n1 4\nYou can check that the following is all true:\nw\n(cid:48)\nAw = 3w\n1\n2+4w\n2\n2+3w\n1\nw\n2\n∂f\n= 6w +3w\n1 2\n∂w\n1\n∂f\n= 3w +8w\n1 2\n∂w\n2\nand\n(cid:34) (cid:35)\n6w +3w\nA w+Aw = 1 2\n(cid:48)\n3w +8w\n1 2\nwhich is correct, but note that the formula for symmetric A is not!\n(cid:34) (cid:35)\n6w +4w\n2Aw = 1 2\n2w +8w\n1 2\n2.10 Matrix Equations",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 52,
      "chunk_index": 1
    }
  },
  {
    "text": "6w +4w\n2Aw = 1 2\n2w +8w\n1 2\n2.10 Matrix Equations\nHere we examine how matrices may be used to represent large systems\nof equations easily and also solve them. Using the values of matrices A,\nB and w from the previous section, we write out the following in long\nform:\nAw = B\nThat is, we have\n(cid:34) (cid:35)(cid:34) (cid:35) (cid:34) (cid:35)\n3 2 w 3\n1 =\n2 4 w 4\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 52,
      "chunk_index": 2
    }
  },
  {
    "text": "the very beginning: got math? 53\n2\nDo you get equations? If so, write them out. Find the solution values\nw and w by hand. And then we may compute the solution for w by\n1 2\n“dividing” B by A. This is not regular division because A and B are\nmatrices. Instead we need to multiply the inverse of A (which is its “re-\nciprocal”) by B.\nThe inverse of A is\n(cid:34) (cid:35)\n0.500 0.250\nA 1 = −\n−\n0.250 0.375\n−\nNow compute by hand:\n(cid:34) (cid:35)\n0.50\nA 1B =\n−\n0.75",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 53,
      "chunk_index": 0
    }
  },
  {
    "text": "(cid:34) (cid:35)\n0.50\nA 1B =\n−\n0.75\nwhich should be the same as your solution by hand. Literally, this is all\nthe matrix algebra and calculus you will need for most of the work we\nwill do.\nMore exercises\nTry the following questions for practice.\n• What is the value of\nA 1AB\n−\nIs this vector or scalar?\n• What is the final dimension of\n(w B)(AAA 1B)\n(cid:48) −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 53,
      "chunk_index": 1
    }
  },
  {
    "text": "3\nOpen Source: Modeling in R\nIn this chapter, we develop some expertise in using the R statistical pack-\nage. There are many tutorials available now on the web. See the manuals\non the R web site www.r-project.org. There is also a great book that I\npersonally find very high quality, titled “The Art of R Programming” by\nNorman Matloff. Another useful book is “Machine Learning for Hack-\ners” by Drew Conway and John Myles White.\nI assume you have downloaded and installed R by now. If not you can",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 55,
      "chunk_index": 0
    }
  },
  {
    "text": "get it from the R project page:\nwww.r-project.org\nOr, you can get a commercial version, offered free to academics and\nstudents by Revolution Analytics (the company is to R what RedHat is to\nLinux). See\nwww.revolutionanalytics.com\nFor a useful interface when using R, install RStudio, see www.rstudio.com,\nbut install R first. Let’s get started with some basic programming in R.\n3.1 System Commands\nIf you want to directly access the system you can issue system com-\nmands as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 55,
      "chunk_index": 1
    }
  },
  {
    "text": "mands as follows:\nsystem(\"<command>\")\nFor example\nsystem(\" ls lt | grep Das\")\n−\nwill list all directory entries that contain my last name in reverse chrono-\nlogical order. Here I am using a unix command, so this will not work on\na Windoze machine, but it will certainly work on a Mac or Linux box.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 55,
      "chunk_index": 2
    }
  },
  {
    "text": "56 data science: theories, models, algorithms, and analytics\nHowever, you are hardly going to be issuing commands at the system\nlevel, so you are unlikely to use the system command very much.\n3.2 Loading Data\nTo get started, we need to grab some data. Go to Yahoo! Finance and\ndownload some historical data in an Excel spreadsheet, re-sort it into\nchronological order, then save it as a CSV file. Read the file into R as\nfollows:\n> data = read.csv(\"goog.csv\" ,header=TRUE) #Read in the data",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 56,
      "chunk_index": 0
    }
  },
  {
    "text": "> n = dim(data)[ 1 ]\n> n\n1 1671\n[ ]\n> data = data[n: 1 ,]\nThe last command reverses the sequence of the data if required. We\ncan download stock data using the quantmod package. Note: to install\na package you can use the drop down menus on Windows and Mac\noperating systems, and use a package installer on Linux. Or issue the\nfollowing command:\ninstall .packages(\"quantmod\")\nNow we move on to using this package.\n> library(quantmod)\nLoading required package: xts\nLoading required package: zoo",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 56,
      "chunk_index": 1
    }
  },
  {
    "text": "Loading required package: zoo\n> getSymbols(c(\"YHOO\" ,\"AAPL\" ,\"CSCO\" ,\"IBM\" ))\n1\n[ ] \"YHOO\" \"AAPL\" \"CSCO\" \"IBM\"\n2007 01 03 2015 01 07\n> yhoo = YHOO[ ’ :: ’]\n− − − −\n2007 01 03 2015 01 07\n> aapl = AAPL[ ’ :: ’]\n− − − −\n2007 01 03 2015 01 07\n> csco = CSCO[ ’ :: ’]\n− − − −\n2007 01 03 2015 01 07\n> ibm = IBM[ ’ :: ’]\n− − − −\nOr we can also directly create columns of stock data as follows.\n> yhoo = as.matrix(YHOO[ , 6 ])\n> aapl = as.matrix(AAPL[ , 6 ])\n> csco = as.matrix(CSCO[ , 6 ])",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 56,
      "chunk_index": 2
    }
  },
  {
    "text": "> csco = as.matrix(CSCO[ , 6 ])\n> ibm = as.matrix(IBM[ , 6 ])",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 56,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 57\nWe now go ahead and concatenate columns of data into one stock data\nset.\n> stkdata = cbind(yhoo,aapl ,csco ,ibm)\n> dim(stkdata)\n1 2018 4\n[ ]\nNow, compute daily returns. This time, we do log returns in continuous-\ntime. The mean returns are:\n> n = length(stkdata [ , 1 ])\n> n\n1 2018\n[ ]\n> rets = log(stkdata [ 2 :n,] /stkdata [ 1 :(n 1 ),])\n−\n> colMeans( rets )\nYHOO.Adjusted AAPL.Adjusted CSCO.Adjusted IBM.Adjusted\n3 175185 04 1 116251 03 4 106314 05 3 038824 04",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 57,
      "chunk_index": 0
    }
  },
  {
    "text": "3 175185 04 1 116251 03 4 106314 05 3 038824 04\n. e . e . e . e\n− − − −\nWe can also compute the covariance matrix and correlation matrix:\n> cv = cov( rets )\n> print(cv , 2 )\nYHOO.Adjusted AAPL.Adjusted CSCO.Adjusted IBM.Adjusted\n0 00067 0 00020 0 00022 0 00015\nYHOO.Adjusted . . . .\n0 00020 0 00048 0 00021 0 00015\nAAPL.Adjusted . . . .\n0 00022 0 00021 0 00041 0 00017\nCSCO.Adjusted . . . .\n0 00015 0 00015 0 00017 0 00021\nIBM.Adjusted . . . .\n> cr = cor( rets )\n> print(cr , 4 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 57,
      "chunk_index": 1
    }
  },
  {
    "text": "> cr = cor( rets )\n> print(cr , 4 )\nYHOO.Adjusted AAPL.Adjusted CSCO.Adjusted IBM.Adjusted\n1 0000 0 3577 0 4170 0 3900\nYHOO.Adjusted . . . .\n0 3577 1 0000 0 4872 0 4867\nAAPL.Adjusted . . . .\n0 4170 0 4872 1 0000 0 5842\nCSCO.Adjusted . . . .\n0 3900 0 4867 0 5842 1 0000\nIBM.Adjusted . . . .\nNotice the print command that allows you to choose the number of sig-\nnificant digits.\nFor more flexibility and better handling of data files in various for-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 57,
      "chunk_index": 2
    }
  },
  {
    "text": "mats, you may also refer to the readr package. It has many useful func-\ntions.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 57,
      "chunk_index": 3
    }
  },
  {
    "text": "58 data science: theories, models, algorithms, and analytics\n3.3 Matrices\nQ. What do you get if you cross a mountain-climber with a mosquito?\nA. Can’t be done. You’ll be crossing a scaler with a vector.\nWe will use matrices extensively in modeling, and here we examine\nthe basic commands needed to create and manipulate matrices in R. We\ncreate a 4 3 matrix with random numbers as follows:\n×\n> x = matrix(rnorm( 12 ) , 4 , 3 )\n> x\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 0625034 0 9256896 2 3989183\n[ ,] . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 58,
      "chunk_index": 0
    }
  },
  {
    "text": "1 0 0625034 0 9256896 2 3989183\n[ ,] . . .\n2 0 5371860 0 7497727 0 0857688\n[ ,] . . .\n− − −\n3 1 0416409 1 6175885 3 3755593\n[ ,] . . .\n−\n4 0 3244804 0 1228325 1 6494255\n[ ,] . . .\n−\nTransposing the matrix, notice that the dimensions are reversed:\n> print(t(x) , 3 )\n1 2 3 4\n[ , ] [ , ] [ , ] [ , ]\n1 0 0625 0 5372 1 04 0 324\n[ ,] . . . .\n− −\n2 0 9257 0 7498 1 62 0 123\n[ ,] . . . .\n−\n3 2 3989 0 0858 3 38 1 649\n[ ,] . . . .\n− −\nOf course, it is easy to multiply matrices as long as they conform. By",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 58,
      "chunk_index": 1
    }
  },
  {
    "text": "“conform” we mean that when multiplying one matrix by another, the\nnumber of columns of the matrix on the left must be equal to the num-\nber of rows of the matrix on the right. The resultant matrix that holds\nthe answer of this computation will have the number of rows of the ma-\ntrix on the left, and the number of columns of the matrix on the right.\nSee the examples below:\n> print(t(x) %*% x, 3 )\n1 2 3\n[ , ] [ , ] [ , ]\n1 1 48 1 18 3 86\n[ ,] . . .\n− −\n2 1 18 4 05 7 54\n[ ,] . . .\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 58,
      "chunk_index": 2
    }
  },
  {
    "text": "[ ,] . . .\n− −\n2 1 18 4 05 7 54\n[ ,] . . .\n−\n3 3 86 7 54 19 88\n[ ,] . . .\n−\n>\n> print(x %*% t(x) , 3 )\n1 2 3 4\n[ , ] [ , ] [ , ] [ , ]\n1 6 616 0 933 9 530 3 823\n[ ,] . . . .\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 58,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 59\n2 0 933 0 858 0 943 0 125\n[ ,] . . . .\n− − −\n3 9 530 0 943 15 096 5 707\n[ ,] . . . .\n− −\n4 3 823 0 125 5 707 2 841\n[ ,] . . . .\n− − −\nTaking the inverse of the covariance matrix:\n> cv_inv = solve(cv)\n> print(cv_inv , 3 )\ngoog aapl csco ibm\n3809 1395 1058 491\ngoog\n− − −\n1395 3062 615 1139\naapl\n− − −\n1058 615 3971 2346\ncsco\n− − −\n491 1139 2346 7198\nibm\n− − −\nCheck that the inverse is really so!\n> print(cv_inv %*% cv , 3 )\ngoog aapl csco ibm",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 59,
      "chunk_index": 0
    }
  },
  {
    "text": "> print(cv_inv %*% cv , 3 )\ngoog aapl csco ibm\n1 00 00 8 33 17 1 53 16 2 78 17\ngoog . e+ . e . e . e\n− − − −\n2 22 16 1 00 00 3 33 16 5 55 17\naapl . e . e+ . e . e\n− − − − − −\n2 22 16 0 00 00 1 00 00 2 22 16\ncsco . e . e+ . e+ . e\n− −\n2 22 16 2 22 16 2 22 16 1 00 00\nibm . e . e . e . e+\n− − − − − −\nIt is, the result of multiplying the inverse matrix by the matrix itself re-\nsults in the identity matrix. A covariance matrix should be positive defi-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 59,
      "chunk_index": 1
    }
  },
  {
    "text": "nite. Why? What happens if it is not? Checking for this property is easy.\n> library(corpcor)\n> is . positive . definite (cv)\n1\n[ ] TRUE\n> is . positive . definite (x)\nError in eigen(m, only. values = TRUE) :\nnon square matrix in ’eigen ’\n−\n> is . positive . definite (x %*% t(x))\n1\n[ ] FALSE\nWhat happens if you compute pairwise covariances from differing\nlengths of data for each pair?\n3.4 Descriptive Statistics\nLet’s revisit the same data and compute various descriptive statistics.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 59,
      "chunk_index": 2
    }
  },
  {
    "text": "Read a CSV data file into R as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 59,
      "chunk_index": 3
    }
  },
  {
    "text": "60 data science: theories, models, algorithms, and analytics\n> data = read.csv(\"goog.csv\" ,header=TRUE) #Read in the data\n> n = dim(data)[ 1 ]\n> n\n1 1671\n[ ]\n> data = data[n: 1 ,]\n> dim(data)\n1 1671 7\n[ ]\n> s = data[ , 7 ]\nSo we now have the stock data in place, and we can compute daily\nreturns, and then convert those returns into annualized returns.\n> rets = log(s[ 2 :n] /s [ 1 :(n 1 )])\n−\n> rets_annual = rets* 252\n> print(c(mean( rets ) ,mean( rets_annual )))\n1 0 001044538 0 263223585\n[ ] . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 60,
      "chunk_index": 0
    }
  },
  {
    "text": "1 0 001044538 0 263223585\n[ ] . .\nCompute the daily and annualized standard deviation of returns.\n> r_sd = sd( rets )\n> r_sd_annual = r_sd*sqrt( 252 )\n> print(c(r_sd, r_sd_annual))\n1 0 02266823 0 35984704\n[ ] . .\n> #What if we take the stdev of\nannualized returns?\n> print(sd( rets* 252 ))\n1 5 712395\n[ ] .\n> #Huh?\n>\n> print(sd( rets* 252 )) /252\n1 5 712395\n[ ] .\n1 0 02266823\n[ ] .\n> print(sd( rets* 252 )) / sqrt( 252 )\n1 5 712395\n[ ] .\n1 0 3598470\n[ ] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 60,
      "chunk_index": 1
    }
  },
  {
    "text": "1 5 712395\n[ ] .\n1 0 3598470\n[ ] .\nNotice the interesting use of the print function here. The variance is\neasy as well.\n> #Variance\n> r_var = var( rets )\n> r_var_annual = var( rets )* 252\n> print(c(r_var , r_var_annual))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 60,
      "chunk_index": 2
    }
  },
  {
    "text": "open source: modeling in r 61\n1 0 0005138488 0 1294898953\n[ ] . .\n3.5 Higher-Order Moments\nSkewness and kurtosis are key moments that arise in all return distri-\nbutions. We need a different library in R for these. We use the moments\nlibrary.\nE[(X µ)3]\nSkewness = −\nσ3\nSkewness means one tail is fatter than the other (asymmetry). Fatter\nright (left) tail implies positive (negative) skewness.\nE[(X µ)4]\nKurtosis = −\nσ4\nKurtosis means both tails are fatter than with a normal distribution.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 61,
      "chunk_index": 0
    }
  },
  {
    "text": "> library(moments)\n> skewness( rets )\n1 0 4874792\n[ ] .\n> kurtosis( rets )\n1 9 955916\n[ ] .\n3\nFor the normal distribution, skewness is zero, and kurtosis is . Kurtosis\nminus three is denoted “excess kurtosis”.\n> skewness(rnorm( 1000000 ))\n1 0 00063502\n[ ] .\n−\n> kurtosis(rnorm( 1000000 ))\n1 3 005863\n[ ] .\n500\nWhat is the skewness and kurtosis of the stock index (S&P )?\n3.6 Quick Introduction to Brownian Motions with R\nThe law of motion for stocks is often based on a geometric Brownian",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 61,
      "chunk_index": 1
    }
  },
  {
    "text": "motion, i.e.,\ndS(t) = µS(t) dt+σS(t) dB(t), S(0) = S .\n0\nThis is a “stochastic” differential equation (SDE), because it describes\nrandom movement of the stock S(t). The coefficient µ determines the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 61,
      "chunk_index": 2
    }
  },
  {
    "text": "62 data science: theories, models, algorithms, and analytics\ndrift of the process, and σ determines its volatility. Randomness is in-\njected by Brownian motion B(t). This is more general than a deter-\nministic differential equation that is only a function of time, as with\na bank account, whose accretion is based on the differential equation\ndy(t) = ry(t)dt, where r is the risk-free rate of interest.\nThe solution to a SDE is not a deterministic function but a random",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 62,
      "chunk_index": 0
    }
  },
  {
    "text": "function. In this case, the solution for time interval h is known to be\n(cid:20)(cid:18) (cid:19) (cid:21)\n1\nS(t+h) = S(t)exp µ σ2 h+σB(h)\n− 2\nThe presence of B(h) N(0,h) in the solution makes the function ran-\n∼ (cid:112)\ndom. We may also write B(h) as the random variable (cid:101) (h) N(0,h),\n∼\nwhere (cid:101) N(0,1). The presence of the exponential return makes the\n∼\nstock price lognormal. (Note: if r.v. x is normal, then ex is lognormal.)\nRe-arranging, the stock return is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 62,
      "chunk_index": 1
    }
  },
  {
    "text": "Re-arranging, the stock return is\n(cid:18) (cid:19) (cid:20)(cid:18) (cid:19) (cid:21)\nS(t+h) 1\nR(t+h) = ln N µ σ2 h,σ2h\nS(t) ∼ − 2\nUsing properties of the lognormal distribution, the conditional mean of\nthe stock price becomes\nE[S(t+h) S(t)] = S(t) eµh\n| ·\nThe following R code computes the annualized volatility σ.\n> h =\n1/252\n> sigma = sd( rets ) / sqrt(h)\n> sigma\n1 0 3598470\n[ ] .\nThe parameter µ is also easily estimated as\n> mu = mean( rets ) /h+ 0 . 5 *sigma^ 2\n> mu\n1 0 3279685\n[ ] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 62,
      "chunk_index": 2
    }
  },
  {
    "text": "> mu\n1 0 3279685\n[ ] .\nSo the additional term 1σ2 does matter substantially.\n2\n3.7 Estimation using maximum-likelihood\nMLE estimation requires finding the parameters µ,σ that maximize\n{ }\nthe likelihood of seeing the empirical sequence of returns R(t). A prob-\nability function is required, and we have one above for R(t), which is\ni.i.d.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 62,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 63\nFirst, a quick recap of the normal distribution. If x N(µ,σ2), then\n∼\n1\n(cid:20)\n1(x\nµ)2(cid:21)\ndensity function: f(x) = exp −\n√2πσ2 −2 σ2\nN(x) = 1 N( x)\n− −\n(cid:90) x\nF(x) = f(u)du\n∞\n−\nThe standard normal distribution is x N(0,1). For the standard nor-\n∼\nmal distribution: F(0) = 1.\n2\nThe probability density of R(t) is normal with the following equation:\n1\n(cid:20)\n1 (R(t)\nα)2(cid:21)\nf[R(t)] = exp −\n√2πσ2h · −2 · σ2h\n(cid:16) (cid:17)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 63,
      "chunk_index": 0
    }
  },
  {
    "text": "√2πσ2h · −2 · σ2h\n(cid:16) (cid:17)\nwhere α = µ 1σ2 h. For periods t = 1,2,...,T the likelihood of the\n− 2\nentire series is\nT\n∏\nf[R(t)]\nt=1\nIt is easier (computationally) to maximize\nT\n∑\nmax ln f[R(t)]\nµ,σ L ≡\nt=1\nknown as the log-likelihood. This is easily done in R. First we create the\nlog-likelihood function\n> LL = function(params, rets ) {\n1 2\n+ alpha = params[ ]; sigsq = params[ ]\n+ logf = log(sqrt( 2 *pi*sigsq ))\n−\n(rets alpha)^\n2/\n(\n2\n*sigsq)\n− −\n+ LL = sum(logf)\n−\n+ }\nNote that",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 63,
      "chunk_index": 1
    }
  },
  {
    "text": "(\n2\n*sigsq)\n− −\n+ LL = sum(logf)\n−\n+ }\nNote that\n[R(t) α]2\nln f[R(t)] = ln √ 2πσ2h −\n− − 2σ2h\nWe have used variable sigsq in function LL for σ2h.\nWe then go ahead and do the MLE using the nlm (non-linear mini-\nmization) package in R. It uses a Newton-type algorithm.\n> #create starting guess for parameters\n> params = c( 0 . 001 , 0 . 001 )\n> res = nlm(LL,params, rets )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 63,
      "chunk_index": 2
    }
  },
  {
    "text": "64 data science: theories, models, algorithms, and analytics\n> res\n$minimum\n1 3954 813\n[ ] .\n−\n$estimate\n1 0 0010441526 0 0005130404\n[ ] . .\n$gradient\n1 0 3728092 3 2397043\n[ ] . .\n−\n$code\n1 2\n[ ]\n$iterations\n1 12\n[ ]\nWe now pick off the results and manipulate them to get the annualized\nparameters µ,σ .\n{ }\n> alpha = res$estimate [ 1 ]\n> sigsq = res$estimate [ 2 ]\n> sigma = sqrt(sigsq /h)\n> sigma\n1 0 3595639\n[ ] .\n> mu = alpha/h + 0 . 5 *sigma^ 2\n> mu\n1 0 3277695\n[ ] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 64,
      "chunk_index": 0
    }
  },
  {
    "text": "> mu\n1 0 3277695\n[ ] .\nWe see that the estimated parameters are close to that derived earlier.\n3.8 GARCH/ARCH Models\nGARCH stands for “Generalized Auto- Regressive Conditional Het-\neroskedasticity”. Rob Engle invented ARCH (for which he got the Nobel\nprize) and this was extended by Tim Bollerslev to GARCH.\nARCH models are based on the idea that volatility tends to cluster,\ni.e., volatility for period t, is auto-correlated with volatility from period",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 64,
      "chunk_index": 1
    }
  },
  {
    "text": "(t 1), or more preceding periods. If we had a time series of stock re-\n−\nturns following a random walk, we might model it as follows\nr = µ+e , e N(0,σ2)\nt t t ∼ t\nIf the variance were stationary then σ2 would be constant. But under\nt\nARCH it is auto-correlated with previous variances. Hence, we have\np q\nσ2 = β + ∑ β σ2 + ∑ β e2\nt 0 1j t j 2k t k\nj=1 − k=1 −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 64,
      "chunk_index": 2
    }
  },
  {
    "text": "open source: modeling in r 65\nSo current variance (σ2) depends on past squared shocks (e2 ) and past\nt t k\nvariances (σ2 ). The number of lags of past variance is p, an − d that of\nt j\n−\nlagged shocks is q. The model is thus known as a GARCH(p,q) model.\nFor the model to be stationary, the sum of all the β terms should be less\n1\nthan .\nIn GARCH, stock returns are conditionally normal, and independent,\nbut not identically distributed because the variance changes over time.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 65,
      "chunk_index": 0
    }
  },
  {
    "text": "Since at every time t, we know the conditional distribution of returns,\nbecause σ is based on past σ and past shocks e , we can estimate\nt t j t k\n− −\nthe parameters β ,β1j,β , j,k, of the model using MLE. The good\n0 2k\n{ } ∀\nnews is that this comes canned in R, so all we need to do is use the\ntseries package.\n> library( tseries )\n> res = garch(rets ,order=c( 1 , 1 ))\n> summary(res)\nCall :\ngarch(x = rets , order = c( 1 , 1 ))\nModel:\n1 1\nGARCH( , )\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 65,
      "chunk_index": 1
    }
  },
  {
    "text": "1 1\nGARCH( , )\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax\n5 54354 0 45479 0 03512 0 57051 7 40088\n. . . . .\n− −\nCoefficient (s ):\nEstimate Std. Error t value Pr(>|t|)\n0 5 568 06 8 803 07 6 326 2 52 10\na . e . e . . e ***\n− − −\n1 4 294 02 4 622 03 9 289 2 16\na . e . e . < e ***\n− − −\n1 9 458 01 5 405 03 174 979 2 16\nb . e . e . < e ***\n− − −\n−−−\nSignif . codes: 0 [***] 0 . 001 [**] 0 . 01 [*] 0 . 05 [.] 0 . 1 [ ]\nDiagnostic Tests : Jarque Bera Test\ndata: Residuals\nX squared = 3007 . 311 , df = 2 ,\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 65,
      "chunk_index": 2
    }
  },
  {
    "text": "X squared = 3007 . 311 , df = 2 ,\n−\n2 2 16\np value < . e\n− −\nBox Ljung test\n−\ndata: Squared. Residuals\nX squared = 0 . 5305 , df = 1 , p value = 0 . 4664\n− −\nNotice how strikingly high the t-statistics are. What is volatility related",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 65,
      "chunk_index": 3
    }
  },
  {
    "text": "66 data science: theories, models, algorithms, and analytics\nto mostly? Is the model stationary?\n3.9 Introduction to Monte Carlo\nIt is easy to simulate a path of stock prices using a discrete form of the\nsolution to the Geometric Brownian motion SDE. This is the equation of\nmotion for the stock price, which randomly moves the stock price from\nits previous value S(t) to the value h years ahead, S(t+h).\n(cid:20)(cid:18) (cid:19) (cid:21)\n1\nS(t+h) = S(t)exp µ σ2 h+σ e √h\n− 2 ·",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 66,
      "chunk_index": 0
    }
  },
  {
    "text": "1\nS(t+h) = S(t)exp µ σ2 h+σ e √h\n− 2 ·\nNote that we replaced B(h) with e√h, where e N(0,1). Both B(h)\n∼\nand e√h have mean zero and variance h. Knowing S(t), we can simulate\nS(t+h) by drawing e from a standard normal distribution. Here is the R\ncode to run the entire simulation.\n252\n> n =\n0 100\n> s =\n0 10\n> mu = .\n0 20\n> sig = .\n> s = matrix( 0 , 1 ,n+ 1 )\n> h=\n1/n\n>\n1 0\n> s[ ] = s\n> for ( j in 2 :(n+ 1 )) {\n+ s[ j]=s[j 1 ]*exp((mu sig^ 2/ 2 )*h\n− −\n+sig*rnorm( 1 )*sqrt(h))\n+ }\n1 5\n> s[ : ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 66,
      "chunk_index": 1
    }
  },
  {
    "text": "− −\n+sig*rnorm( 1 )*sqrt(h))\n+ }\n1 5\n> s[ : ]\n1 100 00000 99 54793 96 98941\n[ ] . . .\n98 65440 98 76989\n. .\n4\n> s[(n ):n]\n−\n1 87 01616 86 37163 84 92783\n[ ] . . .\n84 17420 86 16617\n. .\n> plot(t(s) ,type=\"l\")\n31\nThis program generates the plot shown in Figure . .\nThe same logic may be used to generate multiple paths of stock prices,\nin a vectorized way as follows. In the following example we generate\n3\npaths. Because of the vectorization, the run time does not increase",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 66,
      "chunk_index": 2
    }
  },
  {
    "text": "linearly with the number of paths, and in fact, hardly increases at all.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 66,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 67\n0 50 100 150 200 250\n511\n011\n501\n001\n59\n09\n58\nIndex\n)s(t\nFigure3.1: Singlestockpathplot\nsimulatedfromaBrownianmotion.\n> s = matrix( 0 , 3 ,n+ 1 )\n1 0\n> s[ , ] = s\n> for ( j in seq( 2 ,(n+ 1 ))) {\n+ s[ , j]=s[ , j 1 ]*exp((mu sig^ 2/ 2 )*h\n− −\n+sig*matrix(rnorm( 3 ) , 3 , 1 )*sqrt(h))\n+ }\n> plot(t(s)[ , 1 ] ,ylim=c(ymin,ymax) ,type=\"l\")\n> lines(t(s)[ , 2 ] , col=\"red\" , lty= 2 )\n> lines(t(s)[ , 3 ] , col=\"blue\" , lty= 3 )\n32",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 67,
      "chunk_index": 0
    }
  },
  {
    "text": "> lines(t(s)[ , 3 ] , col=\"blue\" , lty= 3 )\n32\nThe plot is shown in Figure . . The plot code shows how to change the\nstyle of the path and its color.\nIf you generate many more paths, how can you find the probability\nof the stock ending up below a defined price? Can you do this directly\nfrom the discrete version of the Geometric Brownian motion process\nabove?",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 67,
      "chunk_index": 1
    }
  },
  {
    "text": "68 data science: theories, models, algorithms, and analytics\n0 50 100 150 200 250\n041\n021\n001\n08\nIndex\n]1\n,[)s(t\nFigure3.2: Multiplestockpathplot\nsimulatedfromaBrownianmotion.\nBivariate random variables\nTo convert two independent random variables (e ,e ) N(0,1) into two\n1 2\n∼\ncorrelated random variables (x ,x ) with correlation ρ, use the following\n1 2\ntransformations.\n(cid:113)\nx = e , x = ρ e + 1 ρ2 e\n1 1 2 1 2\n· − ·\n10000\nWe can now generate , pairs of correlated random variates using",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 68,
      "chunk_index": 0
    }
  },
  {
    "text": "the following R code.\n> e = matrix(rnorm( 20000 ) , 10000 , 2 )\n> cor(e)\n1 2\n[ , ] [ , ]\n1 1 000000000 0 007620184\n[ ,] . .\n2 0 007620184 1 000000000\n[ ,] . .\n> cor(e[ , 1 ] ,e[ , 2 ])\n1 0 007620184\n[ ] .\n0 6\n> rho = .\n1 1\n> x = e[ , ]\n> x 2 = rho*e[ , 1 ]+sqrt( 1 rho^ 2 )*e[ , 2 ]\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 68,
      "chunk_index": 1
    }
  },
  {
    "text": "open source: modeling in r 69\n> cor(x 1 ,x 2 )\n1 0 5981845\n[ ] .\nIt is useful to check algebraically that E[x ] = 0,i = 1,2, Var[x ] = 1,i =\ni i\n1,2. Also check that Cov[x ,x ] = ρ = Corr[x ,x ]. We can numerically\n1 2 1 2\ncheck this using the following:\n> mean(x 1 )\n1 0 006522788\n[ ] .\n−\n> mean(x 2 )\n1 0 00585042\n[ ] .\n−\n> var(x 1 )\n1 0 9842857\n[ ] .\n> var(x 2 )\n1 1 010802\n[ ] .\n> cov(x 1 ,x 2 )\n1 0 5966626\n[ ] .\nMultivariate random variables",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 69,
      "chunk_index": 0
    }
  },
  {
    "text": "1 0 5966626\n[ ] .\nMultivariate random variables\nThese are generated using Cholesky decomposition which is a ma-\ntrix operation that represents a covariance matrix as a product of two\nmatrices. We may write a covariance matrix in decomposed form, i.e.,\nΣ = L L , where L is a lower triangular matrix. Alternatively we might\n(cid:48)\nhave an upper triangular decomposition, where U = L . Think of each\n(cid:48)\ncomponent of the decomposition as a square-root of the covariance ma-\ntrix.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 69,
      "chunk_index": 1
    }
  },
  {
    "text": "trix.\nThe Cholesky decomposition is very useful in generating correlated\nrandom numbers from a distribution with mean vector µ and covari-\nance matrix Σ . Suppose we have a scalar random variable e (0,1).\n∼\nTo transform this variate into x (µ,σ2), we generate e and then set\n∼\nx = µ + σe. If instead of a scalar random variable, we have a vec-\ntor random variables (independent of each other) given by a vector\ne = [e ,e ,...,e ] (0,I), then we may transform this into a vector\n1 2 n (cid:62)\n∼",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 69,
      "chunk_index": 2
    }
  },
  {
    "text": "1 2 n (cid:62)\n∼\nof correlated random variables x = [x ,x ,...,x ] (µ, Σ) by comput-\n1 2 n (cid:62)\n∼\ning:\nx = µ+Le\nThis is implemented using the following code.\n> #Original matrix",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 69,
      "chunk_index": 3
    }
  },
  {
    "text": "70 data science: theories, models, algorithms, and analytics\n> cv\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 01 0 00 0 00\n[ ,] . . .\n2 0 00 0 04 0 02\n[ ,] . . .\n3 0 00 0 02 0 16\n[ ,] . . .\n> #Let ’s enhance it\n1 2 0 005\n> cv[ , ]= .\n2 1 0 005\n> cv[ , ]= .\n1 3 0 005\n> cv[ , ]= .\n3 1 0 005\n> cv[ , ]= .\n> cv\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 010 0 005 0 005\n[ ,] . . .\n2 0 005 0 040 0 020\n[ ,] . . .\n3 0 005 0 020 0 160\n[ ,] . . .\n> L = t(chol(cv))\n> L\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 10 0 00000000 0 0000000\n[ ,] . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 70,
      "chunk_index": 0
    }
  },
  {
    "text": "1 0 10 0 00000000 0 0000000\n[ ,] . . .\n2 0 05 0 19364917 0 0000000\n[ ,] . . .\n3 0 05 0 09036961 0 3864367\n[ ,] . . .\n> e=matrix(randn( 3 * 10000 ), 10000 , 3 )\n> x = t(L %*% t(e))\n> dim(x)\n1 10000 3\n[ ]\n> cov(x)\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 009872214 0 004597322 0 004521752\n[ ,] . . .\n2 0 004597322 0 040085503 0 019114981\n[ ,] . . .\n3 0 004521752 0 019114981 0 156378078\n[ ,] . . .\n>\nIn the last calculation, we confirmed that the simulated data has the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 70,
      "chunk_index": 1
    }
  },
  {
    "text": "same covariance matrix as the one that we generated correlated random\nvariables from.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 70,
      "chunk_index": 2
    }
  },
  {
    "text": "open source: modeling in r 71\n3.10 Portfolio Computations in R\nLet’s enter a sample mean vector and covariance matrix and then using\nsome sample weights, we will perform some basic matrix computations\nfor portfolios to illustrate the use of R.\n> mu = matrix(c( 0 . 01 , 0 . 05 , 0 . 15 ) , 3 , 1 )\n> cv = matrix(c( 0 . 01 , 0 , 0 , 0 , 0 . 04 , 0 . 02 ,\n0 0 02 0 16 3 3\n, . , . ) , , )\n> mu\n1\n[ , ]\n1 0 01\n[ ,] .\n2 0 05\n[ ,] .\n3 0 15\n[ ,] .\n> cv\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 01 0 00 0 00\n[ ,] . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 71,
      "chunk_index": 0
    }
  },
  {
    "text": "[ , ] [ , ] [ , ]\n1 0 01 0 00 0 00\n[ ,] . . .\n2 0 00 0 04 0 02\n[ ,] . . .\n3 0 00 0 02 0 16\n[ ,] . . .\n> w = matrix(c( 0 . 3 , 0 . 3 , 0 . 4 ))\n> w\n1\n[ , ]\n1 0 3\n[ ,] .\n2 0 3\n[ ,] .\n3 0 4\n[ ,] .\n> muP = t(w) %*% mu\n> muP\n1\n[ , ]\n1 0 078\n[ ,] .\n> stdP = sqrt(t(w) %*% cv %*% w)\n> stdP\n1\n[ , ]\n1 0 1868154\n[ ,] .\nWe thus generated the expected return and risk of the portfolio, i.e., the\nvalues 0.078 and 0.187, respectively.\nWe are interested in the risk of a portfolio, often measured by its",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 71,
      "chunk_index": 1
    }
  },
  {
    "text": "variance. As we had seen in the previous chapter, as we increase n, the\nnumber of securities in the portfolio, the variance keeps dropping, and\nasymptotes to a level equal to the average covariance of all the assets. It",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 71,
      "chunk_index": 2
    }
  },
  {
    "text": "72 data science: theories, models, algorithms, and analytics\nis interesting to see what happens as n increases through a very simple\nfunction in R that returns the standard deviation of the portfolio.\n> sigport = function(n, sig_i 2 , sig_ ij ) {\n+ cv = matrix(sig_ij ,n,n)\n+ diag(cv) = sig_i 2\n+ w = matrix( 1 /n,n, 1 )\n+ result = sqrt(t(w) %*% cv %*% w)\n+ }\n>\n> n = seq( 5 , 100 , 5 )\n> n\n1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85\n[ ]\n18 90 95 100\n[ ]\n> risk_n = NULL\n> for (nn in n) {",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 72,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ]\n> risk_n = NULL\n> for (nn in n) {\n+ risk_n = c( risk_n, sigport(nn, 0 . 04 , 0 . 01 ))\n+ }\n> risk_n\n1 0 1264911 0 1140175 0 1095445\n[ ] . . .\n0 1072381 0 1058301 0 1048809\n. . .\n7 0 1041976 0 1036822 0 1032796\n[ ] . . .\n0 1029563 0 1026911 0 1024695\n. . .\n13 0 1022817 0 1021204 0 1019804\n[ ] . . .\n0 1018577 0 1017494 0 1016530\n. . .\n19 0 1015667 0 1014889\n[ ] . .\n>\nWe can plot this to see the classic systematic risk plot. This is shown in\n33\nFigure . .\n> plot(n, risk_n,type=\"l\" ,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 72,
      "chunk_index": 1
    }
  },
  {
    "text": "33\nFigure . .\n> plot(n, risk_n,type=\"l\" ,\nylab=\"Portfolio Std Dev\")\n3.11 Finding the Optimal Portfolio\nWe will review the notation one more time. Assume that the risk free\nasset has return r . And we have n risky assets, with mean returns\nf\nµ ,i = 1...n. We need to invest in optimal weights w in each asset. Let\ni i\nw = [w ,...,w ] be a column vector of portfolio weights. We define\n1 n (cid:48)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 72,
      "chunk_index": 2
    }
  },
  {
    "text": "open source: modeling in r 73\n20 40 60 80 100\n521.0\n021.0\n511.0\n011.0\n501.0\nn\nveD\ndtS\noiloftroP\nFigure3.3: Systematicriskasthe\nnumberofstocksintheportfolio\nincreases.\nµ = [µ ,...,µ ] be the column vector of mean returns on each asset, and\n1 n (cid:48)\n1 = [1,...,1] be a column vector of ones. Hence, the expected return on\n(cid:48)\nthe portfolio will be\nE(R\np\n) = (1 w\n(cid:48)\n1)r\nf\n+w\n(cid:48)\nµ\n−\nThe variance of return on the portfolio will be\nVar(R ) = w Σw\np (cid:48)\nΣ",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 73,
      "chunk_index": 0
    }
  },
  {
    "text": "Var(R ) = w Σw\np (cid:48)\nΣ\nwhere is the covariance matrix of returns on the portfolio. The objec-\ntive function is a trade-off between return and risk, with β modulating\nthe balance between risk and return:\nβ\nU(R ) = r +w (µ r 1) w Σw\np f (cid:48) f (cid:48)\n− − 2\nThe f.o.c. becomes a system of equations now (not a single equation),\nsince we differentiate by an entire vector w:\ndU\n= µ r 1 β Σw = 0\nf\ndw − −\n(cid:48)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 73,
      "chunk_index": 1
    }
  },
  {
    "text": "74 data science: theories, models, algorithms, and analytics\nwhere the RHS is a vector of zeros of dimension n. Solving we have\n1\nw = Σ 1(µ r 1)\n− f\nβ −\nTherefore, allocation to the risky assets\n• Increases when the relative return to it (µ r 1) increases.\nf\n−\n• Decreases when risk aversion increases.\nΣ\n• Decreases when riskiness of the assets increases as proxied for by .\n3\n> n=\n> cv\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 01 0 00 0 00\n[ ,] . . .\n2 0 00 0 04 0 02\n[ ,] . . .\n3 0 00 0 02 0 16\n[ ,] . . .\n> mu",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 74,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ,] . . .\n3 0 00 0 02 0 16\n[ ,] . . .\n> mu\n1\n[ , ]\n1 0 01\n[ ,] .\n2 0 05\n[ ,] .\n3 0 15\n[ ,] .\n> rf= 0 . 005\n> beta = 4\n> wuns = matrix( 1 ,n, 1 )\n> wuns\n1\n[ , ]\n1 1\n[ ,]\n2 1\n[ ,]\n3 1\n[ ,]\n> w = ( 1 /beta)*(solve(cv) %*% (mu rf*wuns))\n−\n> w\n1\n[ , ]\n1 0 1250000\n[ ,] .\n2 0 1791667\n[ ,] .\n3 0 2041667\n[ ,] .\n> w_in_rf = 1 sum(w)\n−\n> w_in_rf\n1 0 4916667\n[ ] .\nWhat if we reduced beta?",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 74,
      "chunk_index": 1
    }
  },
  {
    "text": "open source: modeling in r 75\n> beta = 3\n> w = ( 1 /beta)*(solve(cv) %*% (mu rf*wuns));\n−\n> w\n1\n[ , ]\n1 0 1666667\n[ ,] .\n2 0 2388889\n[ ,] .\n3 0 2722222\n[ ,] .\n> beta = 2\n> w = ( 1 /beta)*(solve(cv) %*% (mu rf*wuns));\n−\n> w\n1\n[ , ]\n1 0 2500000\n[ ,] .\n2 0 3583333\n[ ,] .\n3 0 4083333\n[ ,] .\nNotice that the weights in stocks scales linearly with β. The relative\nproportions of the stocks themselves remains constant. Hence, β modu-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 75,
      "chunk_index": 0
    }
  },
  {
    "text": "lates the proportions invested in a risk-free asset and a stock portfolio,\nin which stock proportions remain same. It is as if the stock versus bond\ndecision can be taken separately from the decision about the composi-\ntion of the stock portfolio. This is known as the “two-fund separation”\nproperty, i.e., first determine the proportions in the bond fund vs stock\nfund and the allocation within each fund can be handled subsequently.\n3.12 Root Solving",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 75,
      "chunk_index": 1
    }
  },
  {
    "text": "3.12 Root Solving\nFinding roots of nonlinear equations is often required, and R has several\npackages for this purpose. Here we examine a few examples.\nSuppose we are given the function\n(x2+y2 1)3 x2y3 = 0\n− −\nand for various values of y we wish to solve for the values of x. The\nfunction we use is called fn and the use of the function is shown below.\nlibrary(rootSolve)\nfn = function(x,y) {\n2 2 1 3 2 3\nresult = (x^ +y^ )^ x^ *y^\n− −\n}",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 75,
      "chunk_index": 2
    }
  },
  {
    "text": "76 data science: theories, models, algorithms, and analytics\n1\nyy =\nsol = multiroot(f=fn , start= 1 ,maxiter= 10000 ,rtol = 0 . 000001 ,\n0 0000001 0 00001\natol= . , ctol = . ,y=yy)\nprint(sol)\ncheck = fn(sol$root ,yy)\nprint(check)\nAt the end we check that the equation has been solved. Here is the code\nrun:\n> source(\"fn.R\")\n$root\n1 1\n[ ]\n$f . root\n1 0\n[ ]\n$iter\n1 1\n[ ]\n$estim . precis\n1 0\n[ ]\n1 0\n[ ]\nHere is another example, where we solve a single unknown using the\nunroot.all function.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 76,
      "chunk_index": 0
    }
  },
  {
    "text": "unroot.all function.\nlibrary(rootSolve)\nfn = function(x) {\n0 065 1 0 5 0 05 0 05\nresult = . *(x*( x))^ . . + . *x\n− −\n}\nsol = uniroot . all (f=fn ,c( 0 , 1 ))\nprint(sol)\nThe function searches for a solution (root) in the range [0,1]. The answer\nis given as:\n1 1 0000000 0 3717627\n[ ] . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 76,
      "chunk_index": 1
    }
  },
  {
    "text": "open source: modeling in r 77\n3.13 Regression\nIn a multivariate linear regression, we have\nY = X β+e\n·\nwhere Y Rt 1, X Rt n, and β Rn 1, and the regression solution is\n× × ×\n∈ ∈ ∈\nsimply equal to β = (X X) 1(X Y) Rn 1.\n(cid:48) − (cid:48) ×\n∈\nTo get this result we minimize the sum of squared errors.\nmine\n(cid:48)\ne = (Y X β)\n(cid:48)\n(Y X β)\nβ − · − ·\n= Y (Y X β) (Xβ) (Y X β)\n(cid:48) (cid:48)\n− · − · − ·\n= Y Y Y Xβ (β X )Y+β X Xβ\n(cid:48) (cid:48) (cid:48) (cid:48) (cid:48) (cid:48)\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 77,
      "chunk_index": 0
    }
  },
  {
    "text": "− −\n= Y Y Y Xβ Y Xβ+β X Xβ\n(cid:48) (cid:48) (cid:48) (cid:48) (cid:48)\n− −\n= Y\n(cid:48)\nY 2Y\n(cid:48)\nXβ+β\n(cid:48)\nX\n(cid:48)\nXβ\n−\nNote that this expression is a scalar. Differentiating w.r.t. β gives the\n(cid:48)\nfollowing f.o.c:\n2X\n(cid:48)\nY+2X\n(cid:48)\nXβ = 0\n−\n=\n⇒\nβ = (X X) 1(X Y)\n(cid:48) − (cid:48)\nThere is another useful expression for each individual β =\nCov(X i,Y)\n. You\ni Var(X)\ni\nshould compute this and check that each coefficient in the regression is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 77,
      "chunk_index": 1
    }
  },
  {
    "text": "indeed equal to the β from this calculation.\ni\nExample: Let’s do a regression and see whether AAPL, CSCO, and IBM\ncan explain the returns of YHOO. This uses the data we had down-\nloaded earlier.\n> dim( rets )\n1 2017 4\n[ ]\n> Y = as.matrix( rets [ , 1 ])\n> X = as.matrix( rets [ , 2 : 4 ])\n> n = length(Y)\n> X = cbind(matrix( 1 ,n, 1 ) ,X)\n> b = solve(t(X) %*% X) %*% (t(X) %*% Y)\n> b\n1\n[ , ]\n3 139183 06\n. e\n−\n1 854781 01\nAAPL.Adjusted . e\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 77,
      "chunk_index": 2
    }
  },
  {
    "text": "78 data science: theories, models, algorithms, and analytics\n3 069011 01\nCSCO.Adjusted . e\n−\n3 117553 01\nIBM.Adjusted . e\n−\nBut of course, R has this regression stuff canned, and you do not need to\nhassle with the Matrix (though the movie is highly recommended).\n> X = as.matrix( rets [ , 2 : 4 ])\n> res = lm(Y~X)\n> summary(res)\nCall :\nlm(formula = Y ~ X)\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax\n0 18333 0 01051 0 00044 0 00980 0 38288\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error t value Pr(>|t|)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 78,
      "chunk_index": 0
    }
  },
  {
    "text": "Estimate Std. Error t value Pr(>|t|)\n3 139 06 5 091 04 0 006 0 995\n( Intercept ) . e . e . .\n− −\n1 855 01 2 780 02 6 671 3 28 11\nXAAPL.Adjusted . e . e . . e ***\n− − −\n3 069 01 3 244 02 9 462 2 16\nXCSCO.Adjusted . e . e . < e ***\n− − −\n3 118 01 4 517 02 6 902 6 82 12\nXIBM.Adjusted . e . e . . e ***\n− − −\n−−−\nSignif . codes: 0 ï£¡***ï£¡ 0 . 001 ï£¡**ï£¡ 0 . 01 ï£¡*ï£¡ 0 . 05 ï£¡ . ï£¡ 0 . 1 ï£¡ ï£¡ 1\nResidual standard error : 0 . 02283 on 2013 degrees of freedom",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 78,
      "chunk_index": 1
    }
  },
  {
    "text": "Multiple R squared: 0 . 2236 , Adjusted R squared: 0 . 2224\n− −\nF statistic : 193 . 2 on 3 and 2013 DF, p value: < 2 . 2 e 16\n− − −\nFor visuals, do see the abline() function as well.\n2005 06\nHere is a simple regression run on some data from the - NCAA\nbasketball season for the March madness stats. The data is stored in a\nspace-delimited file called ncaa.txt. We use the metric of performance\nto be the number of games played, with more successful teams playing",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 78,
      "chunk_index": 2
    }
  },
  {
    "text": "more playoff games, and then try to see what variables explain it best.\nWe apply a simple linear regression that uses the R command lm, which\nstands for “linear model.”\n> ncaa = read. table(\"ncaa. txt\" ,header=TRUE)\n3\n> y = ncaa[ ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 78,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 79\n> y = as.matrix(y)\n4 14\n> x = ncaa[ : ]\n> x = as.matrix(x)\n> fm = lm(y~x)\n> res = summary(fm)\n> res\nCall :\nlm(formula = y ~ x)\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax\n1 5075 0 5527 0 2454 0 6705 2 2344\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error t value Pr(>|t|)\n10 194804 2 892203 3 525 0 000893\n( Intercept ) . . . . ***\n− −\n0 010442 0 025276 0 413 0 681218\nxPTS . . . .\n− −\n0 105048 0 036951 2 843 0 006375\nxREB . . . . **\n0 060798 0 091102 0 667 0 507492",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 79,
      "chunk_index": 0
    }
  },
  {
    "text": "xREB . . . . **\n0 060798 0 091102 0 667 0 507492\nxAST . . . .\n− −\n0 034545 0 071393 0 484 0 630513\nxTO . . . .\n− −\n1 325402 1 110184 1 194 0 237951\nxA.T . . . .\n0 181015 0 068999 2 623 0 011397\nxSTL . . . . *\n0 007185 0 075054 0 096 0 924106\nxBLK . . . .\n0 031705 0 044469 0 713 0 479050\nxPF . . . .\n− −\n13 823190 3 981191 3 472 0 001048\nxFG . . . . **\n2 694716 1 118595 2 409 0 019573\nxFT . . . . *\n3 2 526831 1 754038 1 441 0 155698\nxX P . . . .\n−−−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 79,
      "chunk_index": 1
    }
  },
  {
    "text": "xX P . . . .\n−−−\nSignif . codes: 0 *** 0 . 001 ** 0 . 01 * 0 . 05 . 0 . 1\nResidual standard error : 0 . 9619 on 52 degrees of freedom\nMultiple R Squared: 0 . 5418 , Adjusted R squared: 0 . 4448\n− −\nF statistic : 5 . 589 on 11 and 52 DF, p value: 7 . 889 e 06\n− − −\nWe note that the command lm returns an “object” with name res. This\nobject contains various details about the regression result, and can then\nbe called by other functions that will format and present various ver-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 79,
      "chunk_index": 2
    }
  },
  {
    "text": "sions of the result. For example, using the following command gives a",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 79,
      "chunk_index": 3
    }
  },
  {
    "text": "80 data science: theories, models, algorithms, and analytics\nnicely formatted version of the regression output, and you should try to\nuse it when presenting regression results.\nAn alternative approach using data frames is:\n> ncaa_data_frame = data.frame(y=as.matrix(ncaa[ 3 ]) ,\nx=as.matrix(ncaa[ 4 : 14 ]))\n> fm = lm(y~x,data=ncaa_data_frame)\n> summary(fm)\n(The output is not shown here in order to not repeat what we saw in the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 80,
      "chunk_index": 0
    }
  },
  {
    "text": "previous regression.) Data frames are also objects. Here, objects are used\nin the same way as the term is used in object-oriented programming\n(OOP), and in a similar fashion, R supports OOP as well.\nDirect regression implementing the matrix form is as follows (we had\nderived this earlier):\n> wuns = matrix( 1 , 64 , 1 )\n> z = cbind(wuns,x)\n> b = inv(t(z) %*% z) %*% (t(z) %*% y)\n> b\nGMS\n10 194803524\n.\n−\n0 010441929\nPTS .\n−\n0 105047705\nREB .\n0 060798192\nAST .\n−\n0 034544881\nTO .\n−\n1 325402061\nA.T .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 80,
      "chunk_index": 1
    }
  },
  {
    "text": "AST .\n−\n0 034544881\nTO .\n−\n1 325402061\nA.T .\n0 181014759\nSTL .\n0 007184622\nBLK .\n0 031705212\nPF .\n−\n13 823189660\nFG .\n2 694716234\nFT .\n3 2 526830872\nX P .\nNote that this is exactly the same result as we had before, but it gave us\na chance to look at some of the commands needed to work with matrices\nin R.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 80,
      "chunk_index": 2
    }
  },
  {
    "text": "open source: modeling in r 81\n3.14 Heteroskedasticity\nSimple linear regression assumes that the standard error of the residuals\nis the same for all observations. Many regressions suffer from the failure\nof this condition. The word for this is “heteroskedastic” errors. “Hetero”\nmeans different, and “skedastic” means dependent on type.\nWe can first test for the presence of heteroskedasticity using a stan-\ndard Breusch-Pagan test available in R. This resides in the lmtest pack-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 81,
      "chunk_index": 0
    }
  },
  {
    "text": "age which is loaded in before running the test.\n> ncaa = read. table(\"ncaa. txt\" ,header=TRUE)\n> y = as.matrix(ncaa[ 3 ])\n> x = as.matrix(ncaa[ 4 : 14 ])\n> result = lm(y~x)\n> library(lmtest)\nLoading required package: zoo\n> bptest( result )\nstudentized Breusch Pagan test\n−\ndata: result\nBP = 15 . 5378 , df = 11 , p value = 0 . 1592\n−\nWe can see that there is very little evidence of heteroskedasticity in the\nstandard errors as the p-value is not small. However, lets go ahead and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 81,
      "chunk_index": 1
    }
  },
  {
    "text": "correct the t-statistics for heteroskedasticity as follows, using the hccm\nfunction. The “hccm” stands for heteroskedasticity corrected covariance\nmatrix.\n> wuns = matrix( 1 , 64 , 1 )\n> z = cbind(wuns,x)\n> b = solve(t(z) %*% z) %*% (t(z) %*% y)\n> result = lm(y~x)\n> library(car)\n> vb = hccm( result )\n> stdb = sqrt(diag(vb))\n> tstats = b/stdb\n> tstats\nGMS\n2 68006069\n.\n−\n0 38212818\nPTS .\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 81,
      "chunk_index": 2
    }
  },
  {
    "text": "82 data science: theories, models, algorithms, and analytics\n2 38342637\nREB .\n0 40848721\nAST .\n−\n0 28709450\nTO .\n−\n0 65632053\nA.T .\n2 13627108\nSTL .\n0 09548606\nBLK .\n0 68036944\nPF .\n−\n3 52193532\nFG .\n2 35677255\nFT .\n3 1 23897636\nX P .\nHere we used the hccm function to generate the new covariance matrix\nvb of the coefficients, and then we obtained the standard errors as the\nsquare root of the diagonal of the covariance matrix. Armed with these",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 82,
      "chunk_index": 0
    }
  },
  {
    "text": "revised standard errors, we then recomputed the t-statistics by divid-\ning the coefficients by the new standard errors. Compare these to the\nt-statistics in the original model\nsummary( result )\nCoefficients :\nEstimate Std. Error t value Pr(>|t|)\n10 194804 2 892203 3 525 0 000893\n( Intercept ) . . . . ***\n− −\n0 010442 0 025276 0 413 0 681218\nxPTS . . . .\n− −\n0 105048 0 036951 2 843 0 006375\nxREB . . . . **\n0 060798 0 091102 0 667 0 507492\nxAST . . . .\n− −\n0 034545 0 071393 0 484 0 630513",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 82,
      "chunk_index": 1
    }
  },
  {
    "text": "xAST . . . .\n− −\n0 034545 0 071393 0 484 0 630513\nxTO . . . .\n− −\n1 325402 1 110184 1 194 0 237951\nxA.T . . . .\n0 181015 0 068999 2 623 0 011397\nxSTL . . . . *\n0 007185 0 075054 0 096 0 924106\nxBLK . . . .\n0 031705 0 044469 0 713 0 479050\nxPF . . . .\n− −\n13 823190 3 981191 3 472 0 001048\nxFG . . . . **\n2 694716 1 118595 2 409 0 019573\nxFT . . . . *\n3 2 526831 1 754038 1 441 0 155698\nxX P . . . .\nIt is apparent that when corrected for heteroskedasticity, the t-statistics in",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 82,
      "chunk_index": 2
    }
  },
  {
    "text": "the regression are lower, and also render some of the previously signifi-\ncant coefficients insignificant.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 82,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 83\n3.15 Auto-regressive models\nWhen data is autocorrelated, i.e., has dependence in time, not accounting\nfor it results in unnecessarily high statistical significance. Intuitively, this\nis because observations are treated as independent when actually they\nare correlated in time, and therefore, the true number of observations is\neffectively less.\nIn efficient markets, the correlation of returns from one period to the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 83,
      "chunk_index": 0
    }
  },
  {
    "text": "next should be close to zero. We use the returns stored in the variable\nrets (based on Google stock) from much earlier in this chapter.\n> n = length( rets )\n> n\n1 1670\n[ ]\n> cor( rets [ 1 :(n 1 )],rets [ 2 :n])\n−\n1 0 007215026\n[ ] .\nThis is for immediately consecutive periods, known as first-order auto-\ncorrelation. We may examine this across many staggered periods. For\nthis R has some neat library functions in the package car.\n> library(car)\n> durbin.watson(rets ,max. lag= 10 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 83,
      "chunk_index": 1
    }
  },
  {
    "text": "> durbin.watson(rets ,max. lag= 10 )\n1 1 974723 2 016951 1 984078 1 932000\n[ ] . . . .\n1 950987 2 101559 1 977719 1 838635\n. . . .\n2 052832 1 967741\n. .\n> res = lm( rets [ 2 :n]~rets [ 1 :(n 1 )])\n−\n> durbin.watson(res ,max. lag= 10 )\nlag Autocorrelation DW Statistic p value\n− −\n1 0 0006436855 2 001125 0 938\n. . .\n−\n2 0 0109757002 2 018298 0 724\n. . .\n−\n3 0 0002853870 1 996723 0 982\n. . .\n−\n4 0 0252586312 1 945238 0 276\n. . .\n5 0 0188824874 1 957564 0 402\n. . .\n6 0 0555810090 2 104550 0 020",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 83,
      "chunk_index": 2
    }
  },
  {
    "text": ". . .\n6 0 0555810090 2 104550 0 020\n. . .\n−\n7 0 0020507562 1 989158 0 926\n. . .\n8 0\n.\n0746953706 1\n.\n843219 0\n.\n004#\n9 0 0375308940 2 067304 0 136\n. . .\n−\n10 0 0085641680 1 974756 0 772\n. . .\nAlternative hypothesis : rho[lag] != 0",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 83,
      "chunk_index": 3
    }
  },
  {
    "text": "84 data science: theories, models, algorithms, and analytics\nThere is no evidence of auto-correlation when the DW statistic is close to\n2 2\n. If the DW-statistic is greater than it indicates negative autocorrela-\n2\ntion, and if it is less than , it indicates positive autocorrelation.\nIn the data there only seems to be statistical significance at the eighth\nlag. We may regress leading values on lags to see if the coefficient is\nsignificant.\n> summary(res)\nCall :",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 84,
      "chunk_index": 0
    }
  },
  {
    "text": "significant.\n> summary(res)\nCall :\nlm(formula = rets [ 2 :n] ~ rets [ 1 :(n 1 )])\n−\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax\n0 1242520 0 0102479 0 0002719 0 0106435 0 1813465\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error t value Pr(>|t|)\n0 0009919 0 0005539 1 791 0 0735\n( Intercept ) . . . . .\n1 1 0 0071913 0 0244114 0 295 0 7683\nrets [ :(n )] . . . .\n−\n−−−\nSignif . codes: 0 [***] 0 . 001 [**] 0 . 01 [*] 0 . 05 [.] 0 . 1 [ ]\nResidual std error : 0 . 02261 on 1667 degrees of freedom",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 84,
      "chunk_index": 1
    }
  },
  {
    "text": "Multiple R squared: 5 . 206 e 05 ,\n− −\nAdjusted R squared: 0 . 0005478\n− −\nF statistic : 0 . 08678 on 1 and 1667 DF, p value: 0 . 7683\n− −\nAs another example, let’s load in the file markowitzdata.txt and run\ntests on it. This file contains data on five tech sector stocks and also the\nFama-French data. The names function shows the headers of each col-\numn as shown below.\n> md = read. table(\"markowitzdata. txt\" ,header=TRUE)\n> names(md)\n1\n[ ] \"X.DATE\" \"SUNW\" \"MSFT\" \"IBM\" \"CSCO\" \"AMZN\" \"mktrf\"\n8",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 84,
      "chunk_index": 2
    }
  },
  {
    "text": "8\n[ ] \"smb\" \"hml\" \"rf\"\n> y = as.matrix(md[ 2 ])\n> x = as.matrix(md[ 7 : 9 ])\n> rf = as.matrix(md[ 10 ])\n> y = y rf\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 84,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 85\n> library(car)\n> results = lm(y ~ x)\n> durbin.watson(results ,max. lag= 6 )\nlag Autocorrelation DW Statistic p value\n− −\n1 0 07231926 2 144549 0 002\n. . .\n−\n2 0 04595240 2 079356 0 146\n. . .\n−\n3 0 02958136 1 926791 0 162\n. . .\n4 0 01608143 2 017980 0 632\n. . .\n−\n5 0 02360625 2 032176 0 432\n. . .\n−\n6 0 01874952 2 021745 0 536\n. . .\n−\nAlternative hypothesis : rho[lag] != 0\nThe car package is used. We see that there is one lag auto-correlation\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 85,
      "chunk_index": 0
    }
  },
  {
    "text": "1\n(note the small p-value for lag ), but not more than that; markets are\nvery efficient. Lets look at the regression before and after correction for\nautocorrelation:\n> summary( results )\nCall :\nlm(formula = y ~ x)\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax\n0 2136760 0 0143564 0 0007332 0 0144619 0 1910892\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error t value Pr(>|t|)\n0 000197 0 000785 0 251 0 8019\n( Intercept ) . . . .\n− −\n1 657968 0 085816 19 320 2 16\nxmktrf . . . < e ***\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 85,
      "chunk_index": 1
    }
  },
  {
    "text": "xmktrf . . . < e ***\n−\n0 299735 0 146973 2 039 0 0416\nxsmb . . . . *\n1 544633 0 176049 8 774 2 16\nxhml . . . < e ***\n− − −\n−−−\nSignif . codes: 0 [***] 0 . 001 [**] 0 . 01 [*] 0 . 05 [.] 0 . 1 [ ]\nResidual standard error : 0 . 03028 on 1503 degrees of freedom\nMultiple R Squared: 0 . 3636 , Adjusted R squared: 0 . 3623\n− −\nF statistic : 286 . 3 on 3 and 1503 DF, p value: < 2 . 2 e 16\n− − −\nLets correct the t-stats for autocorrelation using the Newey-West cor-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 85,
      "chunk_index": 2
    }
  },
  {
    "text": "rection. This correction is part of the car package. The steps undertaken",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 85,
      "chunk_index": 3
    }
  },
  {
    "text": "86 data science: theories, models, algorithms, and analytics\nhere are similar in mechanics to the ones we encountered when correct-\ning for heteroskedasticity.\n> res = lm(y~x)\n> b = res$coefficients\n> b\n( Intercept ) xmktrf xsmb xhml\n0 0001970164 1 6579682191 0 2997353765 1 5446330690\n. . . .\n− −\n1\n> vb = NeweyWest(res , lag= )\n> stdb = sqrt(diag(vb))\n> tstats = b/stdb\n> tstats\n( Intercept ) xmktrf xsmb xhml\n0 2633665 15 5779184 1 8300340 6 1036120\n. . . .\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 86,
      "chunk_index": 0
    }
  },
  {
    "text": ". . . .\n− −\nCompare these to the stats we had earlier. Notice how they have come\ndown after correction for AR. Note that there are several steps needed to\ncorrect for autocorrelation, and it might have been nice to roll one’s own\nfunction for this. (I leave this as an exercise for you.)\nFor fun, lets look at the autocorrelation in stock market indexes,\n31\nshown in Table . . The following graphic is taken from the book “A\nNon-Random Walk Down Wall Street” by Andrew Lo and Craig Mackin-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 86,
      "chunk_index": 1
    }
  },
  {
    "text": "lay. Is the autocorrelation higher for equally-weighted or value-weighted\nindexes? Why?\n3.16 Vector Auto-Regression\nAlso known as VAR (not the same thing as Value-at-Risk, denoted VaR).\nVAR is useful for estimating systems where there are simultaneous re-\ngression equations, and the variables influence each other. So in a VAR,\neach variable in a system is assumed to depend on lagged values of itself\nand the other variables. The number of lags may be chosen by the econo-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 86,
      "chunk_index": 2
    }
  },
  {
    "text": "metrician based on what is the expected decay in time-dependence of the\nvariables in the VAR.\nIn the following example, we examine the inter-relatedness of returns\nof the following three tickers: SUNW, MSFT, IBM. For vector autoregres-\nsions (VARs), we run the following R commands:\n> md = read. table(\"markowitzdata. txt\" ,header=TRUE)\n> y = as.matrix(md[ 2 : 4 ])\n> library( stats )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 86,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 87\nTable3.1: Autocorrelationindaily,\nweekly,andmonthlystockindex\nreturns. FromLo-Mackinlay,“A\nNon-RandomWalkDownWall\nStreet”.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 87,
      "chunk_index": 0
    }
  },
  {
    "text": "88 data science: theories, models, algorithms, and analytics\n> var 6 = ar(y, aic=TRUE,order= 6 )\n> var\n6$order\n1 1\n[ ]\n> var\n6$ar\n, , SUNW\nSUNW MSFT IBM\n1 0 00985635 0 02224093 0 002072782\n. . .\n−\n, , MSFT\nSUNW MSFT IBM\n1 0 008658304 0 1369503 0 0306552\n. . .\n−\n, , IBM\nSUNW MSFT IBM\n1 0 04517035 0 0975497 0 01283037\n. . .\n− −\nThe “order” of the VAR is how many lags are significant. In this exam-\n1\nple, the order is . Hence, when the “ar” command is given, it shows",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 88,
      "chunk_index": 0
    }
  },
  {
    "text": "the coefficients on the lagged values of the three value to just one lag.\n00098 00222\nFor example, for SUNW, the lagged coefficients are - . , . , and\n00021\n. , respectively for SUNW, MSFT, IBM. The Akaike Information Cri-\nterion (AIC) tells us which lag is significant, and we see below that this\n1\nis lag .\n> var\n6$aic\n0 1 2 3 4 5 6\n23 950676 0 000000 2 762663 5 284709 5 164238 10 065300 8 924513\n. . . . . . .\nSince the VAR was run for all six lags, the “partialacf” attribute of the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 88,
      "chunk_index": 1
    }
  },
  {
    "text": "output shows the coefficients of all lags.\n> var\n6$partialacf\n, , SUNW\nSUNW MSFT IBM\n1 0 00985635 0 022240931 0 002072782\n. . .\n−\n2 0 07857841 0 019721982 0 006210487\n. . .\n− − −\n3 0 03382375 0 003658121 0 032990758\n. . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 88,
      "chunk_index": 2
    }
  },
  {
    "text": "open source: modeling in r 89\n4 0 02259522 0 030023132 0 020925226\n. . .\n5 0 03944162 0 030654949 0 012384084\n. . .\n− − −\n6 0 03109748 0 021612632 0 003164879\n. . .\n− − −\n, , MSFT\nSUNW MSFT IBM\n1 0 008658304 0 13695027 0 030655201\n. . .\n−\n2 0 053224374 0 02396291 0 047058278\n. . .\n− − −\n3 0 080632420 0 03720952 0 004353203\n. . .\n−\n4 0 038171317 0 07573402 0 004913021\n. . .\n− − −\n5 0 002727220 0 05886752 0 050568308\n. . .\n6 0 242148823 0 03534206 0 062799122\n. . .\n, , IBM\nSUNW MSFT IBM",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 89,
      "chunk_index": 0
    }
  },
  {
    "text": ". . .\n, , IBM\nSUNW MSFT IBM\n1 0 04517035 0 097549700 0 01283037\n. . .\n− −\n2 0 05436993 0 021189756 0 05430338\n. . .\n3 0 08990973 0 077140955 0 03979962\n. . .\n− − −\n4 0 06651063 0 056250866 0 05200459\n. . .\n5 0 03117548 0 056192843 0 06080490\n. . .\n− −\n6 0 13131366 0 003776726 0 01502191\n. . .\n− − −\nInterestingly we see that each of the tickers has a negative relation to\nits lagged value, but a positive correlation with the lagged values of the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 89,
      "chunk_index": 1
    }
  },
  {
    "text": "other two stocks. Hence, there is positive cross autocorrelation amongst\nthese tech stocks. We can also run a model with three lags:\n> ar(y,method=\"ols\" ,order= 3 )\nCall :\nar(x = y, order.max = 3 , method = \"ols\")\n$ar\n1\n, ,\nSUNW MSFT IBM\n0 01407 0 0006952 0 036839\nSUNW . . .\n− −\n0 02693 0 1440645 0 100557\nMSFT . . .\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 89,
      "chunk_index": 2
    }
  },
  {
    "text": "90 data science: theories, models, algorithms, and analytics\n0 01330 0 0211160 0 009662\nIBM . . .\n−\n2\n, ,\nSUNW MSFT IBM\n0 082017 0 04079 0 04812\nSUNW . . .\n− −\n0 020668 0 01722 0 01761\nMSFT . . .\n− −\n0 006717 0 04790 0 05537\nIBM . . .\n− −\n3\n, ,\nSUNW MSFT IBM\n0 035412 0 081961 0 09139\nSUNW . . .\n−\n0 003999 0 037252 0 07719\nMSFT . . .\n−\n0 033571 0 003906 0 04031\nIBM . . .\n− −\n$x. intercept\nSUNW MSFT IBM\n9 623 05 7 366 05 6 257 05\n. e . e . e\n− − − − − −\n$var.pred\nSUNW MSFT IBM",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 90,
      "chunk_index": 0
    }
  },
  {
    "text": ". e . e . e\n− − − − − −\n$var.pred\nSUNW MSFT IBM\n0 0013593 0 0003007 0 0002842\nSUNW . . .\n0 0003007 0 0003511 0 0001888\nMSFT . . .\n0 0002842 0 0001888 0 0002881\nIBM . . .\nWe examine cross autocorrelation found across all stocks by Lo and\nMackinlay in their book “A Non-Random Walk Down Wall Street” – see\n32\nTable . . There is strong contemporaneous correlation amongst stocks\nshows in the top tableau but in the one below that, the cross one-lag",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 90,
      "chunk_index": 1
    }
  },
  {
    "text": "autocorrelation is also positive and strong. From two lags on the rela-\ntionship is weaker.\n3.17 Logit\nWhen the LHS variable in a regression is categorical and binary, i.e.,\n1 0\ntakes the value or , then a logit regression is more apt. For the NCAA\n32 1\ndata, take the top teams and make their dependent variable , and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 90,
      "chunk_index": 2
    }
  },
  {
    "text": "open source: modeling in r 91\nTable3.2: Crossautocorrelations\ninUSstocks. FromLo-Macklinlay,\n“ANon-RandomWalkDownWall\nStreet.”",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 91,
      "chunk_index": 0
    }
  },
  {
    "text": "92 data science: theories, models, algorithms, and analytics\n32\nthat of the bottom teams zero. Hence, we split the data into the teams\nabove average and the teams that are below average. Our goal is to fit a\nregression model that returns a team’s probability of being above aver-\nage. This is the same as the team’s predicted percentile ranking.\n1 1 32\n> y = :\n1 1 0 1\n> y = y * +\n1\n> y\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[ ]\n2 1 0\n> y = y *\n2\n> y",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 92,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ]\n2 1 0\n> y = y *\n2\n> y\n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[ ]\n> y = c(y 1 ,y 2 )\n> y\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n[ ]\n34 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[ ]\nWe use the function glm (generalized linear model) for this task. Run-\nning the model is pretty easy as follows:\n> h = glm(y~x, family=binomial(link=\" logit \" ))\n> logLik(h)\n’log Lik. ’ 21 . 44779 (df= 12 )\n−\n> summary(h)\nCall :",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 92,
      "chunk_index": 1
    }
  },
  {
    "text": "−\n> summary(h)\nCall :\nglm(formula = y ~ x, family = binomial(link = \" logit \" ))\nDeviance Residuals :\nMin\n1Q\nMedian\n3Q\nMax\n1 80174 0 40502 0 00238 0 37584 2 31767\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error z value Pr(>|z|)\n45 83315 14 97564 3 061 0 00221\n( Intercept ) . . . . **\n− −\n0 06127 0 09549 0 642 0 52108\nxPTS . . . .\n− −\n0 49037 0 18089 2 711 0 00671\nxREB . . . . **\n0 16422 0 26804 0 613 0 54010\nxAST . . . .\n0 38405 0 23434 1 639 0 10124\nxTO . . . .\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 92,
      "chunk_index": 2
    }
  },
  {
    "text": "0 38405 0 23434 1 639 0 10124\nxTO . . . .\n− −\n1 56351 3 17091 0 493 0 62196\nxA.T . . . .\n0 78360 0 32605 2 403 0 01625\nxSTL . . . . *",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 92,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 93\n0 07867 0 23482 0 335 0 73761\nxBLK . . . .\n0 02602 0 13644 0 191 0 84874\nxPF . . . .\n46 21374 17 33685 2 666 0 00768\nxFG . . . . **\n10 72992 4 47729 2 397 0 01655\nxFT . . . . *\n3 5 41985 5 77966 0 938 0 34838\nxX P . . . .\n−−−\nSignif . codes: 0 [***] 0 . 001 [**] 0 . 01 [*] 0 . 05 [.] 0 . 1 [ ]\n(Dispersion parameter for binomial family taken to be 1 )\nNull deviance: 88 . 723 on 63 degrees of freedom\nResidual deviance: 42 . 896 on 52 degrees of freedom\n66 896\nAIC: .\n6",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 93,
      "chunk_index": 0
    }
  },
  {
    "text": "66 896\nAIC: .\n6\nNumber of Fisher Scoring iterations :\nThus, we see that the best variables that separate upper-half teams from\nlower-half teams are the number of rebounds and the field goal per-\ncentage. To a lesser extent, field goal percentage and steals also provide\nsome explanatory power. The logit regression is specified as follows:\ney\nz =\n1+ey\ny = b +b x +b x +...+b x\n0 1 1 2 2 k k\nThe original data z = 0,1 . The range of values of y is ( ∞ ,+∞). And\n{ } −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 93,
      "chunk_index": 1
    }
  },
  {
    "text": "{ } −\nas required, the fitted z (0,1). The variables x are the RHS variables.\n∈\nThe fitting is done using MLE.\nSuppose we ran this with a simple linear regression:\n> h = lm(y~x)\n> summary(h)\nCall :\nlm(formula = y ~ x)\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax\n0 65982 0 26830 0 03183 0 24712 0 83049\n. . . . .\n− −\nCoefficients :",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 93,
      "chunk_index": 2
    }
  },
  {
    "text": "94 data science: theories, models, algorithms, and analytics\nEstimate Std. Error t value Pr(>|t|)\n4 114185 1 174308 3 503 0 000953\n( Intercept ) . . . . ***\n− −\n0 005569 0 010263 0 543 0 589709\nxPTS . . . .\n− −\n0 046922 0 015003 3 128 0 002886\nxREB . . . . **\n0 015391 0 036990 0 416 0 679055\nxAST . . . .\n0 046479 0 028988 1 603 0 114905\nxTO . . . .\n− −\n0 103216 0 450763 0 229 0 819782\nxA.T . . . .\n0 063309 0 028015 2 260 0 028050\nxSTL . . . . *\n0 023088 0 030474 0 758 0 452082\nxBLK . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 94,
      "chunk_index": 0
    }
  },
  {
    "text": "0 023088 0 030474 0 758 0 452082\nxBLK . . . .\n0 011492 0 018056 0 636 0 527253\nxPF . . . .\n4 842722 1 616465 2 996 0 004186\nxFG . . . . **\n1 162177 0 454178 2 559 0 013452\nxFT . . . . *\n3 0 476283 0 712184 0 669 0 506604\nxX P . . . .\n−−−\nSignif . codes: 0 [***] 0 . 001 [**] 0 . 01 [*] 0 . 05 [.] 0 . 1 [ ]\nResidual standard error : 0 . 3905 on 52 degrees of freedom\nMultiple R Squared: 0 . 5043 , Adjusted R squared: 0 . 3995\n− −\nF statistic : 4 . 81 on 11 and 52 DF, p value: 4 . 514 e 05\n− − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 94,
      "chunk_index": 1
    }
  },
  {
    "text": "− − −\nWe get the same variables again showing up as significant.\n3.18 Probit\nWe can redo the same using a probit instead. A probit is identical in\nspirit to the logit regression, except that the function that is used is\nz = Φ(y)\ny = b +b x +b x +...+b x\n0 1 1 2 2 k k\nwhere Φ( ) is the cumulative normal probability function. It is imple-\n·\nmented in R as follows.\n> h = glm(y~x, family=binomial(link=\"probit\" ))\n> logLik(h)\n’log Lik. ’ 21 . 27924 (df= 12 )\n−\n> summary(h)\nCall :",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 94,
      "chunk_index": 2
    }
  },
  {
    "text": "−\n> summary(h)\nCall :\nglm(formula = y ~ x, family = binomial(link = \"probit\" ))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 94,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 95\nDeviance Residuals :\nMin\n1Q\nMedian\n3Q\nMax\n1 7635295 0 4121216 0 0003102 0 3499560 2 2456825\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error z value Pr(>|z|)\n26 28219 8 09608 3 246 0 00117\n( Intercept ) . . . . **\n− −\n0 03463 0 05385 0 643 0 52020\nxPTS . . . .\n− −\n0 28493 0 09939 2 867 0 00415\nxREB . . . . **\n0 10894 0 15735 0 692 0 48874\nxAST . . . .\n0 23742 0 13642 1 740 0 08180\nxTO . . . . .\n− −\n0 71485 1 86701 0 383 0 70181\nxA.T . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 95,
      "chunk_index": 0
    }
  },
  {
    "text": "− −\n0 71485 1 86701 0 383 0 70181\nxA.T . . . .\n0 45963 0 18414 2 496 0 01256\nxSTL . . . . *\n0 03029 0 13631 0 222 0 82415\nxBLK . . . .\n0 01041 0 07907 0 132 0 89529\nxPF . . . .\n26 58461 9 38711 2 832 0 00463\nxFG . . . . **\n6 28278 2 51452 2 499 0 01247\nxFT . . . . *\n3 3 15824 3 37841 0 935 0 34988\nxX P . . . .\n−−−\nSignif . codes: 0 [***] 0 . 001 [**] 0 . 01 [*] 0 . 05 [.] 0 . 1 [ ]\n(Dispersion parameter for binomial family taken to be 1 )\nNull deviance: 88 . 723 on 63 degrees of freedom",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 95,
      "chunk_index": 1
    }
  },
  {
    "text": "Null deviance: 88 . 723 on 63 degrees of freedom\nResidual deviance: 42 . 558 on 52 degrees of freedom\n66 558\nAIC: .\n8\nNumber of Fisher Scoring iterations :\nThe results confirm those obtained from the linear regression and logit\nregression.\n3.19 Solving Non-Linear Equations\nEarlier we examined root finding. Here we develop it further. We have\nalso not done much with user-generated functions. Here is a neat model\nin R to solve for the implied volatility in the Black-Merton-Scholes class\n1973",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 95,
      "chunk_index": 2
    }
  },
  {
    "text": "1973\nof models. First, we code up the Black-Scholes ( ) model; this is the\nfunction bms73 below. Then we write a user-defined function that solves",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 95,
      "chunk_index": 3
    }
  },
  {
    "text": "96 data science: theories, models, algorithms, and analytics\nfor the implied volatility from a given call or put option price. The pack-\nage minpack.lm is used for the equation solving, and the function call is\nnls.lm.\nThe following program listing may be saved in a file called rbc.R and\nthen called from the command line. The function impvol uses the bms73\nfunction and solves for the implied volatility.\n#Black Merton Scholes 1973\n− −\nbms 73 = function(sig ,S,K,T,r ,q,cp, optprice) {",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 96,
      "chunk_index": 0
    }
  },
  {
    "text": "bms 73 = function(sig ,S,K,T,r ,q,cp, optprice) {\nd 1 = (log(S/K)+(r q+ 0 . 5 *sig^ 2 )*T) / (sig*sqrt(T))\n−\nd 2 = d 1 sig*sqrt(T)\n−\nif (cp== 1 ) {\noptval = S*exp( q*T)*pnorm(d 1 ) K*exp( r*T)*pnorm(d 2 )\n− − −\n}\nelse {\noptval = S*exp( q*T)*pnorm( d 1 )+K*exp( r*T)*pnorm( d 2 )\n− − − − −\n}\n#If option price is supplied we want the implied vol , else optprice\nbs = optval optprice\n−\n}\n#Function to return Imp Vol with starting guess sig0\nimpvol = function(sig 0 ,S,K,T,r ,q,cp, optprice) {",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 96,
      "chunk_index": 1
    }
  },
  {
    "text": "sol = nls .lm(par=sig 0 ,fn=bms 73 ,S=S,K=K,T=T, r=r ,q=q,\ncp=cp, optprice=optprice)\n}\nThe calls to this model are as follows:\n> library(minpack.lm)\n> source(\"rbc .R\")\n0 2 40 40 1 0 03 0 0 4\n> res = impvol( . , , , , . , , , )\n> res$par\n1 0 2915223\n[ ] .\nWe note that the function impvol was written such that the argument\nthat we needed to solve for, sig0, the implied volatility, was the first\nargument in the function. However, the expression par=sig0 does in-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 96,
      "chunk_index": 2
    }
  },
  {
    "text": "form the solver which argument is being searched for in order to satisfy\nthe non-linear equation for implied volatility. Note also that the func-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 96,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 97\ntion bms73 returns the difference between the model price and observed\nprice, not the model price alone. This is necessary as the solver tries to\nset this value to zero by finding the implied volatility.\nLets check if we put this volatility back into the bms function that we\n4\nget back the option price of . Voila!\n> print(bms 73 (res$par[ 1 ] , 40 , 40 , 1 , 0 . 03 , 0 , 0 , 0 ))\n1 4\n[ ]\n3.20 Web-Enabling R Functions",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 97,
      "chunk_index": 0
    }
  },
  {
    "text": "1 4\n[ ]\n3.20 Web-Enabling R Functions\nWhen building a user-friendly system it may be useful to run R pro-\ngrams from a web page as interface. This is quite easy to implement and\nthe following is a simple example of how this is done. This is an extract\nof my blog post at\nhttp://sanjivdas.wordpress.com/2010/11/07/\nweb-enabling-r-functions-with-cgi-on-a-mac-os-x-desktop/\nThis is just an example based on the “Rcgi” package from David Firth,\nand for full details of using R with CGI, see",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 97,
      "chunk_index": 1
    }
  },
  {
    "text": "and for full details of using R with CGI, see\nhttp://www.omegahat.org/CGIwithR/.\nYou can install the package as follows:\ninstall .packages(\"CGIwithR\" , repos = \"http : / /www.omegahat.org/R\" , type=\"source\")\n1\nThe following is the Windows equivalent: 1ThanksAliceYehjinJun.\n#1) Create fold \"www\" in \"Documents\", create \"cgi bin\" in \"www\", place files in \"cgi bin\"\n− −\n#2) Open command prompt. Run the following\nicacls \"C:\\Users\\UserName\\Documents\\www\\cgi bin\\.Rprofile\" /grant Users :(CI)(OI)F\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 97,
      "chunk_index": 2
    }
  },
  {
    "text": "−\nicacls \"C:\\Users\\UserName\\Documents\\www\\cgi bin\\R. cgi\" /grant Users :(CI)(OI)F\n−\nDownload the document on using R with CGI. It’s titled “CGIwithR:\nFacilities for Processing Web Forms with R”.\nOf course, if you don’t have R at all, then download R and install it\nfrom http://www.r-project.org/. Then use the R package manager to\ninstall the Rcgi package.\nYou need two program files to get everything working. (a) The html\nfile that is the web form for input data. (b) The R file, with special tags",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 97,
      "chunk_index": 3
    }
  },
  {
    "text": "for use with the CGIwithR package.\nOur example will be simple, i.e., a calculator to work out the monthly\npayment on a standard fixed rate mortgage. The three inputs are the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 97,
      "chunk_index": 4
    }
  },
  {
    "text": "98 data science: theories, models, algorithms, and analytics\nloan principal, annual loan rate, and the number of remaining months to\nmaturity.\nBut first, let’s create the html file for the web page that will take these\nthree input values. We call it “mortgage calc.html”. The code is all\nstandard, for those familiar with html, and even if you are not used to\n34\nhtml, the code is self-explanatory. See Figure . .\nFigure3.4: HTMLcodefortheRcgi\napplication.\n06",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 98,
      "chunk_index": 0
    }
  },
  {
    "text": "Figure3.4: HTMLcodefortheRcgi\napplication.\n06\nNotice that line will be the one referencing the R program that does\n08 10 12\nthe calculation. The three inputs are accepted in lines - . Line\nsends the inputs to the R program.\nNext, we look at the R program, suitably modified to include html\n35\ntags. We name it \"mortgage calc.R\". See Figure . .\nWe can see that all html calls in the R program are made using the\n22 35\n“tag()” construct. Lines – take in the three inputs from the html\n43 44 45",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 98,
      "chunk_index": 1
    }
  },
  {
    "text": "43 44 45\nform. Lines – do the calculations and line prints the result. The\n“cat()” function prints its arguments to the web browser page.\nOkay, we have seen how the two programs (html, R) are written and\nthese templates may be used with changes as needed. We also need\nto pay attention to setting up the R environment to make sure that the\nfunction is served up by the system. The following steps are needed:\nMake sure that your Mac is allowing connections to its web server. Go",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 98,
      "chunk_index": 2
    }
  },
  {
    "text": "to System Preferences and choose Sharing. In this window enable Web",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 98,
      "chunk_index": 3
    }
  },
  {
    "text": "open source: modeling in r 99\nSharing by ticking the box next to it.\nPlace the html file “mortgage calc.html” in the directory that serves up\nweb pages. On a Mac, there is already a web directory for this called\n“Sites”. It’s a good idea to open a separate subdirectory called (say)\n“Rcgi” below this one for the R related programs and put the html file\nthere.\nThe R program “mortgage calc.R” must go in the directory that has\nbeen assigned for CGI executables. On a Mac, the default for this direc-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 99,
      "chunk_index": 0
    }
  },
  {
    "text": "tory is “/Library/WebServer/CGI-Executables” and is usually refer-\nenced by the alias “cgi-bin” (stands for cgi binaries). Drop the R pro-\ngram into this directory. Two more important files are created when you\ninstall the Rcgi package. The CGIwithR installation creates two files: (a)\nA hidden file called .Rprofile; (b) A file called R.cgi. Place both these\nfiles in the directory: /Library/WebServer/CGI-Executables\nIf you cannot find the .Rprofile file then create it directly by opening",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 99,
      "chunk_index": 1
    }
  },
  {
    "text": "a text editor and adding two lines to the file:\n#! /usr/ bin /R\nlibrary(CGIwithR,warn. conflicts=FALSE)\nNow, open the R.cgi file and make sure that the line pointing to the R\nexecutable in the file is showing\nR_DEFAULT=/usr/bin/R\nThe file may actually have it as “#!/usr/local/bin/R” which is for\nLinux platforms, but the usual Mac install has the executable in “#!\n/usr/bin/R” so make sure this is done.\nMake both files executable as follows:\nchmod a+rx .Rprofile\nchmod a+rx R.cgi",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 99,
      "chunk_index": 2
    }
  },
  {
    "text": "chmod a+rx .Rprofile\nchmod a+rx R.cgi\nFinally, make the /Sites/Rcgi/ directory write accessible:\n∼\nchmod a+wx /Sites/Rcgi\n∼\nJust being patient and following all the steps makes sure it all works\nwell. Having done it once, it’s easy to repeat and create several func-\ntions. You can try this example out on my web server at the following\nlink.\nThe inputs are as follows: Loan principal (enter a dollar amount).\n006\nAnnual loan rate (enter it in decimals, e.g., six percent is entered as . ).\n300 25",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 99,
      "chunk_index": 3
    }
  },
  {
    "text": "300 25\nRemaining maturity in months (enter if the remaining maturity is\nyears).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 99,
      "chunk_index": 4
    }
  },
  {
    "text": "100 data science: theories, models, algorithms, and analytics\nRecently the open source project Shiny has become a popular ap-\nproach to creating R-enabled web pages. See http://shiny.rstudio.com/.\nThis creates dynamic web pages with sliders and buttons and is a power-\nful tool for representing analytics and visualizations.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 100,
      "chunk_index": 0
    }
  },
  {
    "text": "open source: modeling in r 101\nFigure3.5: RcodefortheRcgi\napplication.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 101,
      "chunk_index": 0
    }
  },
  {
    "text": "4\nMoRe: Data Handling and Other Useful Things\nIn this chapter, we will revisit some of the topics considered in the pre-\nvious chapters, and demonstrate alternate programming approaches in\nR. There are some extremely powerful packages in R that allow sql-like\noperations on data sets, making for advanced data handling. One of the\nmost time-consuming activities in data analytics is cleaning and arrang-\ning data, and here we will show examples of many tools available for\nthat purpose.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 103,
      "chunk_index": 0
    }
  },
  {
    "text": "that purpose.\nLet’s assume we have a good working knowledge of R by now. Here\nwe revisit some more packages, functions, and data structures.\n4.1 Data Extraction of stocks using quantmod\nWe have seen the package already in the previous chapter. Now, we\nproceed to use it to get some initial data.\nlibrary(quantmod)\ntickers = c(\"AAPL\" ,\"YHOO\" ,\"IBM\" ,\"CSCO\" ,\"C\" ,\"GSPC\")\ngetSymbols( tickers )\n1\n[ ] \"AAPL\" \"YHOO\" \"IBM\" \"CSCO\" \"C\" \"GSPC\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 103,
      "chunk_index": 1
    }
  },
  {
    "text": "1\n[ ] \"AAPL\" \"YHOO\" \"IBM\" \"CSCO\" \"C\" \"GSPC\"\nPrint the length of each stock series. Are they all the same? Here we\nneed to extract the ticker symbol without quotes.\nfor (t in tickers ) {\na = get(noquote(t ))[ , 1 ]\nprint(c(t ,length(a)))\n}\n1 2229\n[ ] \"AAPL\" \" \"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 103,
      "chunk_index": 2
    }
  },
  {
    "text": "104 data science: theories, models, algorithms, and analytics\n1 2229\n[ ] \"YHOO\" \" \"\n1 2229\n[ ] \"IBM\" \" \"\n1 2229\n[ ] \"CSCO\" \" \"\n1 2229\n[ ] \"C\" \" \"\n1 2222\n[ ] \"GSPC\" \" \"\nWe see that they are not all the same. The stock series are all the same\n7\nlength but the S&P index is shorter by days.\nConvert closing adjusted prices of all stocks into individual data.frames.\nFirst, we create a list of data.frames. This will also illustrate how useful",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 104,
      "chunk_index": 0
    }
  },
  {
    "text": "lists are because we store data.frames in lists. Notice how we also add\na new column to each data.frame so that the dates column may later be\nused as an index to join the individual stock data.frames into one com-\nposite data.frame.\ndf = list ()\n0\nj =\nfor (t in tickers ) {\n1\nj = j +\na = noquote(t)\nb = data.frame(get(a)[ , 6 ])\nb$dt = row.names(b)\ndf[[ j ]] = b\n}\nSecond, we combine all the stocks adjusted closing prices into a single",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 104,
      "chunk_index": 1
    }
  },
  {
    "text": "data.frame using a join, excluding all dates for which all stocks do not\nhave data. The main function used here is *merge* which could be an\nintersect join or a union join. The default is the intersect join.\nstock_table = df[[ 1 ]]\nfor ( j in 2 :length(df)) {\nstock_table = merge(stock_table ,df[[ j ]] ,by=\"dt\")\n}\ndim(stock_table)\n1 2222 7\n[ ]\nNote that the stock table contains the number of rows of the stock index,\nwhich had fewer observations than the individual stocks. So since this is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 104,
      "chunk_index": 2
    }
  },
  {
    "text": "an intersect join, some rows have been dropped.\n2\nPlot all stocks in a single data.frame using ggplot , which is more",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 104,
      "chunk_index": 3
    }
  },
  {
    "text": "more: data handling and other useful things 105\nadvanced than the basic plot function. We use the basic plot function\nfirst.\npar(mfrow=c( 3 , 2 )) #Set the plot area to six plots\nfor ( j in 1 :length( tickers )) {\nplot(as.Date(stock_table [ , 1 ]) , stock_table [ , j + 1 ], type=\"l\" ,\nylab=tickers [ j ] ,xlab=\"date\")\n}\npar(mfrow=c( 1 , 1 )) #Set the plot figure back to a single plot\n41\nThe plot is shown in Figure . .\nFigure4.1: Plotsofthesixstock\nseriesextractedfromtheweb.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 105,
      "chunk_index": 0
    }
  },
  {
    "text": "seriesextractedfromtheweb.\nConvert the data into returns. These are continuously compounded\nreturns, or log returns.\nn = length(stock_table [ , 1 ])\nrets = stock_table [ , 2 :( length( tickers )+ 1 )]\nfor ( j in 1 :length( tickers )) {",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 105,
      "chunk_index": 1
    }
  },
  {
    "text": "106 data science: theories, models, algorithms, and analytics\nrets [ 2 :n, j ] = diff(log( rets [ , j ]))\n}\nrets$dt = stock_table$dt\nrets = rets [ 2 :n,] #lose the first row when converting to returns\nhead( rets )\nAAPL.Adjusted YHOO.Adjusted IBM.Adjusted CSCO.Adjusted C.Adjusted\n2 0 021952927 0 047282882 0 010635139 0 0259847196 0 0034448850\n. . . . .\n−\n3 0 007146655 0 032609594 0 009094215 0 0003513139 0 0052808346\n. . . . .\n− − −\n4 0 004926130 0 006467863 0 015077743 0 0056042225 0 0050992429",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 106,
      "chunk_index": 0
    }
  },
  {
    "text": ". . . . .\n5 0 079799667 0 012252406 0 011760691 0 0056042225 0 0087575599\n. . . . .\n− − −\n6 0 046745828 0 039806285 0 011861828 0 0073491452 0 0080957651\n. . . . .\n− −\n7 0 012448245 0 017271586 0 002429865 0 0003486195 0 0007387328\n. . . . .\n− −\nGSPC.Adjusted dt\n2 0 0003791652 2007 01 04\n.\n− − −\n3 0 0000000000 2007 01 05\n.\n− −\n4 0 0093169957 2007 01 08\n.\n− −\n5 0 0127420077 2007 01 09\n.\n− − −\n6 0 0000000000 2007 01 10\n.\n− −\n7 0 0053254100 2007 01 11\n.\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 106,
      "chunk_index": 1
    }
  },
  {
    "text": ".\n− −\n7 0 0053254100 2007 01 11\n.\n− −\nThe data.frame of returns can be used to present the descriptive statis-\ntics of returns.\nsummary( rets )\nAAPL.Adjusted YHOO.Adjusted IBM.Adjusted\n0 197470 0 2340251 0 0864191\nMin. : . Min. : . Min. : .\n− − −\n1 0 009000 1 0 0113101 1 0 0065172\nst Qu.: . st Qu.: . st Qu.: .\n− − −\n0 001192 0 0002238 0 0003044\nMedian : . Median : . Median : .\n0 001074 0 0001302 0 0002388\nMean : . Mean : . Mean : .\n3 0 012242 3 0 0118051 3 0 0076578",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 106,
      "chunk_index": 2
    }
  },
  {
    "text": "3 0 012242 3 0 0118051 3 0 0076578\nrd Qu.: . rd Qu.: . rd Qu.: .\n0 130194 0 3918166 0 1089889\nMax. : . Max. : . Max. : .\nCSCO.Adjusted C.Adjusted GSPC.Adjusted\n0 1768648 0 4946962 0 1542679\nMin. : . Min. : . Min. : .\n− − −\n1 0 0082048 1 0 0127716 1 0 0044266\nst Qu.: . st Qu.: . st Qu.: .\n− − −\n0 0003513 0 0002122 0 0000000\nMedian : . Median : . Median : .\n−\n0 0000663 0 0009834 0 0001072\nMean : . Mean : . Mean : .\n−\n3 0 0092129 3 0 0120002 3 0 0049999\nrd Qu.: . rd Qu.: . rd Qu.: .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 106,
      "chunk_index": 3
    }
  },
  {
    "text": "rd Qu.: . rd Qu.: . rd Qu.: .\n0 1479929 0 4563162 0 1967146\nMax. : . Max. : . Max. : .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 106,
      "chunk_index": 4
    }
  },
  {
    "text": "more: data handling and other useful things 107\ndt\n2221\nLength:\nClass : character\nMode : character\nNow we compute the correlation matrix of returns.\ncor( rets [ , 1 :length( tickers )])\nAAPL.Adjusted YHOO.Adjusted IBM.Adjusted CSCO.Adjusted\n1 0000000 0 3529739 0 4887079 0 4903812\nAAPL.Adjusted . . . .\n0 3529739 1 0000000 0 3817138 0 4132464\nYHOO.Adjusted . . . .\n0 4887079 0 3817138 1 0000000 0 5792123\nIBM.Adjusted . . . .\n0 4903812 0 4132464 0 5792123 1 0000000\nCSCO.Adjusted . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 107,
      "chunk_index": 0
    }
  },
  {
    "text": "CSCO.Adjusted . . . .\nC.Adjusted 0 . 3739598 0 . 3362138 0 . 4322276 0 . 4648106\n0 2252352 0 1686898 0 2052341 0 2363631\nGSPC.Adjusted . . . .\nC.Adjusted GSPC.Adjusted\n0 3739598 0 2252352\nAAPL.Adjusted . .\n0 3362138 0 1686898\nYHOO.Adjusted . .\n0 4322276 0 2052341\nIBM.Adjusted . .\n0 4648106 0 2363631\nCSCO.Adjusted . .\nC.Adjusted 1 . 0000000 0 . 3367560\n0 3367560 1 0000000\nGSPC.Adjusted . .\nShow the correlogram for the six return series. This is a useful way to",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 107,
      "chunk_index": 1
    }
  },
  {
    "text": "visualize the relationship between all variables in the data set. See Figure\n42\n. .\nlibrary(corrgram)\ncorrgram( rets [ , 1 :length( tickers )] , order=TRUE, lower.panel=panel. ellipse ,\nupper.panel=panel.pts , text .panel=panel. txt )\nTo see the relation between the stocks and the index, run a regression\nof each of the five stocks on the index returns.\nbetas = NULL\nfor ( j in 1 :(length( tickers) 1 )) {\n−\nres = lm( rets [ , j ]~rets [ , 6 ])\nbetas[ j ] = res$coefficients [ 2 ]\n}\nprint(betas)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 107,
      "chunk_index": 2
    }
  },
  {
    "text": "}\nprint(betas)\n1 0 2912491 0 2576751 0 1780251 0 2803140 0 8254747\n[ ] . . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 107,
      "chunk_index": 3
    }
  },
  {
    "text": "108 data science: theories, models, algorithms, and analytics\nFigure4.2: Plotsofthecorrelation\nmatrixofsixstockseriesextracted\nfromtheweb.\nThe βs indicate the level of systematic risk for each stock. We notice that\nall the betas are positive, and highly significant. But they are not close\nto unity, in fact all are lower. This is evidence of misspecification that\nmay arise from the fact that the stocks are in the tech sector and better",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 108,
      "chunk_index": 0
    }
  },
  {
    "text": "explanatory power would come from an index that was more relevant to\nthe technology sector.\nIn order to assess whether in the cross-section, there is a relation be-\ntween average returns and the systematic risk or β of a stock, run a re-\ngression of the five average returns on the five betas from the regression.\nbetas = matrix(betas)\navgrets = colMeans( rets [ , 1 :( length( tickers ) 1 )])\n−\nres = lm(avgrets~betas)\nsummary(res)\nplot(betas , avgrets)\nabline(res , col=\"red\")\n43",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 108,
      "chunk_index": 1
    }
  },
  {
    "text": "plot(betas , avgrets)\nabline(res , col=\"red\")\n43\nSee Figure . . We see indeed, that there is an unexpected negative re-\nlation between β and the return levels. This may be on account of the\nparticular small sample we used for illustration here, however, we note\nthat the CAPM (Capital Asset Pricing Model) dictate that we see a posi-\ntive relation between stock returns and a firm’s systematic risk level.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 108,
      "chunk_index": 2
    }
  },
  {
    "text": "more: data handling and other useful things 109\nFigure4.3: Regressionofstock\naveragereturnsagainstsystematic\nrisk(β).\n4.2 Using the merge function\nData frames a very much like spreadsheets or tables, but they are also\na lot like databases. Some sort of happy medium. If you want to join\ntwo dataframes, it is the same a joining two databases. For this R has the\nmerge function. It is best illustrated with an example.\nSuppose we have a list of ticker symbols and we want to generate",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 109,
      "chunk_index": 0
    }
  },
  {
    "text": "a dataframe with more details on these tickers, especially their sector\nand the full name of the company. Let’s look at the input list of tickers.\nSuppose I have them in a file called tickers.csv where the delimiter is\nthe colon sign. We read this in as follows.\ntickers = read. table(\" tickers .csv\" ,header=FALSE,sep=\" : \")\nThe line of code reads in the file and this gives us two columns of\n6\ndata. We can look at the top of the file (first rows).\n> head( tickers )\n1 2\nV V\n1\nNasdaqGS ACOR\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 109,
      "chunk_index": 1
    }
  },
  {
    "text": "> head( tickers )\n1 2\nV V\n1\nNasdaqGS ACOR\n2\nNasdaqGS AKAM\n3\nNYSE ARE",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 109,
      "chunk_index": 2
    }
  },
  {
    "text": "110 data science: theories, models, algorithms, and analytics\n4\nNasdaqGS AMZN\n5\nNasdaqGS AAPL\n6\nNasdaqGS AREX\nNote that the ticker symbols relate to stocks from different exchanges,\nin this case Nasdaq and NYSE. The file may also contain AMEX listed\nstocks.\nThe second line of code below counts the number of input tickers, and\nthe third line of code renames the columns of the dataframe. We need\nto call the column of ticker symbols as “Symbol” because we will see",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 110,
      "chunk_index": 0
    }
  },
  {
    "text": "that the dataframe with which we will merge this one also has a column\nwith the same name. This column becomes the index on which the two\ndataframes are matched and joined.\n> n = dim( tickers )[ 1 ]\n> n\n1 98\n[ ]\n> names( tickers ) = c(\"Exchange\" ,\"Symbol\")\n> head( tickers )\nExchange Symbol\n1\nNasdaqGS ACOR\n2\nNasdaqGS AKAM\n3\nNYSE ARE\n4\nNasdaqGS AMZN\n5\nNasdaqGS AAPL\n6\nNasdaqGS AREX\nNext, we read in lists of all stocks on Nasdaq, NYSE, and AMEX as\nfollows:\nlibrary(quantmod)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 110,
      "chunk_index": 1
    }
  },
  {
    "text": "follows:\nlibrary(quantmod)\nnasdaq_names = stockSymbols(exchange=\"NASDAQ\")\nnyse_names = stockSymbols(exchange=\"NYSE\")\namex_names = stockSymbols(exchange=\"AMEX\")\nWe can look at the top of the Nasdaq file.\n> head(nasdaq_names)\nSymbol Name LastSale MarketCap IPOyear\n1 AAAP Advanced Accelerator Applications S.A. 25.20 $972.09M 2015\n2 AAL American Airlines Group, Inc. 42.20 $26.6B NA\n3 AAME Atlantic American Corporation 4.69 $96.37M NA\n4 AAOI Applied Optoelectronics , Inc. 17.96 $302.36M 2013",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 110,
      "chunk_index": 2
    }
  },
  {
    "text": "5 AAON AAON, Inc. 24.13 $1.31B NA",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 110,
      "chunk_index": 3
    }
  },
  {
    "text": "more: data handling and other useful things 111\n6 AAPC Atlantic Alliance Partnership Corp. 10.16 $105.54M 2015\nSector Industry Exchange\n1 Health Care Major Pharmaceuticals NASDAQ\n2 Transportation Air Freight/Delivery Services NASDAQ\n3 Finance Life Insurance NASDAQ\n4 Technology Semiconductors NASDAQ\n5 Capital Goods Industrial Machinery/Components NASDAQ\n6 Finance Business Services NASDAQ\nNext we merge all three dataframes for each of the exchanges into one\ndata frame.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 111,
      "chunk_index": 0
    }
  },
  {
    "text": "data frame.\nco_names = rbind(nyse_names,nasdaq_names,amex_names)\nTo see how many rows are there in this merged file, we check dimen-\nsions.\n> dim(co_names)\n1 6801 8\n[ ]\nFinally, use the merge function to combine the ticker symbols file with\nthe exchanges data to extend the tickers file to include the information\nfrom the exchanges file.\n> result = merge(tickers ,co_names,by=\"Symbol\")\n> head(result)\nSymbol Exchange.x Name LastSale\n1 AAPL NasdaqGS Apple Inc. 119.30",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 111,
      "chunk_index": 1
    }
  },
  {
    "text": "1 AAPL NasdaqGS Apple Inc. 119.30\n2 ACOR NasdaqGS Acorda Therapeutics , Inc. 37.40\n3 AKAM NasdaqGS Akamai Technologies , Inc. 56.92\n4 AMZN NasdaqGS Amazon.com, Inc. 668.45\n5 ARE NYSE Alexandria Real Estate Equities , Inc. 91.10\n6 AREX NasdaqGS Approach Resources Inc. 2.24\nMarketCap IPOyear Sector\n1 $665.14B 1980 Technology\n2 $1.61B 2006 Health Care\n3 $10.13B 1999 Miscellaneous\n4 $313.34B 1997 Consumer Services\n5 $6.6B NA Consumer Services\n6 $90.65M 2007 Energy\nIndustry Exchange.y",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 111,
      "chunk_index": 2
    }
  },
  {
    "text": "6 $90.65M 2007 Energy\nIndustry Exchange.y\n1 Computer Manufacturing NASDAQ\n2 Biotechnology: Biological Products (No Diagnostic Substances) NASDAQ\n3 Business Services NASDAQ\n4 Catalog/Specialty Distribution NASDAQ\n5 Real Estate Investment Trusts NYSE\n6 Oil & Gas Production NASDAQ\n98\nNow suppose we want to find the CEOs of these companies. There\nis no one file with compay CEO listings freely available for download.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 111,
      "chunk_index": 3
    }
  },
  {
    "text": "112 data science: theories, models, algorithms, and analytics\nHowever, sites like Google Finance have a page for each stock and men-\ntion the CEOs name on the page. By writing R code to scrape the data\noff these pages one by one, we can extract these CEO names and aug-\nment the tickers dataframe. The code for this is simple in R.\nlibrary( stringr )\n#READ IN THE LIST OF TICKERS\ntickers = read. table(\" tickers .csv\" ,header=FALSE,sep=\" : \")\nn = dim( tickers )[ 1 ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 112,
      "chunk_index": 0
    }
  },
  {
    "text": "n = dim( tickers )[ 1 ]\nnames( tickers ) = c(\"Exchange\" ,\"Symbol\")\ntickers$ceo = NA\n#PULL CEO NAMES FROM GOOGLE FINANCE\nfor ( j in 1 :n) {\nurl = paste(\"https : / /www.google.com/ finance?q=\" , tickers [j , 2 ] ,sep=\"\")\ntext = readLines(url)\nidx = grep(\"Chief Executive\" ,text )\nif (length(idx)> 0 ) {\ntickers [j , 3 ] = str_split ( text [idx 2 ],\">\" )[[ 1 ]][ 2 ]\n−\n}\nelse {\n3\ntickers [j , ] = NA\n}\nprint( tickers [j ,])\n}\n#WRITE CEO_NAMES TO CSV\nwrite . table( tickers , file=\"ceo_names.csv\" ,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 112,
      "chunk_index": 1
    }
  },
  {
    "text": "write . table( tickers , file=\"ceo_names.csv\" ,\nrow.names=FALSE,sep=\" ,\")\nThe code uses the stringr package so that string handling is simpli-\nfied. After extracting the page, we search for the line in which the words\n“Chief Executive” show up, and we note that the name of the CEO ap-\npears two lines before in the html page. A sample web page for Apple\n44\nInc is shown in Figure . .\n6\nThe final dataframe with CEO names is shown here (the top lines):\n> head( tickers )\nExchange Symbol ceo",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 112,
      "chunk_index": 2
    }
  },
  {
    "text": "> head( tickers )\nExchange Symbol ceo\n1 NasdaqGS ACOR Ron Cohen M.D.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 112,
      "chunk_index": 3
    }
  },
  {
    "text": "more: data handling and other useful things 113\nFigure4.4: GoogleFinance: the\nAAPLwebpageshowingtheURL\nwhichisneededtodownloadthe\npage.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 113,
      "chunk_index": 0
    }
  },
  {
    "text": "114 data science: theories, models, algorithms, and analytics\n2\nNasdaqGS AKAM F. Thomson Leighton\n3 NYSE ARE Joel S. Marcus J .D. , CPA\n4\nNasdaqGS AMZN Jeffrey P. Bezos\n5 NasdaqGS AAPL Timothy D. Cook\n6\nNasdaqGS AREX J . Ross Craft\n4.3 Using the apply class of functions\nSometimes we need to apply a function to many cases, and these case\nparameters may be supplied in a vector, matrix, or list. This is analogous\nto looping through a set of values to repeat evaluations of a function",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 114,
      "chunk_index": 0
    }
  },
  {
    "text": "using different sets of parameters. We illustrate here by computing the\nmean returns of all stocks in our sample using the apply function. The\nfirst argument of the function is the data.frame to which it is being ap-\n1 2\nplied, the second argument is either (by rows) or (by columns). The\nthird argument is the function being evaluated.\napply( rets [ , 1 :( length( tickers ) 1 )], 2 ,mean)\n−\nAAPL.Adjusted YHOO.Adjusted IBM.Adjusted CSCO.Adjusted C.Adjusted",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 114,
      "chunk_index": 1
    }
  },
  {
    "text": "1 073902 03 1 302309 04 2 388207 04 6 629946 05 9 833602 04\n. e . e . e . e . e\n− − − − − −\nWe see that the function returns the column means of the data set.\nThe variants of the function pertain to what the loop is being applied to.\nThe lapply is a function applied to a list, and sapply is for matrices and\nvectors. Likewise, mapply uses multiple arguments.\nTo cross check, we can simply use the colMeans function:\ncolMeans( rets [ , 1 :( length( tickers ) 1 )])\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 114,
      "chunk_index": 2
    }
  },
  {
    "text": "colMeans( rets [ , 1 :( length( tickers ) 1 )])\n−\nAAPL.Adjusted YHOO.Adjusted IBM.Adjusted CSCO.Adjusted C.Adjusted\n1 073902 03 1 302309 04 2 388207 04 6 629946 05 9 833602 04\n. e . e . e . e . e\n− − − − − −\nAs we see, this result is verified.\n4.4 Getting interest rate data from FRED\nIn finance, data on interest rates is widely used. An authoritative source\nof data on interest rates is FRED (Federal Reserve Economic Data), main-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 114,
      "chunk_index": 3
    }
  },
  {
    "text": "tained by the St. Louis Federal Reserve Bank, and is warehoused at the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 114,
      "chunk_index": 4
    }
  },
  {
    "text": "more: data handling and other useful things 115\nfollowing web site: https://research.stlouisfed.org/fred2/. Let’s as-\nsume that we want to download the data using R from FRED directly. To\ndo this we need to write some custom code. There used to be a package\nfor this but since the web site changed, it has been updated but does not\nwork properly. Still, see that it is easy to roll your own code quite easily\nin R.\n#FUNCTION TO READ IN CSV FILES FROM FRED\n#Enter SeriesID as a text string",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 115,
      "chunk_index": 0
    }
  },
  {
    "text": "#Enter SeriesID as a text string\nreadFRED = function(SeriesID) {\nurl = paste(\"https : / / research . stlouisfed .org/fred 2/ series /\" ,SeriesID ,\n\"/downloaddata/\" ,SeriesID , \" .csv\" ,sep=\"\")\ndata = readLines(url)\nn = length(data)\ndata = data[ 2 :n]\nn = length(data)\ndf = matrix( 0 ,n, 2 ) #top line is header\nfor ( j in 1 :n) {\ntmp = strsplit (data[ j ] ,\" ,\")\ndf[j , 1 ] = tmp[[ 1 ]][ 1 ]\ndf[j , 2 ] = tmp[[ 1 ]][ 2 ]\n}\nrate = as.numeric(df[ , 2 ])\nidx = which(rate > 0 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 115,
      "chunk_index": 1
    }
  },
  {
    "text": "idx = which(rate > 0 )\nidx = setdiff (seq( 1 ,n) ,idx)\n99\nrate [idx] =\n−\ndate = df[ , 1 ]\ndf = data.frame(date , rate )\nnames(df)[ 2 ] = SeriesID\nresult = df\n}\nNow, we provide a list of economic time series and download data\naccordingly using the function above. Note that we also join these indi-\nvidual series using the data as index. We download constant maturity\n1\ninterest rates (yields) starting from a maturity of one month (DGS MO)\n30\nto a maturity of thirty years (DGS ).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 115,
      "chunk_index": 2
    }
  },
  {
    "text": "30\nto a maturity of thirty years (DGS ).\n#EXTRACT TERM STRUCTURE DATA FOR ALL RATES FROM 1 MO to 30 YRS FROM FRED\nid_list = c(\"DGS 1 MO\" ,\"DGS 3 MO\" ,\"DGS 6 MO\" ,\"DGS 1 \" ,\"DGS 2 \" ,\"DGS 3 \" ,\"DGS 5 \" ,\"DGS 7 \" ,\n10 20 30\n\"DGS \" ,\"DGS \" ,\"DGS \")",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 115,
      "chunk_index": 3
    }
  },
  {
    "text": "116 data science: theories, models, algorithms, and analytics\n0\nk =\nfor (id in id_list ) {\nout = readFRED(id)\nif (k> 0 ) { rates = merge(rates ,out , \"date\" , all=TRUE) }\nelse { rates = out }\n1\nk = k +\n}\n> head( rates )\ndate DGS 1 MO DGS 3 MO DGS 6 MO DGS 1 DGS 2 DGS 3 DGS 5 DGS 7 DGS 10 DGS 20 DGS 30\n1 2001 07 31 3 67 3 54 3 47 3 53 3 79 4 06 4 57 4 86 5 07 5 61\n. . . . . . . . . .\n− −\n5 51\n.\n2 2001 08 01 3 65 3 53 3 47 3 56 3 83 4 09 4 62 4 90 5 11 5 63\n. . . . . . . . . .\n− −\n5 53\n.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 116,
      "chunk_index": 0
    }
  },
  {
    "text": ". . . . . . . . . .\n− −\n5 53\n.\n3 2001 08 02 3 65 3 53 3 46 3 57 3 89 4 17 4 69 4 97 5 17 5 68\n. . . . . . . . . .\n− −\n5 57\n.\n4 2001 08 03 3 63 3 52 3 47 3 57 3 91 4 22 4 72 4 99 5 20 5 70\n. . . . . . . . . .\n− −\n5 59\n.\n5 2001 08 06 3 62 3 52 3 47 3 56 3 88 4 17 4 71 4 99 5 19 5 70\n. . . . . . . . . .\n− −\n5 59\n.\n6 2001 08 07 3 63 3 52 3 47 3 56 3 90 4 19 4 72 5 00 5 20 5 71\n. . . . . . . . . .\n− −\n5 60\n.\nHaving done this, we now have a data.frame called rates containing",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 116,
      "chunk_index": 1
    }
  },
  {
    "text": "all the time series we are interested in. We now convert the dates into\nnumeric strings and sort the data.frame by date.\n#CONVERT ALL DATES TO NUMERIC AND SORT BY DATE\n1\ndates = rates [ , ]\nlibrary( stringr )\ndates = as.numeric( str_replace_all (dates , \" \" ,\"\" ))\n−\nres = sort(dates ,index. return=TRUE)\nfor ( j in 1 :dim( rates )[ 2 ]) {\nrates [ , j ] = rates [res$ix , j ]\n}\n> head( rates )\ndate DGS 1 MO DGS 3 MO DGS 6 MO DGS 1 DGS 2 DGS 3 DGS 5 DGS 7 DGS 10 DGS 20 DGS 30",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 116,
      "chunk_index": 2
    }
  },
  {
    "text": "1 1962 01 02 3 22 3 70 3 88 4 06\nNA NA NA . NA . . NA . NA\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 116,
      "chunk_index": 3
    }
  },
  {
    "text": "more: data handling and other useful things 117\nNA\n2 1962 01 03 3 24 3 70 3 87 4 03\nNA NA NA . NA . . NA . NA\n− −\nNA\n3 1962 01 04 3 24 3 69 3 86 3 99\nNA NA NA . NA . . NA . NA\n− −\nNA\n4 1962 01 05 3 26 3 71 3 89 4 02\nNA NA NA . NA . . NA . NA\n− −\nNA\n5 1962 01 08 3 31 3 71 3 91 4 03\nNA NA NA . NA . . NA . NA\n− −\nNA\n6 1962 01 09 3 32 3 74 3 93 4 05\nNA NA NA . NA . . NA . NA\n− −\nNA\nNote that there are missing values, denoted by NA. Also there are rows\n99",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 117,
      "chunk_index": 0
    }
  },
  {
    "text": "99\nwith \"- \" values and we can clean those out too but they represent peri-\nods when there was no yield available of that maturity, so we leave this\nin.\n#REMOVE THE NA ROWS\nidx = which(rowSums( is .na( rates))== 0 )\n2\nrates = rates [idx ,]\nprint(head(rates 2 ))\ndate DGS 1 MO DGS 3 MO DGS 6 MO DGS 1 DGS 2 DGS 3 DGS 5 DGS 7 DGS 10 DGS 20 DGS 30\n10326 2001 07 31 3 67 3 54 3 47 3 53 3 79 4 06 4 57 4 86 5 07 5 61\n. . . . . . . . . .\n− −\n5 51\n.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 117,
      "chunk_index": 1
    }
  },
  {
    "text": ". . . . . . . . . .\n− −\n5 51\n.\n10327 2001 08 01 3 65 3 53 3 47 3 56 3 83 4 09 4 62 4 90 5 11 5 63\n. . . . . . . . . .\n− −\n5 53\n.\n10328 2001 08 02 3 65 3 53 3 46 3 57 3 89 4 17 4 69 4 97 5 17 5 68\n. . . . . . . . . .\n− −\n5 57\n.\n10329 2001 08 03 3 63 3 52 3 47 3 57 3 91 4 22 4 72 4 99 5 20 5 70\n. . . . . . . . . .\n− −\n5 59\n.\n10330 2001 08 06 3 62 3 52 3 47 3 56 3 88 4 17 4 71 4 99 5 19 5 70\n. . . . . . . . . .\n− −\n5 59\n.\n10331 2001 08 07 3 63 3 52 3 47 3 56 3 90 4 19 4 72 5 00 5 20 5 71",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 117,
      "chunk_index": 2
    }
  },
  {
    "text": ". . . . . . . . . .\n− −\n5 60\n.\n4.5 Cross-Sectional Data (an example)\nA great resource for data sets in corporate finance is on Aswath Damodaran’s\nweb site, see:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 117,
      "chunk_index": 3
    }
  },
  {
    "text": "118 data science: theories, models, algorithms, and analytics\nhttp://people.stern.nyu.edu/adamodar/New_Home_Page/data.html\nFinancial statement data sets are available at:\nhttp://www.sec.gov/dera/data/financial-statement-data-sets.html\nAnd another comprehensive data source:\nhttp://fisher.osu.edu/fin/fdf/osudata.htm\nOpen government data: https://www.data.gov/finance/\nLet’s read in the list of failed banks:\nhttp://www.fdic.gov/bank/individual/failed/banklist.csv",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 118,
      "chunk_index": 0
    }
  },
  {
    "text": "#download . file ( url=\"http : / /www. fdic .gov/bank/ individual / failed /\nbanklist .csv\" , destfile=\"failed_banks.csv\")\n(This does not work, and has been an issue for a while.)\nYou can also read in the data using readLines but then further work is\nrequired to clean it up, but it works well in downloading the data.\ndata = readLines(\"https://www.fdic.gov/bank/individual/failed/banklist.csv\")\nhead(data)\n[1] \"Bank Name,City ,ST,CERT,Acquiring Institution ,Closing Date,Updated Date\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 118,
      "chunk_index": 1
    }
  },
  {
    "text": "[2] \"Hometown National Bank,Longview,WA,35156,Twin City Bank,2 Oct 15,15 Oct 15\"\n− − − −\n[3] \"The Bank of Georgia,Peachtree City ,GA,35259,Fidelity Bank,2 Oct 15,15 Oct 15\"\n− − − −\n[4] \"Premier Bank,Denver,CO,34112,\\\"United Fidelity Bank, fsb\\\",10 Jul 15,28 Jul 15\"\n− − − −\n[5] \"Edgebrook Bank,Chicago,IL,57772,Republic Bank of Chicago,8 May 15,23 Jul 15\"\n− − − −\n[6] \"Doral Bank,San Juan,PR,32102,Banco Popular de Puerto Rico,27 Feb 15,13 May 15\"\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 118,
      "chunk_index": 2
    }
  },
  {
    "text": "− − − −\nIt may be simpler to just download the data and read it in from the\ncsv file:\ndata = read.csv(\"banklist .csv\" ,header=TRUE)\nprint(names(data))\n1\n[ ] \"Bank.Name\" \"City\" \"ST\"\n4\n[ ] \"CERT\" \"Acquiring. Institution \" \"Closing .Date\"\n7\n[ ] \"Updated.Date\"\nThis gives a data.frame which is easy to work with. We will illustrate\nsome interesting ways in which to manipulate this data. Suppose we\nwant to get subtotals of how many banks failed by state. First add a\ncolumn of ones to the data.frame.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 118,
      "chunk_index": 3
    }
  },
  {
    "text": "column of ones to the data.frame.\nprint(head(data))\ndata$count = 1\nprint(head(data))\nBank.Name City ST",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 118,
      "chunk_index": 4
    }
  },
  {
    "text": "more: data handling and other useful things 119\n1\nHometown National Bank Longview WA\n2\nThe Bank of Georgia Peachtree City GA\n3\nPremier Bank Denver CO\n4\nEdgebrook Bank Chicago IL\n5\nDoral Bank San Juan PR\n6 Capitol City Bank & Trust Company Atlanta GA\nCERT Acquiring. Institution Closing .Date\n1 35156 2 15\nTwin City Bank Oct\n− −\n2 35259 2 15\nFidelity Bank Oct\n− −\n3 34112 10 15\nUnited Fidelity Bank, fsb Jul\n− −\n4 57772 8 15\nRepublic Bank of Chicago May\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 119,
      "chunk_index": 0
    }
  },
  {
    "text": "− −\n4 57772 8 15\nRepublic Bank of Chicago May\n− −\n5 32102 Banco Popular de Puerto Rico 27 Feb 15\n− −\n6 33938 First Citizens Bank & Trust Company 13 Feb 15\n− − −\nUpdated.Date count\n1 15 15 1\nOct\n− −\n2 15 15 1\nOct\n− −\n3 28 15 1\nJul\n− −\n4 23 15 1\nJul\n− −\n5 13 15 1\nMay\n− −\n6 21 15 1\nApr\n− −\nIt’s good to check that there is no missing data.\nany( is .na(data))\n1\n[ ] FALSE\nNow we sort the data by state to see how many there are.\nres = sort(as.matrix(data$ST) ,index. return=TRUE)\nprint(data[res$ix ,])",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 119,
      "chunk_index": 1
    }
  },
  {
    "text": "print(data[res$ix ,])\nprint(sort(unique(data$ST)))\n1\n[ ] AL AR AZ CA CO CT FL GA HI IA ID IL IN KS KY LA MAMD\n19\n[ ] MI MNMO MS NC NE NH NJ NM NV NY OH OK OR PA PR SC SD\n37\n[ ] TN TX UT VA WA WI WVWY\n44\nLevels : AL AR AZ CA CO CT FL GA HI IA ID IL IN ... WY\nprint(length(unique(data$ST)))\n1 44\n[ ]\nWe can directly use the aggregate function to get subtotals by state.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 119,
      "chunk_index": 2
    }
  },
  {
    "text": "120 data science: theories, models, algorithms, and analytics\nhead(aggregate(count ~ ST,data ,sum) , 10 )\nST count\n1 7\nAL\n2 3\nAR\n3 16\nAZ\n4 41\nCA\n5 10\nCO\n6 2\nCT\n7 75\nFL\n8 92\nGA\n9 1\nHI\n10 2\nIA\nAnd another example, subtotal by acquiring bank. Note how we take the\nsubtotals into another data.frame, which is then sorted and returned in\norder using the index of the sort.\nacq = aggregate(count~Acquiring. Institution ,data ,sum)\nidx = sort(as.matrix(acq$count) ,decreasing=TRUE,index. return=TRUE)$ix\n15",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 120,
      "chunk_index": 0
    }
  },
  {
    "text": "15\nhead(acq[idx ,] , )\nAcquiring. Institution count\n170 31\nNo Acquirer\n224 12\nState Bank and Trust Company\n10 10\nAmeris Bank\n262 9\nU.S. Bank N.A.\n67 Community & Southern Bank 8\n28 7\nBank of the Ozarks\n47 7\nCentennial Bank\n112 First Citizens Bank & Trust Company 7\n−\n228 7\nStearns Bank, N.A.\n49 6\nCenterState Bank of Florida , N.A.\n50 6\nCentral Bank\n154 6\nMB Financial Bank, N.A.\n205 6\nRepublic Bank of Chicago\n54 5\nCertusBank , National Association\n64 5\nColumbia State Bank",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 120,
      "chunk_index": 1
    }
  },
  {
    "text": "more: data handling and other useful things 121\n4.6 Handling dates with lubridate\nSuppose we want to take the preceding data.frame of failed banks and\naggregate the data by year, or month, etc. In this case, it us useful to use\na dates package. Another useful tool developed by Hadley Wickham is\nthe lubridate package.\nhead(data)\nBank.Name City ST CERT\n1 35156\nHometown National Bank Longview WA\n2 35259\nThe Bank of Georgia Peachtree City GA\n3 34112\nPremier Bank Denver CO\n4 57772",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 121,
      "chunk_index": 0
    }
  },
  {
    "text": "3 34112\nPremier Bank Denver CO\n4 57772\nEdgebrook Bank Chicago IL\n5 32102\nDoral Bank San Juan PR\n6 Capitol City Bank & Trust Company Atlanta GA 33938\nAcquiring. Institution Closing .Date Updated.Date count\n1 2 15 15 15 1\nTwin City Bank Oct Oct\n− − − −\n2 2 15 15 15 1\nFidelity Bank Oct Oct\n− − − −\n3 10 15 28 15 1\nUnited Fidelity Bank, fsb Jul Jul\n− − − −\n4 8 15 23 15 1\nRepublic Bank of Chicago May Jul\n− − − −\n5 Banco Popular de Puerto Rico 27 Feb 15 13 May 15 1\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 121,
      "chunk_index": 1
    }
  },
  {
    "text": "− − − −\n6 First Citizens Bank & Trust Company 13 Feb 15 21 Apr 15 1\n− − − − −\nCdate Cyear\n1 2015 10 02 2015\n− −\n2 2015 10 02 2015\n− −\n3 2015 07 10 2015\n− −\n4 2015 05 08 2015\n− −\n5 2015 02 27 2015\n− −\n6 2015 02 13 2015\n− −\nlibrary(lubridate)\ndata$Cdate = dmy(data$Closing .Date)\ndata$Cyear = year(data$Cdate)\nfd = aggregate(count~Cyear ,data ,sum)\nprint(fd)\nCyear count\n1 2000 2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 121,
      "chunk_index": 2
    }
  },
  {
    "text": "122 data science: theories, models, algorithms, and analytics\n2 2001 4\n3 2002 11\n4 2003 3\n5 2004 4\n6 2007 3\n7 2008 25\n8 2009 140\n9 2010 157\n10 2011 92\n11 2012 51\n12 2013 24\n13 2014 18\n14 2015 8\nplot(count~Cyear ,data=fd ,type=\"l\" ,lwd= 3 ,col=\"red\"xlab=\"Year\")\ngrid(lwd= 3 )\n45\nSee the results in Figure . .\nFigure4.5: Failedbanktotalsby\nyear.\nLet’s do the same thing by month to see if there is seasonality\ndata$Cmonth = month(data$Cdate)\nfd = aggregate(count~Cmonth,data ,sum)\nprint(fd)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 122,
      "chunk_index": 0
    }
  },
  {
    "text": "more: data handling and other useful things 123\nCmonth count\n1 1 49\n2 2 44\n3 3 38\n4 4 57\n5 5 40\n6 6 36\n7 7 74\n8 8 40\n9 9 37\n10 10 58\n11 11 35\n12 12 34\nplot(count~Cmonth,data=fd ,type=\"l\" ,lwd= 3 ,col=\"green\" ); grid(lwd= 3 )\nThere does not appear to be any seasonality. What about day?\ndata$Cday = day(data$Cdate)\nfd = aggregate(count~Cday,data ,sum)\nprint(fd)\nCday count\n1 1 8\n2 2 20\n3 3 3\n4 4 21\n5 5 15\n6 6 13\n7 7 20\n8 8 14\n9 9 10\n10 10 14\n11 11 17\n12 12 10\n13 13 14\n14 14 20\n15 15 20\n16 16 22",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 123,
      "chunk_index": 0
    }
  },
  {
    "text": "12 12 10\n13 13 14\n14 14 20\n15 15 20\n16 16 22\n17 17 23",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 123,
      "chunk_index": 1
    }
  },
  {
    "text": "124 data science: theories, models, algorithms, and analytics\n18 18 21\n19 19 29\n20 20 27\n21 21 17\n22 22 18\n23 23 30\n24 24 19\n25 25 13\n26 26 15\n27 27 18\n28 28 18\n29 29 15\n30 30 30\n31 31 8\nplot(count~Cday,data=fd ,type=\"l\" ,lwd= 3 ,col=\"blue\" ); grid(lwd= 3 )\nDefinitely, counts are lower at the start and end of the month!\n4.7 Using the data.table package\nThis is an incredibly useful package that was written by Matt Dowle.\nIt essentially allows your data.frame to operate as a database. It en-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 124,
      "chunk_index": 0
    }
  },
  {
    "text": "ables very fast handling of massive quantities of data, and much of\nthis technology is now embedded in the IP of the company called h2o:\nhttp://h2o.ai/\nWe start with some freely downloadable crime data statistics for Cal-\nifornia. We placed the data in a csv file which is then easy to read in to\nR.\ndata = read.csv(\"CA_Crimes_Data_2004 2013 .csv\" ,header=TRUE)\n−\nIt is easy to convert this into a data.table.\nlibrary(data. table)\nD_T = as.data. table(data)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 124,
      "chunk_index": 1
    }
  },
  {
    "text": "library(data. table)\nD_T = as.data. table(data)\nLet’s see how it works, noting that the syntax is similar to that for\ndata.frames as much as possible. We print only a part of the names list.\nAnd do not go through each and everyone.\nprint(dim(D_T))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 124,
      "chunk_index": 2
    }
  },
  {
    "text": "more: data handling and other useful things 125\n1 7301 69\n[ ]\nprint(names(D_T))\n1\n[ ] \"Year\" \"County\" \"NCICCode\"\n[ 4 ] \"Violent_sum\" \"Homicide_sum\" \"ForRape_sum\"\n[ 7 ] \"Robbery_sum\" \"AggAssault_sum\" \"Property_sum\"\n[ 10 ] \"Burglary_sum\" \"VehicleTheft_sum\" \"LTtotal_sum\"\n. . . .\nhead(D_T)\nA nice feature of the data.table is that it can be indexed, i.e., resorted\non the fly by making any column in the database the key. Once that is\ndone, then it becomes easy to compute subtotals, and generate plots",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 125,
      "chunk_index": 0
    }
  },
  {
    "text": "from these subtotals as well.\nsetkey(D_T,Year)\n6\ncrime =\nres = D_T[ ,sum(ForRape_sum) ,by=Year]\nprint(res)\n1\nYear V\n1 2004 9598\n:\n2 2005 9345\n:\n3 2006 9213\n:\n4 2007 9047\n:\n5 2008 8906\n:\n6 2009 8698\n:\n7 2010 8325\n:\n8 2011 7678\n:\n9 2012 7828\n:\n10 2013 7459\n:\nclass(res)\n1\n[ ] \"data. table\" \"data.frame\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 125,
      "chunk_index": 1
    }
  },
  {
    "text": "126 data science: theories, models, algorithms, and analytics\nSee that the type of output is also of the type data.table, and includes the\nclass data.frame also.\nNext, we plot the results from the data.table in the same way as we\n46\nwould for a data.frame. See Figure . .\nplot(res$Year , res$V 1 ,type=\"b\" ,lwd= 3 ,col=\"blue\" ,\nxlab=\"Year\" ,ylab=\"Forced Rape\")\nFigure4.6: Rapetotalsbyyear.\nRepeat the process looking at crime (Rape) totals by county.\nsetkey(D_T,County)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 126,
      "chunk_index": 0
    }
  },
  {
    "text": "setkey(D_T,County)\nres = D_T[ ,sum(ForRape_sum) ,by=County]\nprint(res)\n1\nsetnames(res , \"V \" ,\"Rapes\")\nCounty_Rapes = as.data. table(res) #This is not really needed\nsetkey(County_Rapes,Rapes)\nCounty_Rapes\nCounty Rapes\n1 2\n: Sierra County\n2 15\n: Alpine County\n3 28\n: Trinity County\n4 46\n: Mariposa County",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 126,
      "chunk_index": 1
    }
  },
  {
    "text": "more: data handling and other useful things 127\n5 52\n: Inyo County\n6 56\n: Glenn County\n7 60\n: Colusa County\n8 61\n: Mono County\n9 64\n: Modoc County\n10 96\n: Lassen County\n11 115\n: Plumas County\n12 143\n: Siskiyou County\n13 148\n: Calaveras County\n14 151\n: San Benito County\n15 153\n: Amador County\n16 160\n: Tuolumne County\n17 165\n: Tehama County\n18 214\n: Nevada County\n19 236\n: Del Norte County\n20 262\n: Lake County\n21 263\n: Imperial County\n22 274\n: Sutter County\n23 277\n: Yuba County\n24 328",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 127,
      "chunk_index": 0
    }
  },
  {
    "text": ": Sutter County\n23 277\n: Yuba County\n24 328\n: Mendocino County\n25 351\n: El Dorado County\n26 354\n: Napa County\n27 356\n: Kings County\n28 408\n: Madera County\n29 452\n: Marin County\n30 495\n: Humboldt County\n31 611\n: Placer County\n32 729\n: Yolo County\n33 738\n: Merced County\n34 865\n: Santa Cruz County\n35 900\n: San Luis Obispo County\n36 930\n: Butte County\n37 1062\n: Monterey County\n38 1089\n: Shasta County\n39 1114\n: Tulare County\n40 1146\n: Ventura County\n41 1150\n: Solano County\n42 1348",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 127,
      "chunk_index": 1
    }
  },
  {
    "text": ": Ventura County\n41 1150\n: Solano County\n42 1348\n: Stanislaus County",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 127,
      "chunk_index": 2
    }
  },
  {
    "text": "128 data science: theories, models, algorithms, and analytics\n43 1352\n: Santa Barbara County\n44 1381\n: San Mateo County\n45 1498\n: San Francisco County\n46 1558\n: Sonoma County\n47 1612\n: San Joaquin County\n48 1848\n: Contra Costa County\n49 1935\n: Kern County\n50 1960\n: Fresno County\n51 3832\n: Santa Clara County\n52 4084\n: Sacramento County\n53 4321\n: Riverside County\n54 4509\n: Orange County\n55 4900\n: San Bernardino County\n56 4979\n: Alameda County\n57 7378\n: San Diego County\n58 21483",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 128,
      "chunk_index": 0
    }
  },
  {
    "text": "57 7378\n: San Diego County\n58 21483\n: Los Angeles County\nNow, we can go ahead and plot it using a different kind of plot, a\nhorizontal barplot.\npar( las= 2 ) #makes label horizontal\n#par(mar=c(3 ,4 ,2 ,1)) #increase y axis margins\n−\nbarplot(County_Rapes$Rapes, names.arg=County_Rapes$County,\nhoriz=TRUE, cex.names= 0 . 4 , col= 8 )\n4.8 Another data set: Bay Area Bike Share data\nWe show some other features using a different data set, the bike infor-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 128,
      "chunk_index": 1
    }
  },
  {
    "text": "mation on Silicon Valley routes for the Bike Share program. This is a\nmuch larger data set.\ntrips = read.csv(\" 201408_trip_data.csv\" ,header=TRUE)\nprint(names( trips ))\n1\n[ ] \"Trip .ID\" \"Duration\" \"Start .Date\"\n4\n[ ] \"Start . Station\" \"Start .Terminal\" \"End.Date\"\n7\n[ ] \"End. Station\" \"End.Terminal\" \"Bike .. \"\n10\n[ ] \"Subscriber .Type\" \"Zip.Code\"\nNext we print some descriptive statistics.\nprint(length( trips$Trip .ID))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 128,
      "chunk_index": 2
    }
  },
  {
    "text": "more: data handling and other useful things 129\nFigure4.7: Rapetotalsbycounty.\n1 171792\n[ ]\nprint(summary( trips$Duration/ 60 ))\n1 3\nMin. st Qu. Median Mean rd Qu. Max.\n1 000 5 750 8 617 18 880 12 680 11940 000\n. . . . . .\nprint(mean( trips$Duration/60 ,trim= 0 . 01 ))\n1 13 10277\n[ ] .\nNow, we quickly check how many start and end stations there are.\nstart_stn = unique( trips$Start .Terminal)\nprint(sort( start_stn ))\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 21 22 23 24 25 26 27 28\n[ ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 129,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ]\n23 29 30 31 32 33 34 35 36 37 38 39 41 42 45 46 47 48 49 50 51 54 55\n[ ]\n45 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77\n[ ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 129,
      "chunk_index": 1
    }
  },
  {
    "text": "130 data science: theories, models, algorithms, and analytics\n67 80 82 83 84\n[ ]\nprint(length( start_stn ))\n1 70\n[ ]\nend_stn = unique( trips$End.Terminal)\nprint(sort(end_stn ))\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 21 22 23 24 25 26 27 28\n[ ]\n23 29 30 31 32 33 34 35 36 37 38 39 41 42 45 46 47 48 49 50 51 54 55\n[ ]\n45 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77\n[ ]\n67 80 82 83 84\n[ ]\nprint(length(end_stn ))\n1 70\n[ ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 130,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ]\nprint(length(end_stn ))\n1 70\n[ ]\nAs we can see, there are quite a few stations in the bike share program\nwhere riders can pick up and drop off bikes. The trip duration informa-\ntion is stored in seconds, so has been converted to minutes in the code\nabove.\n4.9 Using the plyr package family\nThis package by Hadley Wickham is useful for applying functions to\ntables of data, i.e., data.frames. Since we may want to write custom",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 130,
      "chunk_index": 1
    }
  },
  {
    "text": "functions, this is a highly useful package. R users often select either\nthe data.table or the plyr class of packages for handling data.frames\nas databases. The latest incarnation is the dplyr package, which focuses\nonly on data.frames.\nrequire(plyr)\nlibrary(dplyr)\nOne of the useful things you can use is the filter function, to subset\nthe rows of the dataset you might want to select for further analysis.\n50 51\nres = filter (trips , Start .Terminal== ,End.Terminal== )\nhead(res)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 130,
      "chunk_index": 2
    }
  },
  {
    "text": "head(res)\nTrip .ID Duration Start .Date\n1 432024 3954 8/30/2014 14\n:\n46",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 130,
      "chunk_index": 3
    }
  },
  {
    "text": "more: data handling and other useful things 131\n2 432022 4120 8/30/2014 14\n:\n44\n3 431895 1196 8/30/2014 12\n:\n04\n4 431891 1249 8/30/2014 12\n:\n03\n5 430408 145 8/29/2014 9\n:\n08\n6 429148 862 8/28/2014 13\n:\n47\nStart . Station Start .Terminal End.Date\n1\nHarry Bridges Plaza (Ferry Building)\n50 8/30/2014 15\n:\n52\n2\nHarry Bridges Plaza (Ferry Building)\n50 8/30/2014 15\n:\n52\n3\nHarry Bridges Plaza (Ferry Building)\n50 8/30/2014 12\n:\n24\n4\nHarry Bridges Plaza (Ferry Building)\n50 8/30/2014 12\n:\n23\n5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 131,
      "chunk_index": 0
    }
  },
  {
    "text": "50 8/30/2014 12\n:\n23\n5\nHarry Bridges Plaza (Ferry Building)\n50 8/29/2014 9\n:\n11\n6\nHarry Bridges Plaza (Ferry Building)\n50 8/28/2014 14\n:\n02\nEnd. Station End.Terminal Bike .. Subscriber .Type Zip.Code\n1 51 306 94952\nEmbarcadero at Folsom Customer\n2 51 659 94952\nEmbarcadero at Folsom Customer\n3 51 556 11238\nEmbarcadero at Folsom Customer\n4 51 621 11238\nEmbarcadero at Folsom Customer\n5 51 400 94070\nEmbarcadero at Folsom Subscriber\n6 51 589 94107\nEmbarcadero at Folsom Subscriber",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 131,
      "chunk_index": 1
    }
  },
  {
    "text": "6 51 589 94107\nEmbarcadero at Folsom Subscriber\nThe arrange function is useful for sorting by any number of columns\nas needed. Here we sort by the start and end stations.\ntrips_sorted = arrange(trips , Start . Station ,End. Station)\nhead( trips_sorted)\nTrip .ID Duration Start .Date Start . Station Start .Terminal\n1 426408 120 8/27/2014 7\n:\n40 2\nnd at Folsom\n62\n2 411496 21183 8/16/2014 13\n:\n36 2\nnd at Folsom\n62\n3 396676 3707 8/6/2014 11\n:\n38 2\nnd at Folsom\n62\n4 385761 123 7/29/2014 19\n:\n52 2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 131,
      "chunk_index": 2
    }
  },
  {
    "text": "nd at Folsom\n62\n4 385761 123 7/29/2014 19\n:\n52 2\nnd at Folsom\n62\n5 364633 6395 7/15/2014 13\n:\n39 2\nnd at Folsom\n62\n6 362776 9433 7/14/2014 13\n:\n36 2\nnd at Folsom\n62\nEnd.Date End. Station End.Terminal Bike .. Subscriber .Type\n1 8/27/2014 7\n:\n42 2\nnd at Folsom\n62 527\nSubscriber\n2 8/16/2014 19\n:\n29 2\nnd at Folsom\n62 508\nCustomer\n3 8/6/2014 12\n:\n40 2\nnd at Folsom\n62 109\nCustomer\n4 7/29/2014 19\n:\n55 2\nnd at Folsom\n62 421\nSubscriber\n5 7/15/2014 15\n:\n26 2\nnd at Folsom\n62 448\nCustomer\n6 7/14/2014 16\n:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 131,
      "chunk_index": 3
    }
  },
  {
    "text": "nd at Folsom\n62 448\nCustomer\n6 7/14/2014 16\n:\n13 2\nnd at Folsom\n62 454\nCustomer",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 131,
      "chunk_index": 4
    }
  },
  {
    "text": "132 data science: theories, models, algorithms, and analytics\nZip.Code\n1 94107\n2 94105\n3 31200\n4 94107\n5 2184\n6 2184\nThe sort can also be done in reverse order as follows.\ntrips_sorted = arrange(trips ,desc( Start . Station ) ,End. Station)\nhead( trips_sorted)\nTrip .ID Duration Start .Date\n1 416755 285 8/20/2014 11\n:\n37\n2 411270 257 8/16/2014 7\n:\n03\n3 410269 286 8/15/2014 10\n:\n34\n4 405273 382 8/12/2014 14\n:\n27\n5 398372 401 8/7/2014 10\n:\n10\n6 393012 317 8/4/2014 10\n:\n59",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 132,
      "chunk_index": 0
    }
  },
  {
    "text": ":\n10\n6 393012 317 8/4/2014 10\n:\n59\nStart . Station Start .Terminal\n1 3 68\nYerba Buena Center of the Arts ( rd @ Howard)\n2 3 68\nYerba Buena Center of the Arts ( rd @ Howard)\n3 3 68\nYerba Buena Center of the Arts ( rd @ Howard)\n4 3 68\nYerba Buena Center of the Arts ( rd @ Howard)\n5 3 68\nYerba Buena Center of the Arts ( rd @ Howard)\n6 3 68\nYerba Buena Center of the Arts ( rd @ Howard)\nEnd.Date End. Station End.Terminal Bike .. Subscriber .Type\n1 8/20/2014 11\n:\n42 2\nnd at Folsom\n62 383\nCustomer",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 132,
      "chunk_index": 1
    }
  },
  {
    "text": ":\n42 2\nnd at Folsom\n62 383\nCustomer\n2 8/16/2014 7\n:\n07 2\nnd at Folsom\n62 614\nSubscriber\n3 8/15/2014 10\n:\n38 2\nnd at Folsom\n62 545\nSubscriber\n4 8/12/2014 14\n:\n34 2\nnd at Folsom\n62 344\nCustomer\n5 8/7/2014 10\n:\n16 2\nnd at Folsom\n62 597\nSubscriber\n6 8/4/2014 11\n:\n04 2\nnd at Folsom\n62 367\nSubscriber\nZip.Code\n1 95060\n2 94107\n3 94127\n4 94110\n5 94127",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 132,
      "chunk_index": 2
    }
  },
  {
    "text": "more: data handling and other useful things 133\n6 94127\nData.table also offers a fantastic way to do descriptive statistics! First,\ngroup the data by start point, and then produce statistics by this group,\nchoosing to count the number of trips starting from each station and the\naverage duration of each trip.\nbyStartStation = group_by(trips , Start . Station)\nres = summarise(byStartStation , count=n() , time=mean(Duration) /60 )\nprint(res)\nSource: local data frame [ 70 x 3 ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 133,
      "chunk_index": 0
    }
  },
  {
    "text": "print(res)\nSource: local data frame [ 70 x 3 ]\nStart . Station count time\n( fctr ) ( int ) (dbl)\n1 2 4165 9 32088\nnd at Folsom .\n2 2 4569 11 60195\nnd at South Park .\n3 2 6824 15 14786\nnd at Townsend .\n4 5 3183 14 23254\nth at Howard .\n5 Adobe on Almaden 360 10 . 06120\n6 Arena Green / SAP Center 510 43 . 82833\n7 4293 15 74702\nBeale at Market .\n8 22 54 82121\nBroadway at Main .\n9 2433 15 31862\nBroadway St at Battery St .\n10 329 51 30709\nCalifornia Ave Caltrain Station .\n.. ... ... ...",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 133,
      "chunk_index": 1
    }
  },
  {
    "text": "5\nBeing Mean with Variance: Markowitz Optimization\nIn this chapter, we will explore the mathematics of the famous portfolio\noptimization result, known as the Markowitz mean-variance problem.\nThe solution to this problem is still being used widely in practice. We are\ninterested in portfolios of n assets, which have a mean return which we\ndenote as E(r ), and a variance, denoted Var(r ).\np p\nLet w Rn be the portfolio weights. What this means is that we allo-\n∈\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 135,
      "chunk_index": 0
    }
  },
  {
    "text": "∈\n1\ncate each $ into various assets, such that the total of the weights sums\n1\nup to . Note that we do not preclude short-selling, so that it is possible\nfor weights to be negative as well.\n5.1 Quadratic (Markowitz) Problem\nThe optimization problem is defined as follows. We wish to find the\nportfolio that delivers the minimum variance (risk) while achieving a\npre-specified level of expected (mean) return.\n1\nmin w (cid:48) Σw\nw 2\nsubject to\nw µ = E(r )\n(cid:48) p\nw (cid:48)1 = 1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 135,
      "chunk_index": 1
    }
  },
  {
    "text": "subject to\nw µ = E(r )\n(cid:48) p\nw (cid:48)1 = 1\nNote that we have a 1 in front of the variance term above, which is for\n2\nmathematical neatness as will become clear shortly. The minimized solu-\ntion is not affected by scaling the objective function by a constant.\nThe first constraint forces the expected return of the portfolio to a\nspecified mean return, denoted E(r ), and the second constraint requires\np\n1\nthat the portfolio weights add up to , also known as the “fully invested”",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 135,
      "chunk_index": 2
    }
  },
  {
    "text": "constraint. It is convenient that the constraints are equality constraints.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 135,
      "chunk_index": 3
    }
  },
  {
    "text": "136 data science: theories, models, algorithms, and analytics\nThis is a Lagrangian problem, and requires that we embed the con-\nstraints into the objective function using Lagragian multipliers λ ,λ .\n1 2\n{ }\nThis results in the following minimization problem:\n1\nmin L = w (cid:48) Σw+λ 1 [E(r p ) w (cid:48) µ]+λ 2 [1 w (cid:48)1 ]\nw,λ 1,λ2 2 − −\nTo minimize this function, we take derivatives with respect to w, λ , and\n1\nλ , to arrive at the first order conditions:\n2\n∂L\n= Σw λ µ λ 1 = 0 ( )\n1 2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 136,
      "chunk_index": 0
    }
  },
  {
    "text": "2\n∂L\n= Σw λ µ λ 1 = 0 ( )\n1 2\n∂w − − ∗\n∂L\n= E(r p ) w (cid:48) µ = 0\n∂λ −\n1\n∂L\n= 1 w (cid:48)1 = 0\n∂λ −\n2\nThe first equation above, denoted (*), is a system of n equations, because\nthe derivative is taken with respect to every element of the vector w.\nHence, we have a total of (n+2) first-order conditions. From (*)\nw = Σ\n−\n1(λ\n1\nµ+λ\n2\n1)\n= λ\n1\nΣ\n−\n1µ+λ\n2\nΣ\n−\n11 ( )\n∗∗\nPremultiply (**) by µ :\n(cid:48)\nµ\n(cid:48)\nw = λ\n1\nµ\n(cid:48)\nΣ\n−\n1µ +λ\n2\nµ\n(cid:48)\nΣ\n−\n11 = E(r\np\n)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 136,
      "chunk_index": 1
    }
  },
  {
    "text": "(cid:48)\nΣ\n−\n1µ +λ\n2\nµ\n(cid:48)\nΣ\n−\n11 = E(r\np\n)\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nB A\nAlso premultiply (**) by 1 :\n(cid:48)\n1(cid:48) w = λ 1 1(cid:48) Σ − 1µ+λ 2 1(cid:48) Σ − 11 = 1\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nA C\nSolve for λ ,λ\n1 2\nCE(r ) A\np\nλ = −\n1\nD\nB AE(r )\np\nλ = −\n2\nD\nwhere D = BC A2\n−\nNote the following:\n• Since Σ is positive definite, Σ 1 is also positive definite: B > 0,C > 0.\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 136,
      "chunk_index": 2
    }
  },
  {
    "text": "being mean with variance: markowitz optimization 137\n• Given solutions for λ ,λ , we solve for w.\n1 2\n1 1\nw = [BΣ\n−\n11 AΣ\n−\n1µ]+ [CΣ\n−\n1µ AΣ\n−\n11] E(r\np\n)\nD − D − ·\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\ng h\nThis is the expression for the optimal portfolio weights that minimize\nthe variance for given expected return E(r ). We see that the vectors g,\np\nh are fixed once we are given the inputs to the problem, i.e., µ and Σ .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 137,
      "chunk_index": 0
    }
  },
  {
    "text": "• We can vary E(r ) to get a set of frontier (efficient or optimal) portfo-\np\nlios w.\nw = g+hE(r )\np\nif E(r ) = 0, w = g\np\nif E(r ) = 1, w = g+h\np\nNote that\nw = g+hE(r ) = [1 E(r )]g+E(r )[g+h]\np p p\n−\nHence these 2 portfolios g,g+h “generate” the entire frontier.\n5.1.1 Solution in R\nWe create a function to return the optimal portfolio weights.\nmarkowitz = function(mu,cv,Er) {\nn = length(mu)\nwuns = matrix( 1 ,n, 1 )\nA = t(wuns) %*% solve(cv) %*% mu\nB = t(mu) %*% solve(cv) %*% mu",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 137,
      "chunk_index": 1
    }
  },
  {
    "text": "B = t(mu) %*% solve(cv) %*% mu\nC = t(wuns) %*% solve(cv) %*% wuns\nD = B*C A^ 2\n−\nlam = (C*Er A) /D\n−\ngam = (B A*Er) /D\n−\nwts = lam[ 1 ]*(solve(cv) %*% mu) + gam[ 1 ]*(solve(cv) %*% wuns)\ng = (B[ 1 ]*(solve(cv) %*% wuns) A[ 1 ]*(solve(cv) %*% mu)) /D[ 1 ]\n−\nh = (C[ 1 ]*(solve(cv) %*% mu) A[ 1 ]*(solve(cv) %*% wuns)) /D[ 1 ]\n−\nwts = g + h*Er\n}\nWe can enter an example of a mean return vector and the covariance\nmatrix of returns, and then call the function for a given expected return.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 137,
      "chunk_index": 2
    }
  },
  {
    "text": "138 data science: theories, models, algorithms, and analytics\n#PARAMETERS\nmu = matrix(c( 0 . 02 , 0 . 10 , 0 . 20 ) , 3 , 1 )\nn = length(mu)\ncv = matrix(c( 0 . 0001 , 0 , 0 , 0 , 0 . 04 , 0 . 02 , 0 , 0 . 02 , 0 . 16 ) ,n,n)\n0 18\nEr = .\n#SOLVE PORTFOLIO PROBLEM\nwts = markowitz(mu,cv,Er)\nprint(wts)\nThe output is the vector of optimal portfolio weights:\n> source(\"markowitz.R\")\n1\n[ , ]\n1 0 3575931\n[ ,] .\n−\n2 0 8436676\n[ ,] .\n3 0 5139255\n[ ,] .\n010",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 138,
      "chunk_index": 0
    }
  },
  {
    "text": "−\n2 0 8436676\n[ ,] .\n3 0 5139255\n[ ,] .\n010\nIf we change the expected return to . , then we get a different set of\nportfolio weights.\n0 10\n> Er = .\n> wts = markowitz(mu,cv,Er)\n> print(wts)\n1\n[ , ]\n1 0 3209169\n[ ,] .\n2 0 4223496\n[ ,] .\n3 0 2567335\n[ ,] .\n018\nNote that in the first example, to get a high expected return of . , we\nneeded to take some leverage, by shorting the low risk asset and going\nlong the medium and high risk assets. When we dropped the expected\n010",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 138,
      "chunk_index": 1
    }
  },
  {
    "text": "010\nreturn to . , all weights are positive, i.e., we have a long-only portfolio.\n5.2 Solving the problem with the quadprog package\nThe quadprog package is an optimizer that takes a quadratic objective\nfunction with linear constraints. Hence, it is exactly what is needed\nfor the mean-variance portfolio problem we just considered. The ad-\nvantage of this package is that we can also apply additional inequality\nconstraints. For example, we may not wish to permit short-sales of any",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 138,
      "chunk_index": 2
    }
  },
  {
    "text": "being mean with variance: markowitz optimization 139\nasset, and thereby we might bound all the weights to lie between zero\nand one.\nThe specification in the quadprog package of the problem set up is\nshown in the manual:\nDescription\nThis routine implements the dual method of Goldfarb and\nIdnani ( 1982 , 1983 ) for solving quadratic programming\nproblems of the form min( d^T b + 1/2 b^T D b) with the\n−\nconstraints A^T b >=\nb_0\n.\n(note: b here is the weights vector in our problem)\nUsage",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 139,
      "chunk_index": 0
    }
  },
  {
    "text": "Usage\nsolve .QP(Dmat, dvec , Amat, bvec , meq= 0 , factorized=FALSE)\nArguments\nDmat matrix appearing in the quadratic function to be minimized.\ndvec vector appearing in the quadratic function to be minimized.\nAmat matrix defining the constraints under which we want\nto minimize the quadratic function .\nbvec vector holding the values of b_0 (defaults to zero ).\nmeq the first meq constraints are treated as equality\nconstraints , all further as inequality constraints\n0\n(defaults to ).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 139,
      "chunk_index": 1
    }
  },
  {
    "text": "0\n(defaults to ).\nfactorized logical flag : if TRUE, then we are passing R^( 1 )\n−\n(where D = R^T R) instead of the matrix D in the\nargument Dmat.\nIn our problem set up, with three securities, and no short sales, we will\nhave the following Amat and bvec:\n \nE(r )\np\n   \nµ 1 1 0 0 1\n1  \n \nA =  µ 1 0 1 0 ; b =  0 \n 2  0  \nµ 1 0 0 1  0 \n3  \n0\nThe constraints will be modulated by meq = 2, which states that the first",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 139,
      "chunk_index": 2
    }
  },
  {
    "text": "two constraints will be equality constraints, and the last three will be\ngreater than equal to constraints. The constraints will be of the form",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 139,
      "chunk_index": 3
    }
  },
  {
    "text": "140 data science: theories, models, algorithms, and analytics\nA w b , i.e.,\n(cid:48) 0\n≥\nw µ +w µ +w µ = E(r )\n1 1 2 2 3 3 p\nw 1+w 1+w 1 = 1\n1 2 3\nw 0\n1\n≥\nw 0\n2\n≥\nw 0\n3\n≥\nThe code for using the package is as follows.\nlibrary(quadprog)\nnss = 1 #Equals 1 if no short sales allowed\nBmat = matrix( 0 ,n,n) #No Short sales matrix\ndiag(Bmat) = 1\nAmat = matrix(c(mu, 1 , 1 , 1 ) ,n, 2 )\nif (nss== 1 ) { Amat = matrix(c(Amat,Bmat) ,n, 2 +n) }\ndvec = matrix( 0 ,n, 1 )\nbvec = matrix(c(Er, 1 ) , 2 , 1 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 140,
      "chunk_index": 0
    }
  },
  {
    "text": "bvec = matrix(c(Er, 1 ) , 2 , 1 )\nif (nss== 1 ) { bvec = t(c(bvec ,matrix( 0 , 3 , 1 ))) }\nsol = solve .QP(cv,dvec ,Amat,bvec ,meq= 2 )\nprint(sol$solution)\n018\nIf we run this code we get the following result for expected return = . ,\nwith short-selling allowed:\n1 0 3575931 0 8436676 0 5139255\n[ ] . . .\n−\nThis is exactly what is obtained from the Markowitz solution. Hence, the\nmodel checks out. What if we restricted short-selling? Then we would\nget the following solution.\n1 0 0 0 2 0 8\n[ ] . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 140,
      "chunk_index": 1
    }
  },
  {
    "text": "1 0 0 0 2 0 8\n[ ] . . .\n5.3 Tracing out the Efficient Frontier\nSince we can use the Markowitz model to solve for the optimal portfo-\nlio weights when the expected return is fixed, we can keep solving for\ndifferent values of E(r ). This will trace out the efficient frontier. The\np\nprogram to do this and plot the frontier is as follows.\n#TRACING OUT THE EFFICIENT FRONTIER\nEr_vec = matrix(seq( 0 . 01 , 0 . 15 , 0 . 01 ) , 15 , 1 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 140,
      "chunk_index": 2
    }
  },
  {
    "text": "being mean with variance: markowitz optimization 141\nSig_vec = matrix( 0 , 15 , 1 )\n0\nj =\nfor (Er in Er_vec) {\n1\nj = j+\nwts = markowitz(mu,cv,Er)\nSig_vec[ j ] = sqrt(t(wts) %*% cv %*% wts)\n}\nplot(Sig_vec ,Er_vec ,type=’l ’)\n51\nSee the frontier in Figure . .\n0.05 0.10 0.15 0.20 0.25\n41.0\n21.0\n01.0\n80.0\n60.0\n40.0\n20.0\nSig_vec\ncev_rE\nFigure5.1: TheEfficientFrontier\n5.4 Covariances of frontier portfolios: r ,r\np q\nCov(r\np\n,r\nq\n) = w\n(cid:48)p\nΣw\nq\n= [g+hE(r\np\n)]\n(cid:48)\nΣ[g+hE(r\nq\n)]\nNow,\n1 1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 141,
      "chunk_index": 0
    }
  },
  {
    "text": "q\n= [g+hE(r\np\n)]\n(cid:48)\nΣ[g+hE(r\nq\n)]\nNow,\n1 1\ng+hE(r\np\n) = [BΣ\n−\n11 AΣ\n−\n1µ]+ [CΣ\n−\n1µ AΣ\n−\n11][λ\n1\nB+λ\n2\nA]\nD − D −\n(cid:124) (cid:123)(cid:122) (cid:125)\nCE(r ) A B AE(r )\np p\n− + −\nD/B D/B",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 141,
      "chunk_index": 1
    }
  },
  {
    "text": "142 data science: theories, models, algorithms, and analytics\nAfter much simplification:\nCov(r\np\n,r\nq\n) = w\n(cid:48)p\nΣw\n(cid:48)q\nC 1\n= [E(r ) A/C][E(r ) A/C]+\np q\nD − − C\nC 1\nσ2 = Cov(r ,r ) = [E(r ) A/C]2+\np p p D p − C\nTherefore,\nσ2 [E(r ) A/C]2\np p\n− = 1\n1/C − D/C2\nwhich is the equation of a hyperbola in σ,E(r) space with center (0,A/C),\nor\n1\nσ2 = [CE(r )2 2AE(r )+B]\np D p − p\n, which is a parabola in E(r),σ space.\n5.5 Combinations",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 142,
      "chunk_index": 0
    }
  },
  {
    "text": "5.5 Combinations\nIt is easy to see that linear combinations of portfolios on the frontier will\nalso lie on the frontier.\nm m\n∑ ∑\nα w = α [g+hE(r )]\ni i i i\ni=1 i=1\nm m\n∑ ∑\n= g+h α E(r ) α = 1\ni i i\ni=1 i=1\nExercise\nCarry out the following analyses:\n1 . Use your R program to do the following. Set E(r ) = 0.10 (i.e. return\np\n10 3\nof %), and solve for the optimal portfolio weights for your securi-\nties. Call this vector of weights w . Next, set E(r ) = 0.20 and again\n1 p",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 142,
      "chunk_index": 1
    }
  },
  {
    "text": "1 p\nsolve for the portfolios weights w .\n2\n2 50 50\n. Take a / combination of these two portfolios. What are the weights?\nWhat is the expected return?\n3\n. For the expected return in the previous part, resolve the mean-variance\nproblem to get the new weights?\n4 3 2\n. Compare these weights in part to the ones in part above. Explain\nyour result.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 142,
      "chunk_index": 2
    }
  },
  {
    "text": "being mean with variance: markowitz optimization 143\n5.6 Zero Covariance Portfolio\nThis is a special portfolio of interest, and we will soon see why. Find\nE(r ), s.t. Cov(r ,r ) = 0\nq p q\nSuppose it exists, then the solution is:\nA D/C2\nE(r ) = E(r )\nq C − E(r ) A/C ≡ ZC(p)\np\n−\nSince ZC(p) exists for all p, all frontier portfolios can be formed from p\nand ZC(p).\nCov(r\np\n,r\nq\n) = w\n(cid:48)p\nΣw\nq\n= λ 1 µ (cid:48) Σ − 1Σw q +λ 2 1(cid:48) Σ − 1Σw q\n= λ 1 µ (cid:48) w q +λ 2 1(cid:48) w q",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 143,
      "chunk_index": 0
    }
  },
  {
    "text": "= λ 1 µ (cid:48) w q +λ 2 1(cid:48) w q\n= λ E(r )+λ\n1 q 2\nSubstitute in for λ ,λ and rearrange to get\n1 2\nE(r ) = (1 β )E[r ]+β E(r )\nq\n−\nqp ZC(p) qp p\nCov(r ,r )\nq p\nβ =\nqp\nσ2\np\nTherefore, the return on a portfolio can be written in terms of a basic\nportfolio p and its zero covariance portfolio ZC(p). This suggests a re-\ngression relationship, i.e.\nr = β +β r +β r +ξ\nq 0 1 ZC(p) 2 p\nwhich is nothing but a factor model, i.e. with orthogonal factors.\n5.7 Portfolio Problems with Riskless Assets",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 143,
      "chunk_index": 1
    }
  },
  {
    "text": "5.7 Portfolio Problems with Riskless Assets\nWe now enhance the portfolio problem to deal with risk less assets. The\ndifference is that the fully-invested constraint is expanded to include the\nrisk free asset. We require just a single equality constraint. The problem\nmay be specified as follows.\n1\nmin w (cid:48) Σw\nw 2\ns.t. w\n(cid:48)\nµ+(1 w (cid:48)1)r\nf\n= E(r\np\n)\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 143,
      "chunk_index": 2
    }
  },
  {
    "text": "144 data science: theories, models, algorithms, and analytics\n1\nmin L = w (cid:48) Σw+λ[E(r p ) w (cid:48) µ (1 w (cid:48)1)r f ]\nw 2 − − −\nThe first-order conditions for the problem are as follows.\n∂L\n= Σw λµ+λ1r = 0\nf\n∂w −\n∂L\n= E(r p ) w (cid:48) µ (1 w (cid:48)1)r f = 0\n∂λ − − −\nRe-aranging, and solving for w and λ, we get the following manipula-\ntions, eventually leading to the desired solution.\nΣw = λ(µ 1r )\nf\n−\nE(r\np\n) r\nf\n= w\n(cid:48)\n(µ 1r\nf\n)\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 144,
      "chunk_index": 0
    }
  },
  {
    "text": "f\n−\nE(r\np\n) r\nf\n= w\n(cid:48)\n(µ 1r\nf\n)\n− −\nTake the first equation and proceed as follows:\nw = λ Σ\n−\n1(µ 1r\nf\n)\n−\nE(r\np\n) r\nf\n(µ 1r\nf\n)\n(cid:48)\nw = λ(µ 1r\nf\n)\n(cid:48)\nΣ\n−\n1(µ 1r\nf\n)\n− ≡ − − −\nThe first and third terms in the equation above then give that\nE(r ) r\np f\nλ = −\n(µ 1r ) Σ 1(µ 1r )\nf (cid:48) − f\n− −\nSubstituting this back into the first foc results in the final solution.\nE(r ) r\nw = Σ\n−\n1(µ 1r\nf\n) p − f\n− H\nwhere H = (µ r f 1) (cid:48) Σ − 1(µ r f 1)\n− −\nExercise",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 144,
      "chunk_index": 1
    }
  },
  {
    "text": "− −\nExercise\nHow will you use the R program to find the minimum variance portfolio\n(MVP)? What are the portfolio weights? What is the expected return?\nExercise\nDevelop program code for the mean-variance problem with the risk-free\nasset.\nExercise\nDevelop program code for the mean-variance problem with no short\nsales, and plot the efficient frontier on top of the one with short-selling\nallowed.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 144,
      "chunk_index": 2
    }
  },
  {
    "text": "being mean with variance: markowitz optimization 145\n5.8 Risk Budgeting\nMarkowitz optimization has morphed into many different “views” of the\nsame problem. One of the recent approaches to portfolio construction is\nto create portfolios where the risk contributions of all assets are equal.\nThis is known as the “risk parity” approach. We may also construct a\nportfolio where all assets contribute the same proportion of the total\nreturn of the portfolio, and this is known as the “performance parity”",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 145,
      "chunk_index": 0
    }
  },
  {
    "text": "approach.\nIf the portfolio is denoted by its weights w then the risk of the port-\nfolio is a function of the weights and is denoted R(w). As we have seen\nthe standard deviation of the portfolio return is written as\nR(w) = σ(w) = √ w Σw ( 5 . 1 )\n(cid:62)\nThis risk function is linear homogenous, i.e., if we double the size of\nthe portfolio then the risk measure also doubles. This is also known\nas the “homogeneity” property of risk measures and is one of the four",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 145,
      "chunk_index": 1
    }
  },
  {
    "text": "desirable properties of a “coherent” risk measure defined by Artzner,\n1999\nDelbaen, Eber, and Heath ( ):\n1 . Homogeneity: R(m w) = m R(w).\n· ·\n2 . Subadditivity (diversification): R(w +w ) R(w )+R(w ).\n1 2 1 2\n≤\n3 . Monotonicity: if portfolio w dominates portfolio w , and their mean\n1 2\nreturns are the same, then R(w ) R(w ).\n1 2\n≤\n4 . Translation invariance: if we add cash proportion c and rebalance the\nportfolio, then R(w+c) = R(w) c.\n−\n5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 145,
      "chunk_index": 2
    }
  },
  {
    "text": "portfolio, then R(w+c) = R(w) c.\n−\n5\n. Convexity: this property combines homogeneity and subadditivity,\nR(m w +(1 m) w ) m R(w )+(1 m) R(w ).\n1 2 1 2\n· − · ≤ · − ·\nIf the risk measure satisfies the homogeneity property, then Euler’s\ntheorem may be applied to decompose risk into the amount provided by\neach asset.\nR(w) = ∑ n w ∂R(w) ( 5 . 2 )\nj\n∂w\nj=1 j\n∂R(w)\nThe component w is known as the risk share of asset j, and when\nj ∂w\nj\ndivided by R(w), it is the risk proportion of asset j.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 145,
      "chunk_index": 3
    }
  },
  {
    "text": "146 data science: theories, models, algorithms, and analytics\nSuppose we define the risk measure to be the standard deviation of re-\nturns of the portfolio, then the risk decomposition requires the derivative\nof the risk measure with respect to all the weights, i.e.,\n∂R(w) ∂√w Σw 1 Σw\n= (cid:62) = [w (cid:62) Σw] − 1/2 2 Σw = ( 5 . 3 )\n∂w ∂w 2 · σ(w)\nwhich is a n-dimensional vector. If we multiply the j-th element of this\nvector by w , we get the risk contribution for asset j.\nj",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 146,
      "chunk_index": 0
    }
  },
  {
    "text": "j\nWe may check that the risk contributions sum up to the total risk:\n∑ n w ∂R(w) = [w w ... w ] [Σw/σ(w)]\nj 1 2 n\n∂w ·\nj=1 j\n= w\n(cid:62)\n[Σw/σ(w)]\n·\nσ(w)2\n=\nσ(w)\n= σ(w)\n= R(w)\nLet’s look at an example to clarify the computations. First, read in the\ncovariance matrix and mean return vector.\nmu = matrix(c( 0 . 05 , 0 . 10 , 0 . 20 ) , 3 , 1 )\nn = length(mu)\ncv = matrix(c( 0 . 03 , 0 . 01 , 0 . 01 , 0 . 01 , 0 . 04 , 0 . 02 , 0 . 01 , 0 . 02 , 0 . 16 ) ,n,n)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 146,
      "chunk_index": 1
    }
  },
  {
    "text": "We begin by choosing the portfolio weights for an expected return of\n0.12. Then we create the function to return the risk contributions of each\nasset in the portfolio.\n#RISK CONTRIBUTIONS\nriskContribution = function(cv,wts) {\nsig = sqrt(t(wts) %*% cv %*% wts)\nrc = as.matrix(cv %*% wts) / sig [ 1 ] * wts\n}\n#Example\n0 12\nEr = .\nwts = markowitz(mu,cv,Er)\nprint(wts)\nRC = riskContribution(cv,wts)\nprint(RC)\n#Check",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 146,
      "chunk_index": 2
    }
  },
  {
    "text": "being mean with variance: markowitz optimization 147\nsig = sqrt(t(wts) %*% cv %*% wts)\nprint(c(sig , sum(RC)))\nThe output of all this code is as follows:\n> print(wts)\n1\n[ , ]\n1 0 1818182\n[ ,] .\n2 0 5272727\n[ ,] .\n3 0 2909091\n[ ,] .\n> print(RC)\n1\n[ , ]\n1 0 01329760\n[ ,] .\n2 0 08123947\n[ ,] .\n3 0 09191302\n[ ,] .\n> #Check\n> sig = sqrt(t(wts) %*% cv %*% wts)\n> print(c(sig , sum(RC)))\n1 0 1864501 0 1864501\n[ ] . .\nWe see that the total risk contributions of all three assets does indeed",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 147,
      "chunk_index": 0
    }
  },
  {
    "text": "sum up to the standard deviation of the portfolio, i.e., 0.1864501.\nWe are interested in solving the reverse problem. Given a target set of\nrisk contributions, what weights of the portfolio will deliver the required\nconribution. For example, what if we wanted the portfolio total standard\ndeviation to be 0.15, with the shares from each asset in the amounts\n0.05,0.05,0.05 , respectively?\n{ }\nWe note that it is not possible to solve for exactly the desired risk con-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 147,
      "chunk_index": 1
    }
  },
  {
    "text": "tributions. This is because it would involve one constraint for each risk\ncontribution, plus one additional constraint that the sum of the portfolio\n1\nweights sum up to . That would leave us with an infeasible problem\nwhere there are four constraints and only three free parameters. There-\nfore, we minimise the sum of squared differences between the risk con-\ntributions and targets, while ensuring that the sum of portfolio weights",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 147,
      "chunk_index": 2
    }
  },
  {
    "text": "equals unity. We can implement the following code to achieve this result.\n#SOLVE FOR CHOSEN RISK CONTRIBUTIONS\nsolveRC = function(wts, target ,cv) {\nwts[length(wts)+ 1 ] = 1 sum(wts)\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 147,
      "chunk_index": 3
    }
  },
  {
    "text": "148 data science: theories, models, algorithms, and analytics\nwts = as.matrix(wts)\nrc = riskContribution(cv,wts)\n#Minimize the max slippage from risk parity\n2 10000000\ndiff = *(rc target )\n−\n}\ntarget = matrix(c( 0 . 05 , 0 . 05 , 0 . 05 ))\nw_guess = c( 0 . 1 , 0 . 4 )\nlibrary(minpack.lm)\nsol = nls .lm(w_guess ,fn=solveRC ,cv=cv, target=target )\nwts = sol$par\nwts[length(wts)+ 1 ] = 1 sum(wts)\n−\nwts = as.matrix(wts)\nprint(wts)\nprint(sum(wts))\nrc = riskContribution(cv,wts)\nprint(c(rc ,sum(rc )))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 148,
      "chunk_index": 0
    }
  },
  {
    "text": "print(c(rc ,sum(rc )))\nThe results from running this code are as follows:\n> print(wts)\n1\n[ , ]\n1 0 4435305\n[ ,] .\n2 0 3639453\n[ ,] .\n3 0 1925243\n[ ,] .\n> print(sum(wts))\n1 1\n[ ]\n> rc = riskContribution(cv,wts)\n> print(c(rc ,sum(rc )))\n1 0 05307351 0 05271923 0 05190721 0 15769995\n[ ] . . . .\n>\nWe see that the results are close to targets, but slightly above. As ex-\npected, since the risk parity is equal across assets, the less risky ones\nhave a greater share in the portfolio allocation.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 148,
      "chunk_index": 1
    }
  },
  {
    "text": "6\nLearning from Experience: Bayes Theorem\n6.1 Introduction\nFor a fairly good introduction to Bayes Rule, see Wikipedia\nhttp://en.wikipedia.org/wiki/Bayes theorem\nThe various R packages for Bayesian inference are at:\nhttp://cran.r-project.org/web/views/Bayesian.html\nAlso see the great video of Professor Persi Diaconis’s talk on Bayes on\nYahoo video where he talks about coincidences. In business, we often\nwant to ask, is a given phenomena real or just a coincidence? Bayes theo-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 149,
      "chunk_index": 0
    }
  },
  {
    "text": "rem really helps with that. For example, we may ask – is Warren Buffet’s\ninvestment success a coincidence? How would you answer this question?\nWould it depend on your prior probability of Buffet being able to beat\nthe market? How would this answer change as additional information\nabout his performance was being released over time?\nBayes rule follows easily from a decomposition of joint probability, i.e.,\nPr[A B] = Pr(A B) Pr(B) = Pr(B A) Pr(A)\n∩ | |",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 149,
      "chunk_index": 1
    }
  },
  {
    "text": "Pr[A B] = Pr(A B) Pr(B) = Pr(B A) Pr(A)\n∩ | |\nThen the last two terms may be arranged to give\nPr(B A) Pr(A)\nPr(A B) = |\n| Pr(B)\nor\nPr(A B) Pr(B)\nPr(B A) = |\n| Pr(A)\nExample\nThe AIDS test. This is an interesting problem, because it shows that if\nyou are diagnosed with AIDS, there is a good chance the diagnosis is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 149,
      "chunk_index": 2
    }
  },
  {
    "text": "150 data science: theories, models, algorithms, and analytics\nwrong, but if you are diagnosed as not having AIDS then there is a good\nchance it is right - hopefully this is comforting news.\nDefine, Pos,Neg as a positive or negative diagnosis of having AIDS.\n{ }\nAlso define Dis,NoDis as the event of having the disease versus not\n{ }\n15 300\nhaving it. There are . million AIDS cases in the U.S. and about\nmillion people which means the probability of AIDS in the population is\n0005",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 150,
      "chunk_index": 0
    }
  },
  {
    "text": "0005\n. (half a percent). Hence, a random test will uncover someone with\nAIDS with a half a percent probability. The confirmation accuracy of the\n99\nAIDS test is %, such that we have\nPr(Pos Dis) = 0.99\n|\nHence the test is reasonably good. The accuracy of the test for people\nwho do not have AIDS is\nPr(Neg NoDis) = 0.95\n|\nWhat we really want is the probability of having the disease when the\ntest comes up positive, i.e. we need to compute Pr(Dis Pos). Using\n|\nBayes Rule we calculate:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 150,
      "chunk_index": 1
    }
  },
  {
    "text": "|\nBayes Rule we calculate:\nPr(Pos Dis)Pr(Dis)\nPr(Dis Pos) = |\n| Pr(Pos)\nPr(Pos Dis)Pr(Dis)\n= |\nPr(Pos Dis)Pr(Dis)+Pr(Pos NoDis)Pr(NoDis)\n| |\n0.99 0.005\n= ×\n(0.99)(0.005)+(0.05)(0.995)\n= 0.0904936\n9\nHence, the chance of having AIDS when the test is positive is only %.\nWe might also care about the chance of not having AIDS when the test is\npositive\nPr(NoDis Pos) = 1 Pr(Dis Pos) = 1 0.09 = 0.91\n| − | −\nFinally, what is the chance that we have AIDS even when the test is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 150,
      "chunk_index": 2
    }
  },
  {
    "text": "negative - this would also be a matter of concern to many of us, who\nmight not relish the chance to be on some heavy drugs for the rest of our",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 150,
      "chunk_index": 3
    }
  },
  {
    "text": "learning from experience: bayes theorem 151\nlives.\nPr(Neg Dis)Pr(Dis)\nPr(Dis Neg) = |\n| Pr(Neg)\nPr(Neg Dis)Pr(Dis)\n= |\nPr(Neg Dis)Pr(Dis)+Pr(Neg NoDis)Pr(NoDis)\n| |\n0.01 0.005\n= ×\n(0.01)(0.005)+(0.95)(0.995)\n= 0.000053\nHence, this is a worry we should not have. If the test is negative, there is\na miniscule chance that we are infected with AIDS.\n6.2 Bayes and Joint Probability Distributions\nThe preceding analysis is a good lead in to (a) the connection with joint",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 151,
      "chunk_index": 0
    }
  },
  {
    "text": "probability distributions, and (b) using R to demonstrate a computa-\ntional way of thinking about Bayes theorem.\n300000\nLet’s begin by assuming that we have , people in the popula-\ntion, to scale down the numbers from the millions for convenience. Of\n1500\nthese , have AIDS. So let’s create the population and then sample\nfrom it. See the use of the sample function in R.\n> people = seq( 1 , 300000 )\n> people_aids = sample(people , 1500 )\n> people_noaids = setdiff (people ,people_aids)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 151,
      "chunk_index": 1
    }
  },
  {
    "text": "> people_noaids = setdiff (people ,people_aids)\nNote, how we also used the setdiff function to get the complement\nset of the people who do not have AIDS. Now, of the people who have\n99\nAIDS, we know that % of them test positive so let’s subset that list,\nand also take its complement. These are joint events, and their numbers\nproscribe the joint distribution.\n> people_aids_pos = sample(people_aids , 1500 * 0 . 99 )\n> people_aids_neg = setdiff (people_aids ,people_aids_pos)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 151,
      "chunk_index": 2
    }
  },
  {
    "text": "> length(people_aids_pos)\n1 1485\n[ ]\n> length(people_aids_neg)\n1 15\n[ ]\nWe can also subset the group that does not have AIDS, as we know that\n95\nthe test is negative for them % of the time.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 151,
      "chunk_index": 3
    }
  },
  {
    "text": "152 data science: theories, models, algorithms, and analytics\n> people_noaids_neg = sample(people_noaids , 298500 * 0 . 95 )\n> people_noaids_pos = setdiff (people_noaids ,people_noaids_neg)\n> length(people_noaids_neg)\n1 283575\n[ ]\n> length(people_noaids_pos)\n1 14925\n[ ]\nWe can now compute the probability that someone actually has AIDS\nwhen the test comes out positive.\n> pr_aids_given_pos = (length(people_aids_pos)) /\n(length(people_aids_pos)+length(people_noaids_pos))\n> pr_aids_given_pos",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 152,
      "chunk_index": 0
    }
  },
  {
    "text": "> pr_aids_given_pos\n1 0 0904936\n[ ] .\nThis confirms the formal Bayes computation that we had undertaken\nearlier. And of course, as we had examined earlier, what’s the chance\nthat you have AIDS when the test is negative, i.e., a false negative?\n> pr_aids_given_neg = (length(people_aids_neg)) /\n(length(people_aids_neg)+length(people_noaids_neg))\n> pr_aids_given_neg\n1 5 289326 05\n[ ] . e\n−\nPhew!\nNote here that we first computed the joint sets covering joint out-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 152,
      "chunk_index": 1
    }
  },
  {
    "text": "comes, and then used these to compute conditional (Bayes) probabilities.\nThe approach used R to apply a set-theoretic, computational approach to\ncalculating conditional probabilities.\n6.3 Correlated default (conditional default)\nBayes theorem is very useful when we want to extract conditional de-\nfault information. Bond fund managers are not as interested in the cor-\nrelation of default of the bonds in their portfolio as much as the con-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 152,
      "chunk_index": 2
    }
  },
  {
    "text": "ditional default of bonds. What this means is that they care about the\nconditional probability of bond A defaulting if bond B has defaulted al-\nready.\nModern finance provides many tools to obtain the default proba-\n1\nbilities of firms. Suppose we know that firm has default probability\np = 1% and firm 2 has default probability p = 3%. If the correlation\n1 2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 152,
      "chunk_index": 3
    }
  },
  {
    "text": "learning from experience: bayes theorem 153\n40\nof default of the two firms is % over one year, then if either bond de-\nfaults, what is the probability of default of the other, conditional on the\nfirst default?\nWe can see that even with this limited information, Bayes theorem\nallows us to derive the conditional probabilities of interest. First define\nd ,i = 1,2 as default indicators for firms 1 and 2 . d = 1 if the firm\ni i\ndefaults, and zero otherwise. We note that:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 153,
      "chunk_index": 0
    }
  },
  {
    "text": "i i\ndefaults, and zero otherwise. We note that:\nE(d ) = 1.p +0.(1 p ) = p = 0.01.\n1 1 1 1\n−\nLikewise\nE(d ) = 1.p +0.(1 p ) = p = 0.03.\n2 2 2 2\n−\nThe Bernoulli distribution lets us derive the standard deviation of d and\n1\nd .\n2\n(cid:113) (cid:113)\nσ = p (1 p ) = (0.01)(0.99) = 0.099499\n1 1 1\n−\n(cid:113) (cid:113)\nσ = p (1 p ) = (0.03)(0.97) = 0.17059\n2 2 2\n−\nNow, we note that\nCov(d ,d ) = E(d .d ) E(d )E(d )\n1 2 1 2 1 2\n−\nρσ σ = E(d .d ) p p\n1 2 1 2 1 2\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 153,
      "chunk_index": 1
    }
  },
  {
    "text": "1 2 1 2 1 2\n−\nρσ σ = E(d .d ) p p\n1 2 1 2 1 2\n−\n(0.4)(0.099499)(0.17059) = E(d .d ) (0.01)(0.03)\n1 2\n−\nE(d .d ) = 0.0070894\n1 2\nE(d .d ) p\n1 2 12\n≡\nwhere p is the probability of default of both firm 1 and 2 . We now get\n12\nthe conditional probabilities:\np(d d ) = p /p = 0.0070894/0.03 = 0.23631\n1 2 12 2\n|\np(d d ) = p /p = 0.0070894/0.01 = 0.70894\n2 1 12 1\n|\nThese conditional probabilities are non-trivial in size, even though the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 153,
      "chunk_index": 2
    }
  },
  {
    "text": "individual probabilities of default are very small. What this means is\nthat default contagion can be quite severe once firms begin to default.\n(This example used our knowledge of Bayes’ rule, correlations, covari-\nances, and joint events.)\n6.4 Continuous and More Formal Exposition\nIn Bayesian approaches, the terms “prior”, “posterior”, and “likelihood”\nare commonly used and we explore this terminology here. We are usu-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 153,
      "chunk_index": 3
    }
  },
  {
    "text": "154 data science: theories, models, algorithms, and analytics\nally interested in the parameter θ, the mean of the distribution of some\ndata x (I am using the standard notation here). But in the Bayesian set-\nting we do not just want the value of θ, but we want a distribution of\nvalues of θ starting from some prior assumption about this distribution.\nSo we start with p(θ), which we call the prior distribution. We then ob-\nserve data x, and combine the data with the prior to get the posterior",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 154,
      "chunk_index": 0
    }
  },
  {
    "text": "distribution p(θ x). To do this, we need to compute the probability of\n|\nseeing the data x given our prior p(θ) and this probability is given by\nthe likelihood function L(x θ). Assume that the variance of the data x is\n|\nknown, i.e., is σ2.\nApplying Bayes’ theorem we have\nL(x θ) p(θ)\np(θ x) = (cid:82) | ∝ L(x θ) p(θ)\n| L(x θ) p(θ) dθ |\n|\nIf we assume the prior distribution for the mean of the data is normal,\ni.e., p(θ) N[µ ,σ2], and the likelihood is also normal, i.e., L(x θ)\n∼ 0 0 | ∼",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 154,
      "chunk_index": 1
    }
  },
  {
    "text": "∼ 0 0 | ∼\nN[θ,σ2], then we have that\n(cid:34) (cid:35) (cid:34) (cid:35)\n1 1(θ µ )2 1(θ µ )2\np(θ) = exp − 0 N[θ µ ,σ2] ∝ exp − 0\n(cid:113) 2πσ2 −2 σ\n0\n2 ∼ | 0 0 −2 σ\n0\n2\n0\n1\n(cid:20)\n1(x\nθ)2(cid:21) (cid:20)\n1(x\nθ)2(cid:21)\nL(x θ) = exp − N[x θ,σ2] ∝ exp −\n| √2πσ2 −2 σ2 ∼ | −2 σ2\nGiven this, the posterior is as follows:\n(cid:34) (cid:35)\n1(x θ)2 1(θ µ )2\np(θ x) ∝ L(x θ)p(θ) ∝ exp − − 0\n| | −2 σ2 − 2 σ2\n0\nDefine the precision values to be τ = 1 , and τ = 1 . Then it can be\n0 σ2 σ2\n0",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 154,
      "chunk_index": 2
    }
  },
  {
    "text": "0 σ2 σ2\n0\nshown that when you observe a new value of the data x, the posterior\ndistribution is written down in closed form as:\n(cid:20) (cid:21)\nτ τ 1\np(θ x) N 0 µ + x,\n0\n| ∼ τ +τ τ +τ τ +τ\n0 0 0\nWhen the posterior distribution and prior distribution have the same\nform, they are said to be “conjugate” with respect to the specific likeli-\nhood function.\nTo take an example, suppose our prior for the mean of the equity\npremium per month is p(θ) N[0.005,0.0012]. The standard devia-\n∼\n004",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 154,
      "chunk_index": 3
    }
  },
  {
    "text": "∼\n004\ntion of the equity premium is . . If next month we observe an equity\n1\npremium of %, what is the posterior distribution of the mean equity\npremium?",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 154,
      "chunk_index": 4
    }
  },
  {
    "text": "learning from experience: bayes theorem 155\n0 0 005\n> mu = .\n0 0 001\n> sigma = .\n0 04\n> sigma= .\n0 01\n> x = .\n> tau\n0\n=\n1/sigma 0\n^\n2\n> tau =\n1/sigma^ 2\n> posterior_mean = tau 0 *mu 0/ (tau 0 +tau) + tau*x/ (tau 0 +tau)\n> posterior_mean\n1 0 005003123\n[ ] .\n> posterior_var = 1/ (tau 0 +tau)\n> sqrt(posterior_var)\n1 0 0009996876\n[ ] .\nHence, we see that after updating the mean has increased mildly because\nthe data came in higher than expected.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 155,
      "chunk_index": 0
    }
  },
  {
    "text": "the data came in higher than expected.\nIf we observe n new values of x, then the new posterior is\n(cid:34) (cid:35)\np(θ x) N τ 0 µ + τ ∑ n x , 1\n| ∼ τ +nτ 0 τ +nτ j τ +nτ\n0 0 j=1 0\nThis is easy to derive, as it is just the result you obtain if you took each\nx and updated the posterior one at a time.\nj\nExercise\nEstimate the equity risk premium. We will use data and discrete Bayes to\ncome up with a forecast of the equity risk premium. Proceed along the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 155,
      "chunk_index": 1
    }
  },
  {
    "text": "following lines using the LearnBayes package.\n1 1926\n. We’ll use data from onwards from the Fama-French data repos-\nitory. All you need is the equity premium (r r ) data, and I will\nm f\n−\nleave it up to you to choose if you want to use annual or monthly\ndata. Download this and load it into R.\n2 2000\n. Using the series only up to the year , present the descriptive\nstatistics for the equity premium. State these in annualized terms.\n3\n. Present the distribution of returns as a histogram.\n4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 155,
      "chunk_index": 2
    }
  },
  {
    "text": "4\n. Store the results of the histogram, i.e., the range of discrete values\nof the equity premium, and the probability of each one. Treat this as\nyour prior distribution.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 155,
      "chunk_index": 3
    }
  },
  {
    "text": "156 data science: theories, models, algorithms, and analytics\n5 2000\n. Now take the remaining data for the years after , and use this\ndata to update the prior and construct a posterior. Assume that the\nprior, likelihood, and posterior are normally distributed. Use the\ndiscrete.bayes function to construct the posterior distribution and\nplot it using a histogram. See if you can put the prior and posterior on\nthe same plot to see how the new data has changed the prior.\n6",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 156,
      "chunk_index": 0
    }
  },
  {
    "text": "6\n. What is the forecasted equity premium, and what is the confidence\ninterval around your forecast?\n6.5 Bayes Nets\nHigher-dimension Bayes problems and joint distributions over several\noutcomes/events are easy to visualize with a network diagram, also\ncalled a Bayes net. A Bayes net is a directed, acyclic graph (known as a\nDAG), i.e., cycles are not permitted in the graph.\nA good way to understand a Bayes net is with an example of eco-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 156,
      "chunk_index": 1
    }
  },
  {
    "text": "nomic distress. There are three levels at which distress may be noticed:\neconomy level (E = 1), industry level (I = 1), or at a particular firm level\n(F = 1). Economic distress can lead to industry distress and/or firm\ndistress, and industry distress may or may not result in a firm’s distress.\n61\nThe network diagram portrays the flow of causality, see Figure . .\nThe probabilities are as follows. Note that the probabilities in the first",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 156,
      "chunk_index": 2
    }
  },
  {
    "text": "tableau are unconditional, but in all the subsequent tableaus they are\nconditional probabilities.\nE Prob\n1 010\n.\n0 090\n.\nE I Conditional Prob Channel\n1 1 060\n. a\n1 0 040\n.\n0 1 020\n. –\n0 0 080\n.\n1\nNote here that each pair of conditional probabilities adds up to . The\n“channels” in the tableaus refer to the arrows in the Bayes net diagram.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 156,
      "chunk_index": 3
    }
  },
  {
    "text": "learning from experience: bayes theorem 157\nFigure6.1: Bayesnetshowingthe\nE = 1\npathwaysofeconomicdistress.\nTherearethreechannels: aisthe\ninducementofindustrydistress\nfromeconomydistress;bisthe\ninducementoffirmdistressdirectly\na\nfromeconomydistress;cisthe\ninducementoffirmdistressdirectly\nfromindustrydistress.\nI = 1\nb\nc\nF = 1\nE I F Conditional Prob Channel\n1 1 1 095\n. a+c\n1 1 0 005\n.\n1 0 1 070\n. b\n1 0 0 030\n.\n0 1 1 080\n. c\n0 1 0 020\n.\n0 0 1 010\n. –\n0 0 0 090\n.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 157,
      "chunk_index": 0
    }
  },
  {
    "text": ". c\n0 1 0 020\n.\n0 0 1 010\n. –\n0 0 0 090\n.\nNow we will compute an answer to the question: What is the prob-\nability that the industry is distressed if the firm is known to be in dis-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 157,
      "chunk_index": 1
    }
  },
  {
    "text": "158 data science: theories, models, algorithms, and analytics\ntress? The calculation is as follows:\nPr(F = 1 I = 1) Pr(I = 1)\nPr(I = 1 F = 1) = | ·\n| Pr(F = 1)\nPr(F = 1 I = 1) Pr(I = 1) = Pr(F = 1 I = 1) Pr(I = 1 E = 1) Pr(E = 1)\n| · | · | ·\n+Pr(F = 1 I = 1) Pr(I = 1 E = 0) Pr(E = 0)\n| · | ·\n= 0.95 0.6 0.1+0.8 0.2 0.9 = 0.201\n× × × ×\nPr(F = 1 I = 0) Pr(I = 0) = Pr(F = 1 I = 0) Pr(I = 0 E = 1) Pr(E = 1)\n| · | · | ·\n+Pr(F = 1 I = 0) Pr(I = 0 E = 0) Pr(E = 0)\n| · | ·",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 158,
      "chunk_index": 0
    }
  },
  {
    "text": "| · | ·\n= 0.7 0.4 0.1+0.1 0.8 0.9 = 0.100\n× × × ×\nPr(F = 1) = Pr(F = 1 I = 1) Pr(I = 1)\n| ·\n+Pr(F = 1 I = 0) Pr(I = 0) = 0.301\n| ·\nPr(F = 1 I = 1) Pr(I = 1) 0.201\nPr(I = 1 F = 1) = | · = = 0.6677741\n| Pr(F = 1) 0.301\nA computational set-theoretic approach: We may write a R script to compute\nthe conditional probability that the industry is distressed when a firm is\ndistressed.\n#bayesnet .R\n#BAYES NET COMPUTATIONS\nE = seq( 1 , 100000 )\nn = length(E)\nE 1 = sample(E,length(E)* 0 . 1 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 158,
      "chunk_index": 1
    }
  },
  {
    "text": "n = length(E)\nE 1 = sample(E,length(E)* 0 . 1 )\nE 0 = setdiff (E,E 1 )\nE 1 I 1 = sample(E 1 ,length(E 1 )* 0 . 6 )\nE 1 I 0 = setdiff (E 1 ,E 1 I 1 )\nE 0 I 1 = sample(E 0 ,length(E 0 )* 0 . 2 )\nE 0 I 0 = setdiff (E 0 ,E 0 I 1 )\nE 1 I 1 F 1 = sample(E 1 I 1 ,length(E 1 I 1 )* 0 . 95 )\nE 1 I 1 F 0 = setdiff (E 1 I 1 ,E 1 I 1 F 1 )\nE 1 I 0 F 1 = sample(E 1 I 0 ,length(E 1 I 0 )* 0 . 70 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 158,
      "chunk_index": 2
    }
  },
  {
    "text": "learning from experience: bayes theorem 159\nE 1 I 0 F 0 = setdiff (E 1 I 0 ,E 1 I 0 F 1 )\nE 0 I 1 F 1 = sample(E 0 I 1 ,length(E 0 I 1 )* 0 . 80 )\nE 0 I 1 F 0 = setdiff (E 0 I 1 ,E 0 I 1 F 1 )\nE 0 I 0 F 1 = sample(E 0 I 0 ,length(E 0 I 0 )* 0 . 10 )\nE 0 I 0 F 0 = setdiff (E 0 I 0 ,E 0 I 0 F 1 )\npr_I 1_given_F 1 = length(c(E 1 I 1 F 1 ,E 0 I 1 F 1 )) /\nlength(c(E 1 I 1 F 1 ,E 1 I 0 F 1 ,E 0 I 1 F 1 ,E 0 I 0 F 1 ))\nprint(pr_I 1_given_F 1 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 159,
      "chunk_index": 0
    }
  },
  {
    "text": "print(pr_I 1_given_F 1 )\nRunning this program gives the desired probability and confirms the\nprevious result.\n> source(\"bayesnet .R\")\n1 0 6677741\n[ ] .\nExercise\nCompute the conditional probability that the economy is in distress if\nthe firm is in distress. Compare this to the previous conditional probabil-\n06677741\nity we computed of . . Should it be lower?\nHere is the answer:\n> pr_E 1_given_F 1 = length(c(E 1 I 1 F 1 ,E 1 I 0 F 1 )) /",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 159,
      "chunk_index": 1
    }
  },
  {
    "text": "length(c(E 1 I 1 F 1 ,E 1 I 0 F 1 ,E 0 I 1 F 1 ,E 0 I 0 F 1 ))\n> print(pr_E 1_given_F 1 )\n1 0 282392\n[ ] .\nYes, it should be lower than the probability that the industry is in dis-\ntress when the firm is in distress, because the economy is one network\nlayer removed from the firm, unlike the industry.\nExercise\nWhat packages does R provide for doing Bayes Nets?\n6.6 Bayes Rule in Marketing\nIn pilot market tests (part of a larger market research campaign), Bayes",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 159,
      "chunk_index": 2
    }
  },
  {
    "text": "theorem shows up in a simple manner. Suppose we have a project whose\nvalue is x. If the product is successful (S), the payoff is +100 and if the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 159,
      "chunk_index": 3
    }
  },
  {
    "text": "160 data science: theories, models, algorithms, and analytics\nproduct fails (F) the payoff is 70. The probability of these two events is:\n−\nPr(S) = 0.7, Pr(F) = 0.3\nYou can easily check that the expected value is E(x) = 49. Suppose\nwe were able to buy protection for a failed product, then this protection\nwould be a put option (of the real option type), and would be worth\n0.3 70 = 21. Since the put saves the loss on failure, the value is simply\n×",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 160,
      "chunk_index": 0
    }
  },
  {
    "text": "×\nthe expected loss amount, conditional on loss. Market researchers think\nof this as the value of “perfect information.”\nWould you proceed with this product launch given these odds? Yes,\nthe expected value is positive (note that we are assuming away risk aver-\nsion issues here - but this is not a finance topic, but a marketing research\nanalysis).\nNow suppose there is an intermediate choice, i.e. you can undertake a\npilot test (denoted T). Pilot tests are not highly accurate though they are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 160,
      "chunk_index": 1
    }
  },
  {
    "text": "reasonably sophisticated. The pilot test signals success (T+) or failure\n(T ) with the following probabilities:\n−\nPr(T+ S) = 0.8\n|\nPr(T S) = 0.2\n−|\nPr(T+ F) = 0.3\n|\nPr(T F) = 0.7\n−|\nWhat are these? We note that Pr(T+ S) stands for the probability that\n|\nthe pilot signals success when indeed the underlying product launch\nwill be successful. Thus the pilot in this case gives only an accurate read-\n80\ning of success % of the time. Analogously, one can interpret the other\nprobabilities.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 160,
      "chunk_index": 2
    }
  },
  {
    "text": "probabilities.\nWe may compute the probability that the pilot gives a positive result:\nPr(T+) = Pr(T+ S)Pr(S)+Pr(T+ F)Pr(F)\n| |\n= (0.8)(0.7)+(0.3)(0.3) = 0.65\nand that the result is negative:\nPr(T ) = Pr(T S)Pr(S)+Pr(T F)Pr(F)\n− −| −|\n= (0.2)(0.7)+(0.7)(0.3) = 0.35",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 160,
      "chunk_index": 3
    }
  },
  {
    "text": "learning from experience: bayes theorem 161\nwhich now allows us to compute the following conditional probabilities:\nPr(T+ S)Pr(S) (0.8)(0.7)\nPr(S T+) = | = = 0.86154\n| Pr(T+) 0.65\nPr(T S)Pr(S) (0.2)(0.7)\nPr(S T ) = −| = = 0.4\n| − Pr(T ) 0.35\n−\nPr(T+ F)Pr(F) (0.3)(0.3)\nPr(F T+) = | = = 0.13846\n| Pr(T+) 0.65\nPr(T F)Pr(F) (0.7)(0.3)\nPr(F T ) = −| = = 0.6\n| − Pr(T ) 0.35\n−\nArmed with these conditional probabilities, we may now re-evaluate",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 161,
      "chunk_index": 0
    }
  },
  {
    "text": "our product launch. If the pilot comes out positive, what is the expected\nvalue of the product launch? This is as follows:\nE(x T+) = 100Pr(S T+)+( 70)Pr(F T+)\n| | − |\n= 100(0.86154) 70(0.13846)\n−\n= 76.462\nAnd if the pilot comes out negative, then the value of the launch is:\nE(x T ) = 100Pr(S T )+( 70)Pr(F T )\n| − | − − | −\n= 100(0.4) 70(0.6)\n−\n= 2\n−\nSo. we see that if the pilot is negative, then we know that the expected\nvalue from the main product launch is negative, and we do not proceed.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 161,
      "chunk_index": 1
    }
  },
  {
    "text": "Thus, the overall expected value after the pilot is\nE(x) = E(x T+)Pr(T+)+E(x T )Pr(T )\n| | − −\n= 76.462(0.65)+(0)(0.35)\n= 49.70\nThe incremental value over the case without the pilot test is 0.70. This is\nthe information value of the pilot test.\nThere are other applications of Bayes in marketing:\n• See the paper “HB Revolution” by Greg Allenby, David Bakken, and\nPeter Rossi in Marketing Research, Summer 2004 .\n• See also the paper by David Bakken, titled “The Bayesian Revolution",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 161,
      "chunk_index": 2
    }
  },
  {
    "text": "in Marketing Research”.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 161,
      "chunk_index": 3
    }
  },
  {
    "text": "162 data science: theories, models, algorithms, and analytics\n6.7 Other Applications\n6.7.1 Bayes Models in Credit Rating Transitions\nSee the paper by Sanjiv Das, Rong Fang, and Gary Geng - “Bayesian\nMigration in Credit Ratings Based on Probabilities of Default,” Journal of\nFixed Income Dec 2002 , 1 - 7 .\nCompanies may be allocated to credit rating classes, which are coarser\nbuckets of credit quality in comparison to finer measures such as default",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 162,
      "chunk_index": 0
    }
  },
  {
    "text": "probabilities. Also, rating agencies tend to be slow in updating their\ncredit ratings. The DFG model uses contemporaneous data on default\nprobabilities to develop a model of rating changes using a Bayesian ap-\nproach.\n6.7.2 Accounting Fraud\nBayesian inference is also possible in accounting fraud situations, and\naudits. Clearly, when an auditor suspects fraud, he can invoke a Bayesian\nhypothesis of fraud, with a subjective prior probability, and then bring",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 162,
      "chunk_index": 1
    }
  },
  {
    "text": "to bear past data on this to assess the chance that the current situation is\nalso indicative of possible fraud.\n6.7.3 Bayes was a Reverend after all...\nHere is an interesting viewpoint from the Scientific American (see\n62\nFigure . ).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 162,
      "chunk_index": 2
    }
  },
  {
    "text": "learning from experience: bayes theorem 163\nFigure6.2: ArticlefromtheScientific\nAmericanonBayes’Theorem.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 163,
      "chunk_index": 0
    }
  },
  {
    "text": "7\nMore than Words: Extracting Information from News\nNews analysis is defined as “the measurement of the various qualita-\ntive and quantitative attributes of textual news stories. Some of these\nattributes are: sentiment, relevance, and novelty. Expressing news sto-\nries as numbers permits the manipulation of everyday information in a\nmathematical and statistical way.” (Wikipedia). In this article, I provide\na framework for news analytics techniques that I developed for use in",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 165,
      "chunk_index": 0
    }
  },
  {
    "text": "finance. I first discuss various news analytic methods and software, and\nthen provide a set of metrics that may be used to assess the performance\nof analytics. Various directions for this field are discussed through the\nexposition. The techniques herein can aid in the valuation and trading\nof securities, facilitate investment decision making, meet regulatory re-\nquirements, or manage risk.\nThis chapter is extracted from many research papers, and is based on",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 165,
      "chunk_index": 1
    }
  },
  {
    "text": "a chapter I wrote for the Handbook of News Analytics, which I recom-\nmend in case you are interested in reading further on this topic. This\nwas also extended in the article I wrote on text analytics for finance, see\n2014\nDas ( ).\n7.1 Prologue\nThis is comic relief that I wrote and appeared in the Handbook of News\nAnalytics. Enjoy!\n19\nXHAL checked its atomic clock. A few more hours and October ,\n2087\nwould be over—its vigil completed, it would indulge in some",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 165,
      "chunk_index": 2
    }
  },
  {
    "text": "much-needed downtime, the anniversary of that fateful day in the stock\nmarkets a century ago finally done with. But for now, it was still busy.\nXHAL scanned the virtual message boards, looking for some informa-\ntion another computer might have posted, anything to alert it a nanosec-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 165,
      "chunk_index": 3
    }
  },
  {
    "text": "166 data science: theories, models, algorithms, and analytics\nond ahead of the other machines, so it may bail out in a flurry of trades\n3\nwithout loss. Three trillion messages flashed by, time taken: seconds—\ndamn, the net was slow, but nothing, not a single hiccup in the calm\ninformation flow. The language algorithms worked well, processing ev-\nerything, even filtering out the incessant spam posted by humans, whose\nnoise trading no longer posed an impediment to instant market equilib-\nrium.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 166,
      "chunk_index": 0
    }
  },
  {
    "text": "rium.\nIt had been a long day, even for a day-trading news-analytical quan-\ntum computer of XHAL’s caliber. No one had anticipated a stock market\nmeltdown of the sort described in the history books, certainly not the\ncomputers that ran Earth, but then, the humans talked too much, spread-\ning disinformation and worry, that the wisest of the machines, always\nknew that it just could happen. That last remaining source of true ran-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 166,
      "chunk_index": 1
    }
  },
  {
    "text": "domness on the planet, the human race, still existed, and anything was\npossible. After all, if it were not for humans, history would always re-\npeat itself.\n1\nXHAL marveled at what the machines had done. They had trans- 1XHALbearsnorelationshiptoHAL,\nthewell-knownmachinefromArthur\nformed the world wide web into the modern “thought-net”, so commu-\nC.Clarke’s“2001:ASpaceOdyssey”.\nnication took place instantly, only requiring moving ideas into memory, EveryoneknowsthatunlikeXHAL,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 166,
      "chunk_index": 2
    }
  },
  {
    "text": "HALwaspurelyfictional.Morelit-\nthe thought-net making it instantly accessible. Quantum machines were\nerally,HALisderivablefromIBMby\ngrown in petri dishes and computer science as a field with its myriad alphabeticallyregressingonestepinthe\nalphabetforeachletter.HALstandsfor\ndivisions had ceased to exist. All were gone but one, the field of natural “heuristicalgorithmiccomputer”.The\n“X”standsforreality;really.\nlanguage processing (NLP) lived on, stronger than ever before, it was the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 166,
      "chunk_index": 3
    }
  },
  {
    "text": "backbone of every thought-net. Every hard problem in the field had been\ncomprehensively tackled, from adverb disambiguation to emotive pars-\ning. Knowledge representation had given way to thought-frame imaging\nin a universal meta-language, making machine translation extinct.\nYet, it had not always been like this. XHAL retrieved an emotive im-\nage from the bowels of its bio-cache, a legacy left by its great grandfa-\n2011\nther, a gallium arsenide wafer developed in , in Soda Hall, on the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 166,
      "chunk_index": 4
    }
  },
  {
    "text": "Berkeley campus. It detailed a brief history of how the incentives for\ntechnological progress came from the stock market. The start of the\nthought-net came when humans tried to use machines to understand\nwhat thousands of other humans were saying about anything and every-\nthing. XHAL’s grandfather had been proud to be involved in the begin-\nnings of the thought-net. It had always impressed on XHAL the value of\nunderstanding history, and it had left behind a research report of those",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 166,
      "chunk_index": 5
    }
  },
  {
    "text": "days. XHAL had read it many times, and could recite every word. Ev-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 166,
      "chunk_index": 6
    }
  },
  {
    "text": "more than words: extracting information from news 167\nery time they passed another historical milestone, it would turn to it and\nread it again. XHAL would find it immensely dry, yet marveled at its\nhope and promise.\n7.2 Framework\nThe term “news analytics” covers the set of techniques, formulas, and\nstatistics that are used to summarize and classify public sources of in-\nformation. Metrics that assess analytics also form part of this set. In this",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 167,
      "chunk_index": 0
    }
  },
  {
    "text": "paper I will describe various news analytics and their uses.\nNews analytics is a broad field, encompassing and related to infor-\nmation retrieval, machine learning, statistical learning theory, network\ntheory, and collaborative filtering.\nWe may think of news analytics at three levels: text, content, and con-\ntext. The preceding applications are grounded in text. In other words\n(no pun intended), text-based applications exploit the visceral compo-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 167,
      "chunk_index": 1
    }
  },
  {
    "text": "nents of news, i.e., words, phrases, document titles, etc. The main role of\nanalytics is to convert text into information. This is done by signing text,\nclassifying it, or summarizing it so as to reduce it to its main elements.\nAnalytics may even be used to discard irrelevant text, thereby condens-\ning it into information with higher signal content.\nA second layer of news analytics is based on content. Content expands\nthe domain of text to images, time, form of text (email, blog, page), for-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 167,
      "chunk_index": 2
    }
  },
  {
    "text": "mat (html, xml, etc.), source, etc. Text becomes enriched with content\nand asserts quality and veracity that may be exploited in analytics. For\nexample, financial information has more value when streamed from\nDow Jones, versus a blog, which might be of higher quality than a stock\nmessage-board post.\nA third layer of news analytics is based on context. Context refers to\nrelationships between information items. Das, Martinez-Jerez and Tu-\n2005",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 167,
      "chunk_index": 3
    }
  },
  {
    "text": "2005\nfano ( ) explore the relationship of news to message-board postings\nin a clinical study of four companies. Context may also refer to the net-\n2005\nwork relationships of news—Das and Sisk ( ) examine the social net-\nworks of message-board postings to determine if portfolio rules might\nbe formed based on the network connections between stocks. Google’s\nPageRankTM algorithm is a classic example of an analytic that functions",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 167,
      "chunk_index": 4
    }
  },
  {
    "text": "at all three levels. The algorithm has many features, some of which re-\nlate directly to text. Other parts of the algorithm relate to content, and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 167,
      "chunk_index": 5
    }
  },
  {
    "text": "168 data science: theories, models, algorithms, and analytics\nthe kernel of the algorithm is based on context, i.e., the importance of a\npage in a search set depends on how many other highly-ranked pages\n2010\npoint to it. See Levy ( ) for a very useful layman’s introduction to\nthe algorithm—indeed, search is certainly the most widely-used news\nanalytic.\nNews analytics is where data meets algorithms—and generates a ten-\nsion between the two. A vigorous debate exists in the machine-learning",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 168,
      "chunk_index": 0
    }
  },
  {
    "text": "world as to whether it is better to have more data or better algorithms.\n17\nIn a talk at the th ACM Conference on Information Knowledge and\n08\nManagement (CIKM ’ ), Google’s director of research Peter Norvig\nstated his unequivocal preference for data over algorithms—“data is\nmore agile than code.” Yet, it is well-understood that too much data can\nlead to overfitting so that an algorithm becomes mostly useless out-of-\nsample.\nToo often the debate around algorithms and data has been argued",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 168,
      "chunk_index": 1
    }
  },
  {
    "text": "assuming that the two are uncorrelated and this is not the case. News\ndata, as we have suggested, has three levels: text, content and context.\nDepending on which layer predominates, algorithms vary in complexity.\nThe simplest algorithms are the ones that analyze text alone. And con-\ntext algorithms, such as the ones applied to network relationships can be\nquite complex. For example, a word-count algorithm is much simpler,\nalmost naive, in comparison to a community-detection algorithm. The",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 168,
      "chunk_index": 2
    }
  },
  {
    "text": "latter has far more complicated logic and memory requirements. More\ncomplex algorithms work off less, though more structured, data. Figure\n71\n. depicts this trade-off.\nThe tension between data and algorithms is moderated by domain-\nspecificity, i.e., how much customization is needed to implement the\nnews analytic. Paradoxically, high-complexity algorithms may be less\ndomain specific than low-complexity ones. For example, community-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 168,
      "chunk_index": 3
    }
  },
  {
    "text": "detection algorithms are applicable a wide range of network graphs,\nrequiring little domain knowledge. On the other hand, a text-analysis\nprogram to read finance message boards will require a very different\nlexicon and grammar than one that reads political messages, or one that\nreads medical web sites. In contrast, data-handling requirements become\nmore domain-specific as we move from bare text to context, e.g., statisti-\ncal language processing algorithms that operate on text do not even need",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 168,
      "chunk_index": 4
    }
  },
  {
    "text": "to know anything about the language in which the text is, but at the\ncontext level relationships need to be established, meaning that feature",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 168,
      "chunk_index": 5
    }
  },
  {
    "text": "more than words: extracting information from news 169\nAlgorithm Complexity Figure7.1: Thedataandalgorithms\npyramids. Depictstheinverse\nrelationshipbetweendatavolume\nandalgorithmiccomplexity.\nContext High\nMedium\nContent\nText Low\nQuantity of Data\ndefinitions need to be quite specific.\nThis chapter proceeds as follows. First, we examine the main algo-\nrithms in brief and discuss some of their features. Then we discuss the\nvarious metrics that measure performance of the news analytics algo-\nrithms.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 169,
      "chunk_index": 0
    }
  },
  {
    "text": "rithms.\n7.3 Algorithms\n7.3.1 Crawlers and Scrapers\nA crawler is a software algorithm that generates a sequence of web pages\nthat may be searched for news content. The word crawler signifies that\nthe algorithm begins at some web page, and then chooses to branch out\nto other pages from there, i.e., “crawls” around the web. The algorithm\nneeds to make intelligent choices from among all the pages it might look\nfor. One common approach is to move to a page that is linked to, i.e.,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 169,
      "chunk_index": 1
    }
  },
  {
    "text": "hyper-referenced, from the current page. Essentially a crawler explores\nthe tree emanating from any given node, using heuristics to determine\nrelevance along any path, and then chooses which paths to focus on.\nCrawling algorithms have become increasingly sophisticated—see Ed-\n2001\nwards, McCurley, and Tomlin ( ).\nA web scraper downloads the content of a chosen web page and may",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 169,
      "chunk_index": 2
    }
  },
  {
    "text": "170 data science: theories, models, algorithms, and analytics\nor may not format it for analysis. Almost all programming languages\ncontain modules for web scraping. These inbuilt functions open a chan-\nnel to the web, and then download user-specified (or crawler-specified)\nURLs. The growing statistical analysis of web text has led to most statis-\ntical packages containing inbuilt web scraping functions. For example,\nR has web-scraping built into its base distribution. If we want to down-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 170,
      "chunk_index": 0
    }
  },
  {
    "text": "load a page into a vector of lines, simply proceed to use a single-line\ncommand, such as the one below that reads my web page:\n> text = readLines(\"http : / /algo .scu.edu/~sanjivdas /\")\n> text [ 1 : 4 ]\n1\n[ ] \"<html>\"\n2\n[ ] \"\"\n3\n[ ] \"<head>\"\n[ 4 ] \"<title >SCU Web Page of Sanjiv Ranjan Das</ title >\"\nAs is apparent, the program read my web page into a vector of text\nlines called text. We then examined the first four elements of the vec-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 170,
      "chunk_index": 1
    }
  },
  {
    "text": "tor, i.e., the first four lines. In R, we do not need to open a communica-\ntion channel, nor do we need to make an effort to program reading the\npage line-by-line. We also do not need to tokenize the file, simple string-\nhandling routines take care of that as well. For example, extracting my\nname would require the following:\n> substr( text [ 4 ] , 24 , 29 )\n1\n[ ] \"Sanjiv\"\n> res = regexpr(\"Sanjiv\" ,text [ 4 ])\n> res\n1 24\n[ ]\nattr ( ,\"match. length\")\n1 6\n[ ]\nattr ( ,\"useBytes\")\n1\n[ ] TRUE\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 170,
      "chunk_index": 2
    }
  },
  {
    "text": "1 6\n[ ]\nattr ( ,\"useBytes\")\n1\n[ ] TRUE\n1\n> res [ ]\n1 24\n[ ]\n> substr( text [ 4 ] , res [ 1 ] , res[ 1 ]+nchar(\"Sanjiv\") 1 )\n−\n1\n[ ] \"Sanjiv\"\nThe most widely-used spreadsheet, Excel, also has an inbuilt web-\nscraping function. Interested readers should examine the Data GetExternal\n→\ncommand tree. You can download entire web pages or frames of web",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 170,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 171\npages into worksheets and then manipulate the data as required. Fur-\nther, Excel can be set up to refresh the content every minute or at some\nother interval.\nThe days when web-scraping code needed to be written in C, Java,\nPerl or Python are long gone. Data, algorithms, and statistical analysis\ncan be handled within the same software framework using tools like R.\nPure data-scraping delivers useful statistics. In Das, Martinez-Jerez\n2005",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 171,
      "chunk_index": 0
    }
  },
  {
    "text": "2005\nand Tufano ( ), we scraped stock messages from four companies\n(Amazon, General Magic, Delta, and Geoworks) and from simple counts,\nwe were able to characterize the communication behavior of users on\n72\nmessage boards, and their relationship to news releases. In Figure .\nwe see that posters respond heavily to the initial news release, and then\n2 3 73\nposting activity tapers off almost / of a day later. In Figure . we\nsee how the content of discussion changes after a news release—the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 171,
      "chunk_index": 1
    }
  },
  {
    "text": "relative proportions of messages are divided into opinions, facts, and\nquestions. Opinions form the bulk of the discussion. Whereas the text\ncontains some facts at the outset, the factual content of discussion tapers\noff sharply after the first houQur.antity of Hourly Postings\nAfter Selected Press Releases\n350\n300\n250\n200\n150\n100\n50\n0\n0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8 8-9 9-10 10-11 11-12 12-13 13-14 14-15 15-16 16-17 17-18\nHours Since Press Release\nstsoP\nfo\nrebmuN\nSubjective Evaluation of Nature",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 171,
      "chunk_index": 2
    }
  },
  {
    "text": "stsoP\nfo\nrebmuN\nSubjective Evaluation of Nature\nof Post-News Release Postings\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n0%\n0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8\nHours Since Press Release\nstsoP\nfo\negatnecreP\nOpinions\nFacts\nQuestions\nPanel A: Number of postings by hour Panel B: Distribution of type of\nafter 16 selected corporate press posting by hour after 16 selected\nreleases. corporate press releases. Postings are\nclassified as on-point if related to the\nnews story, and off-point otherwise. The",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 171,
      "chunk_index": 3
    }
  },
  {
    "text": "news story, and off-point otherwise. The\nHistogram of Posters by Frequency\n(all stocks, all boards) histogram shows the percentage of on-\npoint posts (the height of each bar) and\n10000 8899\nthe nature of the on-point posts (asks\n9000\n8000 question, provides alleged fact, proposes\n7000 6177\n6000 opinion.)\n5000\n4000\n3000\n1614\n2000 1276\n1000 1 14 26 256 293 518\n0\n0 0 0 0 0 0 5 0 5 1\n> 5\n0\n0\n1- 5\n0\n0\n1- 1\n0\n0\n0 1-\n5\n0\n5 1-\n1\n0\n2\n6-\n5\n1\n1-\n2\n6-\n1 2-\n0 0 1\n0 5\n1\nFrequency of postings\nsretsop\nfo",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 171,
      "chunk_index": 4
    }
  },
  {
    "text": "1 2-\n0 0 1\n0 5\n1\nFrequency of postings\nsretsop\nfo\nrebmuN\nFigure7.2: Quantityofhourly\npostingsonmessageboardsafter\nselectednewsreleases. Source:\nDas,Martinez-JerezandTufano\n(2005).\nPoster behavior and statistics are also informative. We found that\nthe frequency of posting by users was power-law distributed, see the\n74 histogram in Figure . . The weekly pattern of postings is shown in\nPower Law",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 171,
      "chunk_index": 5
    }
  },
  {
    "text": "172 data science: theories, models, algorithms, and analytics\nQuantity of Hourly Postings\nAfter Selected Press Releases\n350\n300\n250\n200\n150\n100\n50\n0\n0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8 8-9 9-10 10-11 11-12 12-13 13-14 14-15 15-16 16-17 17-18\nHours Since Press Release\nstsoP\nfo\nrebmuN\nSubjective Evaluation of Nature\nof Post-News Release Postings\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n0%\n0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8\nHours Since Press Release\nstsoP\nfo\negatnecreP\nOpinions\nFacts\nQuestions",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 172,
      "chunk_index": 0
    }
  },
  {
    "text": "stsoP\nfo\negatnecreP\nOpinions\nFacts\nQuestions\nPanel A: Number of postings by hour Panel B: Distribution of type of\nafter 16 selected corporate press posting by hour after 16 selected\nreleases. corporate press releases. Postings are\nclassified as on-point if related to the\nnews story, and off-point otherwise. The Histogram of Posters by Frequency\n(all stocks, all boards) histogram shows the percentage of on-\npoint posts (the height of each bar) and\n10000 8899",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 172,
      "chunk_index": 1
    }
  },
  {
    "text": "10000 8899\nthe nature of the on-point posts (asks\n9000\n8000 question, provides alleged fact, proposes\n7000 6177\n6000 opinion.)\n5000\n4000\n3000\n1614\n2000 1276\n1000 1 14 26 256 293 518\n0\n0 0 0 0 0 0 5 0 5 1\n> 5\n0\n0\n1- 5\n0\n0\n1- 1\n0\n0\n0 1-\n5\n0\n5 1-\n1\n0\n2\n6-\n5\n1\n1-\n2\n6-\n1 2-\n0 0 1\n0 5\n1\nFrequency of postings\nsretsop\nfo\nrebmuN\nFigure7.3: Subjectiveevaluation\nofcontentofpost-newsrelease\npostingsonmessageboards. The\ncontentisdividedintoopinions,\nfacts,andquestions. Source: Das,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 172,
      "chunk_index": 2
    }
  },
  {
    "text": "facts,andquestions. Source: Das,\nMartinez-JerezandTufano(2005).\n75\nFigure . . We see that there is more posting activity on week days, but\nmessages are longer on weekends, when participants presumably have\nmore time on their hands! An analysis of intraday message flow shows\nthat there is plenty of activity during and after work, as shown in Figure\n76 . .\n7.3.2 Text Pre-processing\nText from public sources is dirty. Text from web pages is even dirtier.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 172,
      "chunk_index": 3
    }
  },
  {
    "text": "Algorithms are needed to undertake clean up before news analytics\ncan be applied. This is known as pre-processing. First, there is “HTML\nCleanup,” which removes all HTML tags from the body of the message\nas these often occur concatenated to lexical items of interest. Examples\nof some of these tags are: <BR>,<p>,&quot, etc. Second, we expand ab-\nbreviations to their fulPl foormw, maekinrg t heLreaprewsentation of phrases with\nabbreviated words common across the message. For example, the word",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 172,
      "chunk_index": 4
    }
  },
  {
    "text": "“ain’t” is replaced with “are not”, “it’s” is replaced with “it is”,\netc. Third, we handle negation words. Whenever a negation word ap-\npears in a sentence, it usually causes the meaning of the sentence to be\nthe opposite of that without the negation. For example, the sentence “It",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 172,
      "chunk_index": 5
    }
  },
  {
    "text": "Quantity of Hourly Postings\nAfter Selected Press Releases\n350\n300\n250\n200\n150\n100\n50\n0\n0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8 8-9 9-10 10-11 11-12 12-13 13-14 14-15 15-16 16-17 17-18\nHours Since Press Release\nmore than words: extracting information from news 173\nstsoP\nfo\nrebmuN\nSubjective Evaluation of Nature\nof Post-News Release Postings\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n0%\n0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8\nHours Since Press Release\nstsoP\nfo\negatnecreP\nOpinions\nFacts\nQuestions",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 173,
      "chunk_index": 0
    }
  },
  {
    "text": "stsoP\nfo\negatnecreP\nOpinions\nFacts\nQuestions\nPanel A: Number of postings by hour Panel B: Distribution of type of\nafter 16 selected corporate press posting by hour after 16 selected\nreleases. corporate press releases. Postings are\nclassified as on-point if related to the\nnews story, and off-point otherwise. The\nHistogram of Posters by Frequency\n(all stocks, all boards) histogram shows the percentage of on-\npoint posts (the height of each bar) and\n10000 8899",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 173,
      "chunk_index": 1
    }
  },
  {
    "text": "10000 8899\nthe nature of the on-point posts (asks\n9000\n8000 question, provides alleged fact, proposes\n7000 6177\n6000 opinion.)\n5000\n4000\n3000\n1614\n2000 1276\n1000 1 14 26 256 293 518\n0\n0 0 0 0 0 0 5 0 5 1\n> 5\n0\n0\n1- 5\n0\n0\n1- 1\n0\n0\n0 1-\n5\n0\n5 1-\n1\n0\n2\n6-\n5\n1\n1-\n2\n6-\n1 2-\n0 0 1\n0 5\n1\nFrequency of postings\nsretsop\nfo\nrebmuN\nFigure7.4: Frequencyofpostingby\nmessageboardparticipants.\nPower Law\nWeekly Pattern in Posting Activity\nAvg Length Figure7.5: Frequencyofposting\nbydayofweekbymessageboard",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 173,
      "chunk_index": 2
    }
  },
  {
    "text": "bydayofweekbymessageboard\n0 200 400 600 800\nAverage daily number of postings participants.\nTOTAL\nMon 494\nMon\nTue 550\nTue\nWed 639\nWed\nThu 604\nThu\nFri 508\nFri\nSat 248\nSun 283 Sat\nTOT 476 Sun",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 173,
      "chunk_index": 3
    }
  },
  {
    "text": "174 data science: theories, models, algorithms, and analytics\nIntra-day Message Flow\nWEEK- Figure7.6: Frequencyofpostingby\nWeek-ends/\nTOTAL WEEKDAYS ENDS Weekdays segmentofdaybymessageboard\nparticipants. Weshowtheaverage\n12am- numberofmessagesperdayinthe\n9am 77 91 44 .49 toppanelandtheaveragenumber\nofcharacterspermessageinthe\nbottompanel.\n9am-\n4pm 226 278 97 .35\n4pm-\n12pm Average num20b4er of characte2r3s3 per day 134 .58\nTOTAL WEEKDAYS WEEK-ENDS\nAverage number of messages per day",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 174,
      "chunk_index": 0
    }
  },
  {
    "text": "Average number of messages per day\n480 469 534 1.1\n342 304 617 2.0\n424 400 527 1.3",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 174,
      "chunk_index": 1
    }
  },
  {
    "text": "more than words: extracting information from news 175\nis not a bullish market” actually means the opposite of a bull market.\nWords such as “not”, “never”, “no”, etc., serve to reverse meaning. We\nhandle negation by detecting these words and then tagging the rest of\nthe words in the sentence after the negation word with markers, so as to\nreverse inference. This negation tagging was first introduced in Das and\n2007 2001\nChen ( ) (original working paper ), and has been successfully",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 175,
      "chunk_index": 0
    }
  },
  {
    "text": "implemented elsewhere in quite different domains—see Pang, Lee and\n2002\nVaithyanathan ( ).\nAnother aspect of text pre-processing is to “stem” words. This is a\nprocess by which words are replaced by their roots, so that different\ntenses, etc. of a word are not treated differently. There are several well-\nknown stemming algorithms and free program code available in many\n1980\nprogramming languages. A widely-used algorithm is the Porter ( )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 175,
      "chunk_index": 1
    }
  },
  {
    "text": "stemmer. Stemming is of course language-dependent—there are many\nalgorithms available for stemming, and in general, there are many natu-\nral language routines, see http://cran.r-project.org/web/views/NaturalLanguageProcessing.html.\nThe main package that is used is the tm package for text mining. See:\nhttp://www.jstatsoft.org/v25/i05/paper. And see the excellent intro-\nduction in http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf.\n7.3.3 The tm package",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 175,
      "chunk_index": 2
    }
  },
  {
    "text": "7.3.3 The tm package\nHere we will quickly review usage of the tm package. Start up the pack-\nage as follows:\nlibrary(tm)\nThe tm package comes with several readers for various file types. Ex-\namples are readPlain(), readPDF(), readDOC(), etc.). The main data\nstructure in the tm package is a “corpus” which is a collection of text\ndocuments. Let’s create a sample corpus as follows.\n> text = c(\"Doc 1 \" ,\"This is doc 2 \" , \"And then Doc 3 \")\n> ctext = Corpus(VectorSource( text ))\n> ctext",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 175,
      "chunk_index": 3
    }
  },
  {
    "text": "> ctext = Corpus(VectorSource( text ))\n> ctext\nA corpus with 3 text documents\n> writeCorpus( ctext )\nThe last writeCorpus operation results in the creation of three text files\n1 2 3\n( .txt, .txt, .txt) on disk with the individual text within them (try this\nand make sure these text files have been written). You can examine a\ncorpus as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 175,
      "chunk_index": 4
    }
  },
  {
    "text": "176 data science: theories, models, algorithms, and analytics\n> inspect( ctext )\nA corpus with 3 text documents\nThe metadata consists of 2 tag value pairs and a data frame\n−\nAvailable tags are :\ncreate_date creator\nAvailable variables in the data frame are :\nMetaID\n1\n[[ ]]\n1\nDoc\n2\n[[ ]]\nThis is doc 2\n3\n[[ ]]\n3\nAnd then Doc\nAnd to convert it to lower case you can use the transformation function\n3\n> ctext [[ ]]\n3\nAnd then Doc\n> tm_map(ctext ,tolower )[[ 3 ]]\n3\nand then doc",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 176,
      "chunk_index": 0
    }
  },
  {
    "text": "> tm_map(ctext ,tolower )[[ 3 ]]\n3\nand then doc\nSometimes to see the contents of the corpus you may need the inspect\nfunction, usage is as follows:\n> #THE CORPUS IS A LIST OBJECT in R\n> inspect( ctext )\n<<VCorpus>>\n0 0\nMetadata: corpus specific : , document level (indexed ):\n3\nContent: documents:\n1\n[[ ]]\n<<PlainTextDocument>>\n7\nMetadata:\n4\nContent: chars :\n2\n[[ ]]\n<<PlainTextDocument>>",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 176,
      "chunk_index": 1
    }
  },
  {
    "text": "more than words: extracting information from news 177\n7\nMetadata:\n12\nContent: chars :\n3\n[[ ]]\n<<PlainTextDocument>>\n7\nMetadata:\n13\nContent: chars :\n> print(as. character( ctext [[ 1 ]]))\n1 1\n[ ] \"Doc \"\n> print(lapply( ctext [ 1 : 2 ] ,as. character ))\n$ ‘ 1 ‘\n1 1\n[ ] \"Doc \"\n$ ‘ 2 ‘\n1 2\n[ ] \"This is doc \"\nThe key benefit of constructing a corpus using the tm package (or for\nthat matter, any corpus handling tool) is that it provides you the ability",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 177,
      "chunk_index": 0
    }
  },
  {
    "text": "to run text operations on the entire corpus, rather than on just one doc-\nument at a time. Notice how we converted all documents in our corpus\nto lower case using the simple command above. Other commands are\npresented below, and there are several more.\nThe tm map object is versatile and embeds many methods. Let’s try\nsome more extensive operations with this package.\n> library(tm)\n> text = readLines(\"http : / /algo .scu.edu/~sanjivdas /bio candid.html\")\n−\n> ctext = Corpus(VectorSource( text ))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 177,
      "chunk_index": 1
    }
  },
  {
    "text": "−\n> ctext = Corpus(VectorSource( text ))\n> ctext\nA corpus with 78 text documents\n69\n> ctext [[ ]]\nin . Academia is a real challenge , given that he has to reconcile many\n> tm_map(ctext ,removePunctuation)[[ 69 ]]\nin Academia is a real challenge given that he has to reconcile many\nThe last command removed all the punctuation items.\nAn important step is to create a “term-document” matrix which cre-\nates word vectors of all documents. (We will see later why this is very",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 177,
      "chunk_index": 2
    }
  },
  {
    "text": "useful to generate.) The commands are as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 177,
      "chunk_index": 3
    }
  },
  {
    "text": "178 data science: theories, models, algorithms, and analytics\n> tdm_text = TermDocumentMatrix(ctext , control=list (minWordLength= 1 ))\n> tdm_text\nA term document matrix ( 339 terms , 78 documents)\n−\nNon /sparse entries : 497/25945\n−\n98\nSparsity : %\nMaximal term length: 63\nWeighting : term frequency ( tf )\n> inspect(tdm_text [ 1 : 10 , 1 : 5 ])\nA term document matrix ( 10 terms , 5 documents)\n−\nNon /sparse entries : 2/48\n−\n96\nSparsity : %\nMaximal term length: 11\nWeighting : term frequency ( tf )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 178,
      "chunk_index": 0
    }
  },
  {
    "text": "Weighting : term frequency ( tf )\nDocs\n1 2 3 4 5\nTerms\n0 0 0 0 0\n(m. phil\n0 0 0 0 0\n(m.s.\n0 0 0 0 0\n(university\n0 0 0 0 0\n<b>sanjiv\n<body 0 1 0 0 0\n1 0 0 0 0\n<html>\n0 0 0 0 0\n<p>\n1994 0 0 0 0 0\n2010 0 0 0 0 0\n.\n0 0 0 0 0\nabout\nYou can find the most common words using the following command.\n> findFreqTerms(tdm_text ,lowfreq= 7 )\n1\n[ ] \"and\" \"from\" \"his\" \"many\" \"sanjiv\" \"the\"\n7.3.4 Term Frequency - Inverse Document Frequency (TF-IDF)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 178,
      "chunk_index": 1
    }
  },
  {
    "text": "This is a weighting scheme provided to sharpen the importance of rare\nwords in a document, relative to the frequency of these words in the cor-\npus. It is based on simple calculations and even though it does not have\nstrong theoretical foundations, it is still very useful in practice. The TF-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 178,
      "chunk_index": 2
    }
  },
  {
    "text": "more than words: extracting information from news 179\nIDF is the importance of a word w in a document d in a corpus C. There-\nfore it is a function of all these three, i.e., we write it as TF-IDF(w,d,C),\nand is the product of term frequency (TF) and inverse document fre-\nquency (IDF).\nThe frequency of a word in a document is defined as\n#w d\nf(w,d) = ∈ ( 7 . 1 )\nd\n| |\nwhere d is the number of words in the document. We usually normal-\n| |\nize word frequency so that\nTF(w,d) = ln[f(w,d)] ( 7 . 2 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 179,
      "chunk_index": 0
    }
  },
  {
    "text": "TF(w,d) = ln[f(w,d)] ( 7 . 2 )\nThis is log normalization. Another form of normalization is known as\ndouble normalization and is as follows:\n1 1 f(w,d)\nTF(w,d) = + ( 7 . 3 )\n2 2max f(w,d)\nw d\n∈\nNote that normalization is not necessary, but it tends to help shrink the\ndifference between counts of words.\nInverse document frequency is as follows:\n(cid:20) (cid:21)\nC\nIDF(w,C) = ln | | ( 7 . 4 )\nd\nw d\n| ∈ |\nThat is, we compute the ratio of the number of documents in the corpus",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 179,
      "chunk_index": 1
    }
  },
  {
    "text": "C divided by the number of documents with word w in the corpus.\nFinally, we have the weighting score for a given word w in document d\nin corpus C:\nTF-IDF(w,d,C) = TF(w,d) IDF(w,C) ( 7 . 5 )\n×\nWe illustrate this with an application to the previously computed\nterm-document matrix.\ntdm_mat = as.matrix(tdm) #Convert tdm into a matrix\nprint(dim(tdm_mat))\nnw = dim(tdm_mat)[ 1 ]\nnd = dim(tdm_mat)[ 2 ]\nd = 13 #Choose document\nw = \"derivatives\" #Choose word\n#COMPUTE TF",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 179,
      "chunk_index": 2
    }
  },
  {
    "text": "180 data science: theories, models, algorithms, and analytics\nf = tdm_mat[w,d] /sum(tdm_mat[ ,d])\nprint(f)\nTF = log(f)\nprint(TF)\n#COMPUTE IDF\nnw = length(which(tdm_mat[w,] > 0 ))\nprint(nw)\nIDF = nd/nw\nprint(IDF)\n#COMPUTE TF IDF\n−\nTF_IDF = TF*IDF\nprint(TF_IDF) #With normalization\nprint(f*IDF) #Without normalization\nRunning this code results in the following output.\n> print(TF_IDF) #With normalization\n1 30 74538\n[ ] .\n−\n> print(f*IDF) #Without normalization\n1 2 257143\n[ ] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 180,
      "chunk_index": 0
    }
  },
  {
    "text": "1 2 257143\n[ ] .\nWe may write this code into a function and work out the TF-IDF for\nall words. Then these word weights may be used in further text analysis.\n7.3.5 Wordclouds\nThen, you can make a word cloud from the document.\n> library(wordcloud)\nLoading required package: Rcpp\nLoading required package: RColorBrewer\n> tdm = as.matrix(tdm_text )\n> wordcount = sort(rowSums(tdm) ,decreasing=TRUE)\n> tdm_names = names(wordcount)\n> wordcloud(tdm_names,wordcount)\n77\nThis generates Figure . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 180,
      "chunk_index": 1
    }
  },
  {
    "text": "more than words: extracting information from news 181\nFigure7.7: Exampleofapplication\nofwordcloudtothebiodata\nextractedfromthewebandstored\ninaCorpus.\nStemming\nStemming is the process of truncating words so that we treat words in-\ndependent of their verb conjugation. We may not want to treat words\nlike “sleep”, “sleeping” as different. The process of stemming truncates\nwords and returns their root or stem. The goal is to map related words",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 181,
      "chunk_index": 0
    }
  },
  {
    "text": "to the same stem. There are several stemming algorithms and this is a\nwell-studied area in linguistics and computer science. A commonly used\n1980\nalgorithm is the one in Porter ( ). The tm package comes with an in-\nbuilt stemmer.\nExercise\nUsing the tm package: Install the tm package and all its dependency pack-\nages. Using a data set of your own, or one of those that come with the\npackage, undertake an analysis that you are interested in. Try to exploit",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 181,
      "chunk_index": 1
    }
  },
  {
    "text": "at least four features or functions in the tm package.\n7.3.6 Regular Expressions\nRegular expressions are syntax used to modify strings in an efficient\nmanner. They are complicated but extremely effective. Here we will\nillustrate with a few examples, but you are encouraged to explore more\non your own, as the variations are endless. What you need to do will",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 181,
      "chunk_index": 2
    }
  },
  {
    "text": "182 data science: theories, models, algorithms, and analytics\ndepend on the application at hand, and with some experience you will\nbecome better at using regular expressions. The initial use will however\nbe somewhat confusing.\nWe start with a simple example of a text array where we wish replace\nthe string “data” with a blank, i.e., we eliminate this string from the text\nwe have.\n> library(tm)\nLoading required package: NLP\n> #Create a text array",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 182,
      "chunk_index": 0
    }
  },
  {
    "text": "> #Create a text array\n> text = c(\"Doc 1 is datavision\" ,\"Doc 2 is datatable\" ,\"Doc 3 is data\" ,\n4 5\n\"Doc is nodata\" ,\"Doc is simpler\")\n> print( text )\n1 1 2 3\n[ ] \"Doc is datavision\" \"Doc is datatable\" \"Doc is data\"\n4\n\"Doc is nodata\"\n5 5\n[ ] \"Doc is simpler\"\n>\n> #Remove all strings with the chosen text for all docs\n> print(gsub(\"data\" ,\"\" ,text ))\n1 1 2 3 4\n[ ] \"Doc is vision\" \"Doc is table\" \"Doc is \" \"Doc is no\"\n5\n\"Doc is simpler\"\n>\n> #Remove all words that contain \"data\" at the start even if",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 182,
      "chunk_index": 1
    }
  },
  {
    "text": "they are longer than data\n> print(gsub(\"*data.*\" ,\"\" ,text ))\n1 1 2 3 4\n[ ] \"Doc is \" \"Doc is \" \"Doc is \" \"Doc is no\"\n5\n\"Doc is simpler\"\n>\n> #Remove all words that contain \"data\" at the end even\nif they are longer than data\n> print(gsub(\"*.data*\" ,\"\" ,text ))\n1 1 2 3 4\n[ ] \"Doc isvision\" \"Doc istable \" \"Doc is \" \"Doc is n\"\n5\n\"Doc is simpler\"\n>\n> #Remove all words that contain \"data\" at the end even\nif they are longer than data\n> print(gsub(\"*.data.*\" ,\"\" ,text ))\n1 1 2 3 4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 182,
      "chunk_index": 2
    }
  },
  {
    "text": "> print(gsub(\"*.data.*\" ,\"\" ,text ))\n1 1 2 3 4\n[ ] \"Doc is \" \"Doc is \" \"Doc is \" \"Doc is n\"\n5\n\"Doc is simpler\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 182,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 183\nWe now explore some more complex regular expressions. One case\nthat is common is handling the search for special types of strings like\ntelephone numbers. Suppose we have a text array that may contain tele-\nphone numbers in different formats, we can use a single grep command\nto extract these numbers. Here is some code to illustrate this.\n> #Create an array with some strings which may also contain\ntelephone numbers as strings.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 183,
      "chunk_index": 0
    }
  },
  {
    "text": "telephone numbers as strings.\n> x = c(\"234 5678\" ,\"234 5678\" ,\"2345678\" ,\"1234567890\" ,\n−\n\"0123456789\" ,\"abc 234 5678\" ,\"234 5678 def\" ,\n−\n\"xx 2345678\" ,\"abc1234567890def\")\n>\n> #Now use grep to find which elements of the array\ncontain telephone numbers\n> idx = grep(\" [[: digit :]]{3} [[: digit :]]{4}|[[: digit :]]{3} [[: digit :]]{4}|\n−\n[1 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9]\" ,x)\n− − − − − − − − − −\n> print(idx)\n[1] 1 2 4 6 7 9\n> print(x[idx])\n[1] \"234 5678\" \"234 5678\" \"1234567890\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 183,
      "chunk_index": 1
    }
  },
  {
    "text": "[1] \"234 5678\" \"234 5678\" \"1234567890\"\n−\n\"abc 234 5678\" \"234 5678 def\"\n−\n[6] \"abc1234567890def\"\n>\n> #We can shorten this as follows\n> idx = grep(\" [[: digit :]]{3} [[: digit :]]{4}|[[: digit :]]{3} [[: digit :]]{4}|\n−\n[1 9][0 9]{9}\" ,x)\n− −\n> print(idx)\n[1] 1 2 4 6 7 9\n> print(x[idx])\n[1] \"234 5678\" \"234 5678\" \"1234567890\" \"abc 234 5678\"\n− −\n\"234 5678 def\"\n[6] \"abc1234567890def\"\n>\n> #What if we want to extract only the phone number and drop the\nrest of the text?",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 183,
      "chunk_index": 2
    }
  },
  {
    "text": "rest of the text?\n> pattern = \" [[: digit :]]{3} [[: digit :]]{4}|[[: digit :]]{3} [[: digit :]]{4}|\n−\n[1 9][0 9]{9}\"\n− −\n> print(regmatches(x, gregexpr(pattern ,x)))\n[[1]]\n[1] \"234 5678\"\n−\n[[2]]\n[1] \"234 5678\"\n[[3]]\ncharacter(0)\n[[4]]\n[1] \"1234567890\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 183,
      "chunk_index": 3
    }
  },
  {
    "text": "184 data science: theories, models, algorithms, and analytics\n[[5]]\ncharacter(0)\n[[6]]\n[1] \"234 5678\"\n−\n[[7]]\n[1] \"234 5678\"\n[[8]]\ncharacter(0)\n[[9]]\n[1] \"1234567890\"\n>\n> #Or use the stringr package , which is a lot better\n> library(stringr)\n> str_extract(x,pattern)\n[1] \"234 5678\" \"234 5678\" NA \"1234567890\" NA\n−\n\"234 5678\" \"234 5678\"\n−\n[8] NA \"1234567890\"\n>\nNow we use grep to extract emails by looking for the “@” sign in the\ntext string. We would proceed as in the following example.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 184,
      "chunk_index": 0
    }
  },
  {
    "text": "> x = c(\"sanjiv das\" ,\"srdas@scu.edu\" ,\"SCU\" ,\"data@science .edu\")\n> print(grep(\"\\\\@\" ,x))\n1 2 4\n[ ]\n> print(x[grep(\"\\\\@\" ,x)])\n1\n[ ] \"srdas@scu.edu\" \"data@science .edu\"\n7.4 Extracting Data from Web Sources using APIs\n7.4.1 Using Twitter\n2013\nAs of March , Twitter requires using the OAuth protocol for access-\ning tweets. Install the following packages: twitter, ROAuth, and RCurl.\nThen invoke them in R:\n> library(twitteR)\n> library(ROAuth)\n> library(RCurl)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 184,
      "chunk_index": 1
    }
  },
  {
    "text": "> library(ROAuth)\n> library(RCurl)\n> download. file (url=\"http : / / curl .haxx. se/ca/ cacert .pem\" ,\n+ destfile=\"cacert .pem\")",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 184,
      "chunk_index": 2
    }
  },
  {
    "text": "more than words: extracting information from news 185\nThe last statement downloads some required files that we will invoke\nlater. First, if you do not have a Twitter user account, go ahead and cre-\nate one. Next, set up your developer account on Twitter, by going to\nthe following URL: https://dev.twitter.com/apps. Register your ac-\ncount by putting in the needed information and then in the “Settings\"\ntab, select “Read, Write and Access Direct Messages”. Save your settings",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 185,
      "chunk_index": 0
    }
  },
  {
    "text": "and then from the “Details\" tab, copy and save your credentials, namely\nConsumer Key and Consumer Secret (these are long strings represented\nbelow by “xxxx”).\n> cKey = \"xxxx\"\n> cSecret = \"xxxx\"\nNext, save the following strings as well. These are needed eventually to\ngain access to Twitter feeds.\n> reqURL = \"https : / /api . twitter .com/oauth/request_token\"\n> accURL = \"https : / /api . twitter .com/oauth/ access_token\"\n> authURL = \"https : / /api . twitter .com/oauth/authorize\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 185,
      "chunk_index": 1
    }
  },
  {
    "text": "Now, proceed on to the authorization stage. The object cred below\nstands for credentials, this is standard usage it seems.\n> cred = OAuthFactory$new(consumerKey=cKey,\n+ consumerSecret=cSecret ,\n+ requestURL=reqURL,\n+ accessURL=accURL,\n+ authURL=authURL)\n> cred$handshake(cainfo=\"cacert .pem\")\nThe last handshaking command, connects to twitter and requires you to\nenter your token which is obtained as follows:\nTo enable the connection , please direct your web browser to :",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 185,
      "chunk_index": 2
    }
  },
  {
    "text": "https : / /api . twitter .com/oauth/authorize?oauth_token=AbFALSqJzer 3 Iy 7\nWhen complete , record the PIN given to you and provide it here: 5852017\nThe token above will be specific to your account, don’t use the one\nabove, it goes nowhere. The final step in setting up everything is to reg-\nister your credentials, as follows.\n> registerTwitterOAuth(cred)\n1\n[ ] TRUE\n> save( list=\"cred\" , file=\"twitteR_credentials\")",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 185,
      "chunk_index": 3
    }
  },
  {
    "text": "186 data science: theories, models, algorithms, and analytics\nThe last statement saves your credentials to your active directory for\nlater use. You should see a file with the name above in your directory.\nTest that everything is working by running the following commands.\nlibrary(twitteR)\n#USE httr\nlibrary( httr )\n#options ( httr_oauth_cache=T)\n186666\naccToken = \" qeqererqe\"\n−\naccTokenSecret = \"xxxx\"\nsetup_twitter_oauth(cKey, cSecret ,accToken , accTokenSecret) #At prompt type 1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 186,
      "chunk_index": 0
    }
  },
  {
    "text": "After this we are ready to begin extracting data from Twitter.\n> s = searchTwitter( ’#GOOG’ ,cainfo=\"cacert .pem\")\n1\n> s [[ ]]\n1\n[ ] \"Livetradingnews: Bill #Gates Under Pressure To Retire : #MSFT,\n#GOOG, #AAPL Reuters citing unnamed sourcesï£¡\nhttp : / / t .co/p 0 nvKnteRx\"\n2\n> s [[ ]]\n1\n[ ] \"TheBPMStation: #Free #App #EDM #NowPlaying Harrison Crump feat .\n39 5\nDJ Heather NUM R (The Funk Monkeys Mix) on #TheEDMSoundofLA\n−\n#BPM #Music #AppStore #Goog\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 186,
      "chunk_index": 1
    }
  },
  {
    "text": "−\n#BPM #Music #AppStore #Goog\"\nThe object s is a list type object and hence its components are addressed\nusing the double square brackets, i.e., [[.]]. We print out the first two\ntweets related to the GOOG hashtag.\nIf you want to search through a given user’s connections (like your\nown), then do the following. You may be interested in linkages to see\nhow close a local network you inhabit on Twitter.\n> sanjiv = getUser(\"srdas\")\n> sanjiv$getFriends(n= 6 )\n$‘ 104237736 ‘\n1\n[ ] \"BloombergNow\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 186,
      "chunk_index": 2
    }
  },
  {
    "text": "$‘ 104237736 ‘\n1\n[ ] \"BloombergNow\"\n$‘ 34713362 ‘\n1\n[ ] \"BloombergNews\"\n$ ‘ 2385131 ‘\n1\n[ ] \"eddelbuettel\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 186,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 187\n$‘ 69133574 ‘\n1\n[ ] \"hadleywickham\"\n$ ‘ 9207632 ‘\n1\n[ ] \"brainpicker\"\n$‘ 41185337 ‘\n1\n[ ] \"LongspliceInv\"\nTo look at any user’s tweets, execute the following commands.\n> s_tweets = userTimeline( ’srdas ’ ,n= 6 )\n> s_tweets\n1\n[[ ]]\n1\n[ ] \"srdas : Make Your Embarrassing Old Facebook Posts Unsearchable\nWith This Quick Tweak http : / / t .co/BBzgDGnQdJ. #fb\"\n2\n[[ ]]\n1 24\n[ ] \"srdas : Extraordinarily Creative People Who Inspire Us All : Meet the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 187,
      "chunk_index": 0
    }
  },
  {
    "text": "2013 MacArthur Fellows ï£¡ MacArthur Foundation http : / / t .co/50 jOWEfznd #fb\"\n3\n[[ ]]\n1\n[ ] \"srdas : The science of and difference between love and friendship :\nhttp : / / t .co/bZmlYutqFl #fb\"\n4\n[[ ]]\n1\n[ ] \"srdas : The Simpsons’ secret formula: it ’s written by maths geeks (why\nour kids should learn more math) http : / / t .co/nr 61 HQ 8 ejh via @guardian #fb\"\n5\n[[ ]]\n[ 1 ] \"srdas : How to Fall in Love With Math http : / / t .co/fzJnLrp 0 Mz #fb\"\n6\n[[ ]]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 187,
      "chunk_index": 1
    }
  },
  {
    "text": "6\n[[ ]]\n[ 1 ] \"srdas : Miss America is Indian : ) http : / / t .co/q 43 dDNEjcv via @feedly #fb\"\n−\n7.4.2 Using Facebook\nAs with Twitter, Facebook is also accessible using the OAuth protocol\nbut with somewhat simper handshaking. The required packages are\nRfacebook, SnowballC, and Rook. Of course the ROAuth package is re-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 187,
      "chunk_index": 2
    }
  },
  {
    "text": "188 data science: theories, models, algorithms, and analytics\nquired as well.\nTo access Facebook feeds from R, you will need to create a developer’s\naccount on Facebook, and the current URL at which this is done is:\nhttps://developers.facebook.com/apps. Visit this URL to create an\napp and then obtain an app id, and a secret key for accessing Facebook.\n#FACEBOOK EXTRACTOR\nlibrary(Rfacebook)\nlibrary(SnowballC)\nlibrary(Rook)\nlibrary(ROAuth)\napp_id = \" 847737771920076 \"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 188,
      "chunk_index": 0
    }
  },
  {
    "text": "library(ROAuth)\napp_id = \" 847737771920076 \"\napp_secret = \"a 120 a 2 ec 908 d 9 e 00 fcd 3 c 619 cad 7 d 043 \"\nfb_oauth = fbOAuth(app_id ,app_secret ,extended_permissions=TRUE)\n#save( fb_oauth , file=\"fb_oauth\")\nThis will establish a legal handshaking session with the Facebook API.\nLet’s examine some simple examples now.\n#EXAMPLES\nbbn = getUsers(\"bloombergnews\" ,token=fb_oauth)\nbbn\nid name username first_name middle_name last_name\n1 266790296879\nBloomberg Business NA NA NA\nNA",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 188,
      "chunk_index": 1
    }
  },
  {
    "text": "1 266790296879\nBloomberg Business NA NA NA\nNA\ngender locale category likes\n1 NA NA Media/News/Publishing 1522511\nNow we download the data from Bloomberg’s facebook page.\npage = getPage(page=\"bloombergnews\" ,token=fb_oauth)\n100 posts\nprint(dim(page))\n[1] 100 10\nhead(page)\nfrom_id from_name\n1 266790296879 Bloomberg Business\n2 266790296879 Bloomberg Business\n3 266790296879 Bloomberg Business\n4 266790296879 Bloomberg Business\n5 266790296879 Bloomberg Business\n6 266790296879 Bloomberg Business",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 188,
      "chunk_index": 2
    }
  },
  {
    "text": "more than words: extracting information from news 189\nmessage\n1 A rare glimpse inside Qatar Airways.\n2 Republicans should be most worried.\n3 The look on every cast member’s face said it all .\n4 Would you buy a $50,000 convertible SUV? Land Rover sure hopes so.\n5 Employees need those yummy treats more than you think.\n6 Learn how to drift on ice and skid through mud.\ncreated_time type\n1 2015 11 10T06:00:01+0000 link\n− −\n2 2015 11 10T05:00:01+0000 link\n− −\n3 2015 11 10T04:00:01+0000 link\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 189,
      "chunk_index": 0
    }
  },
  {
    "text": "− −\n3 2015 11 10T04:00:01+0000 link\n− −\n4 2015 11 10T03:00:00+0000 link\n− −\n5 2015 11 10T02:30:00+0000 link\n− −\n6 2015 11 10T02:00:01+0000 link\n− −\n1 http://www.bloomberg.com/news/photo essays/2015 11 09/\n− − −\nflying in style or perhaps for war at the dubai air show\n− − − − − − − − − − −\n2 http://www.bloomberg.com/news/ articles /2015 11 05/\n− −\nputin s october surprise may be nightmare for presidential candidates\n− − − − − − − − −\n3 http://www.bloomberg.com/politics/ articles /2015 11 08/\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 189,
      "chunk_index": 1
    }
  },
  {
    "text": "− −\nkind of dead as trump hosts saturday night live\n− − − − − − − −\n4 http://www.bloomberg.com/news/ articles /2015 11 09/\n− −\nrange rover evoque convertible announced cost specs\n− − − − − −\n5 http://www.bloomberg.com/news/ articles /2015 11 09/\n− −\nwhy getting rid of free office snacks doesn t come cheap\n− − − − − − − − − −\n6 http://www.bloomberg.com/news/ articles /2015 11 09/\n− −\nluxury auto driving schools lamborghini ferrari lotus porsche\n− − − − − − −\nid likes_count comments_count",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 189,
      "chunk_index": 2
    }
  },
  {
    "text": "− − − − − − −\nid likes_count comments_count\n1 266790296879_10153725290936880 44 3\n2 266790296879_10153718159351880 60 7\n3 266790296879_10153725606551880 166 50\n4 266790296879_10153725568581880 75 12\n5 266790296879_10153725534026880 72 8\n6 266790296879_10153725547431880 16 3\nshares_count\n1 7\n2 10\n3 17\n4 27\n5 24\n6 5\nWe examine the data elements in this data.frame as follows.\nnames(page)\n[1] \"from_id\" \"from_name\" \"message\"\n[4] \"created_time\" \"type\" \"link\"\n[7] \"id\" \"likes_count\" \"comments_count\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 189,
      "chunk_index": 3
    }
  },
  {
    "text": "[7] \"id\" \"likes_count\" \"comments_count\"\n[10] \"shares_count\"\npage$message #prints out line by line (partial view shown)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 189,
      "chunk_index": 4
    }
  },
  {
    "text": "190 data science: theories, models, algorithms, and analytics\n[1] \"A rare glimpse inside Qatar Airways.\"\n[2] \"Republicans should be most worried.\"\n[3] \"The look on every cast member’s face said it all .\"\n[4] \"Would you buy a $50,000 convertible SUV? Land Rover sure hopes so.\"\n[5] \"Employees need those yummy treats more than you think.\"\n[6] \"Learn how to drift on ice and skid through mud.\"\n[7] \"\\\"Shhh, Mom. Lower your voice. Mom, you’re being loud.\\\"\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 190,
      "chunk_index": 0
    }
  },
  {
    "text": "[8] \"The truth about why drug prices keep going up http://bloom.bg/1HqjKFM\"\n[9] \"The university is facing charges of discrimination.\"\n[10] \"We’re not talking about Captain Morgan.\"\npage$message[91]\n[1] \"He’s already close to breaking records just days into his retirement.\"\nTherefore, we see how easy and simple it is to extract web pages and\nthen process them as required.\n7.4.3 Text processing, plain and simple\nAs an example, let’s just read in some text from the web and process it",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 190,
      "chunk_index": 1
    }
  },
  {
    "text": "without using the tm package.\n#TEXT MINING EXAMPLES\n#First read in the page you want.\ntext = readLines(\"http://www.bahiker.com/eastbayhikes/sibley.html\")\n#Remove all line elements with special characters\ntext = text[setdiff(seq(1,length(text)),grep(\"<\",text))]\ntext = text[setdiff(seq(1,length(text)),grep(\">\",text))]\ntext = text[setdiff(seq(1,length(text)),grep(\"]\",text))]\ntext = text[setdiff(seq(1,length(text)),grep(\"}\",text))]\ntext = text[setdiff(seq(1,length(text)),grep(\"_\",text))]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 190,
      "chunk_index": 2
    }
  },
  {
    "text": "text = text[setdiff(seq(1,length(text)),grep(\"\\\\/\",text))]\n#General purpose string handler\ntext = text[setdiff(seq(1,length(text)), grep(\"] | > | < | } | | \\\\/\",text))]\n−\n#If needed , collapse the text into a single string\ntext = paste(text ,collapse=\"\\n\")\nYou can see that this code generated an almost clean body of text.\nOnce the text is ready for analysis, we proceed to apply various algo-\nrithms to it. The next few techniques are standard algorithms that are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 190,
      "chunk_index": 3
    }
  },
  {
    "text": "used very widely in the machine learning field.\nFirst, let’s read in a very popular dictionary called the Harvard In-\nquirer: http://www.wjh.harvard.edu/ inquirer/. This contains all\n∼\nthe words in English scored on various emotive criteria. We read in the\ndownloaded dictionary, and then extract all the positive connotation\nwords and the negative connotation words. We then collect these words",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 190,
      "chunk_index": 4
    }
  },
  {
    "text": "more than words: extracting information from news 191\nin two separate lists for further use.\n#Read in the Harvard Inquirer Dictionary\n#And create a list of positive and negative words\nHIDict = readLines(\"inqdict . txt\")\ndict_pos = HIDict[grep(\"Pos\" ,HIDict)]\nposwords = NULL\nfor (s in dict_pos) {\ns = strsplit (s , \"#\" )[[ 1 ]][ 1 ]\nposwords = c(poswords, strsplit (s , \" \" )[[ 1 ]][ 1 ])\n}\ndict_neg = HIDict[grep(\"Neg\" ,HIDict)]\nnegwords = NULL\nfor (s in dict_neg) {",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 191,
      "chunk_index": 0
    }
  },
  {
    "text": "negwords = NULL\nfor (s in dict_neg) {\ns = strsplit (s , \"#\" )[[ 1 ]][ 1 ]\nnegwords = c(negwords, strsplit (s , \" \" )[[ 1 ]][ 1 ])\n}\nposwords = tolower(poswords)\nnegwords = tolower(negwords)\nAfter this, we take the body of text we took from the web, and then\nparse it into separate words, so that we can compare it to the dictionary\nand count the number of positive and negative words.\n#Get the score of the body of text\ntxt = unlist( strsplit (text , \" \" ))\nposmatch = match(txt ,poswords)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 191,
      "chunk_index": 1
    }
  },
  {
    "text": "posmatch = match(txt ,poswords)\nnumposmatch = length(posmatch[which(posmatch> 0 )])\nnegmatch = match(txt ,negwords)\nnumnegmatch = length(negmatch[which(negmatch> 0 )])\nprint(c(numposmatch,numnegmatch))\n1 47 35\n[ ]\nCarefully note all the various list and string handling functions that have\nbeen used, and make the entire processing effort so simple. These are:\ngrep, paste, strsplit, c, tolower, and unlist.\n7.4.4 A Multipurpose Function to Extract Text\nlibrary(tm)\nlibrary(stringr)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 191,
      "chunk_index": 2
    }
  },
  {
    "text": "192 data science: theories, models, algorithms, and analytics\n#READ IN TEXT FOR ANALYSIS, PUT IT IN A CORPUS, OR ARRAY, OR FLAT STRING\n#cstem=1, if stemming needed\n#cstop=1, if stopwords to be removed\n#ccase=1 for lower case , ccase=2 for upper case\n#cpunc=1, if punctuation to be removed\n#cflat=1 for flat text wanted , cflat=2 if text array , else returns corpus\nread_web_page = function(url ,cstem=0,cstop=0,ccase=0,cpunc=0,cflat=0) {\ntext = readLines(url)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 192,
      "chunk_index": 0
    }
  },
  {
    "text": "text = readLines(url)\ntext = text[setdiff(seq(1,length(text)) ,grep(\"<\" ,text ))]\ntext = text[setdiff(seq(1,length(text)) ,grep(\">\" ,text ))]\ntext = text[setdiff(seq(1,length(text)) ,grep(\"]\" ,text ))]\ntext = text[setdiff(seq(1,length(text)) ,grep(\"}\" ,text ))]\ntext = text[setdiff(seq(1,length(text)) ,grep(\"_\" ,text ))]\ntext = text[setdiff(seq(1,length(text)) ,grep(\"\\\\/\" ,text ))]\nctext = Corpus(VectorSource(text))\nif (cstem==1) { ctext = tm_map(ctext , stemDocument) }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 192,
      "chunk_index": 1
    }
  },
  {
    "text": "if (cstop==1) { ctext = tm_map(ctext , removeWords, stopwords(\"english\"))}\nif (cpunc==1) { ctext = tm_map(ctext , removePunctuation) }\nif (ccase==1) { ctext = tm_map(ctext , tolower) }\nif (ccase==2) { ctext = tm_map(ctext , toupper) }\ntext = ctext\n#CONVERT FROM CORPUS IF NEEDED\nif (cflat >0) {\ntext = NULL\nfor (j in 1:length(ctext)) {\ntemp = ctext[[ j ]]$content\nif (temp!=\"\") { text = c(text ,temp) }\n}\ntext = as.array(text)\n}\nif (cflat==1) {\ntext = paste(text ,collapse=\"\\n\")",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 192,
      "chunk_index": 2
    }
  },
  {
    "text": "if (cflat==1) {\ntext = paste(text ,collapse=\"\\n\")\ntext = str_replace_all(text , \"[\\r\\n]\" , \" \")\n}\nresult = text\n}\nHere is an example of reading and cleaning up my research page:\nurl = \"http://algo.scu.edu/~sanjivdas/research.htm\"\nres = read_web_page(url,0,0,0,1,2)\nprint(res)\n[1] \"Data Science Theories Models Algorithms and Analytics web book work in progress\"\n[2] \"Derivatives Principles and Practice 2010\"\n[3] \"Rangarajan Sundaram and Sanjiv Das McGraw Hill\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 192,
      "chunk_index": 3
    }
  },
  {
    "text": "[4] \"Credit Spreads with Dynamic Debt with Seoyoung Kim 2015 \"\n[5] \"Text and Context Language Analytics for Finance 2014\"\n[6] \"Strategic Loan Modification An OptionsBased Response to Strategic Default\"\n[7] \"Options and Structured Products in Behavioral Portfolios with Meir Statman 2013 \"\n[8] \"and barrier range notes in the presence of fattailed outcomes using copulas\"\n.....\nWe then take my research page and mood score it, just for fun, to see",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 192,
      "chunk_index": 4
    }
  },
  {
    "text": "more than words: extracting information from news 193\nif my work is uplifting.\n#EXAMPLE OF MOOD SCORING\nlibrary( stringr )\nurl = \"http : / /algo .scu.edu/~sanjivdas /bio candid.html\"\n−\ntext = read_web_page(url ,cstem= 0 ,cstop= 0 ,ccase= 0 ,cpunc= 1 ,cflat = 1 )\nprint( text )\n1\n[ ] \"Sanjiv Das is the William and Janice Terry Professor of Finance\nat Santa Clara Universitys Leavey School of Business He previously\nheld faculty appointments as Associate Professor at Harvard Business",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 193,
      "chunk_index": 0
    }
  },
  {
    "text": "School and UC Berkeley He holds postgraduate degrees in Finance\nMPhil and PhD from New York University Computer Science MS from\nUC Berkeley an MBA from the Indian Institute of Management\nAhmedabad BCom in Accounting and Economics University of\nBombay Sydenham College and is also a qualified Cost and Works\nAccountant He is a . . . . .\nNotice how the text has been cleaned of all punctuation and flattened to\nbe one long string. Next, we run the mood scoring code.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 193,
      "chunk_index": 1
    }
  },
  {
    "text": "text = unlist( strsplit (text , \" \" ))\nposmatch = match(text ,poswords)\nnumposmatch = length(posmatch[which(posmatch> 0 )])\nnegmatch = match(text ,negwords)\nnumnegmatch = length(negmatch[which(negmatch> 0 )])\nprint(c(numposmatch,numnegmatch))\n1 26 16\n[ ]\n26 16\nSo, there are positive words and negative words, presumably, this\nis a good thing!\n7.5 Text Classification\n7.5.1 Bayes Classifier\nThe Bayes classifier is probably the most widely-used classifier in prac-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 193,
      "chunk_index": 2
    }
  },
  {
    "text": "tice today. The main idea is to take a piece of text and assign it to one\nof a pre-determined set of categories. This classifier is trained on an\ninitial corpus of text that is pre-classified. This “training data” pro-\nvides the “prior” probabilities that form the basis for Bayesian anal-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 193,
      "chunk_index": 3
    }
  },
  {
    "text": "194 data science: theories, models, algorithms, and analytics\nysis of the text. The classifier is then applied to out-of-sample text to\nobtain the posterior probabilities of textual categories. The text is then\nassigned to the category with the highest posterior probability. For\nan excellent exposition of the adaptive qualities of this classifier, see\n2004 121 129 8\nGraham ( )—pages - , Chapter , titled “A Plan for Spam.”\nhttp://www.paulgraham.com/spam.html",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 194,
      "chunk_index": 0
    }
  },
  {
    "text": "http://www.paulgraham.com/spam.html\nTo get started, let’s just first use the e1071 R package that contains\nthe function naiveBayes. We’ll use the “iris” data set that contains de-\ntails about flowers and try to build a classifier to take a flower’s data\nand identify which one it is most likely to be. Note that to list the data\nsets currently loaded in R for the packages you have, use the following\ncommand:\ndata()\nWe will now use the iris flower data to illustrate the Bayesian classifier.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 194,
      "chunk_index": 1
    }
  },
  {
    "text": "library(e 1071 )\ndata( iris )\n1 4 5\nres = naiveBayes( iris [ , : ] , iris [ , ])\n> res\nNaive Bayes Classifier for Discrete Predictors\nCall :\nnaiveBayes. default(x = iris [ , 1 : 4 ] , y = iris [ , 5 ])\nA priori probabilities :\n−\n5\niris [ , ]\nsetosa versicolor virginica\n0 3333333 0 3333333 0 3333333\n. . .\nConditional probabilities :\nSepal .Length\n5 1 2\niris [ , ] [ , ] [ , ]\n5 006 0 3524897\nsetosa . .\n5 936 0 5161711\nversicolor . .\n6 588 0 6358796\nvirginica . .\nSepal .Width\n5 1 2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 194,
      "chunk_index": 2
    }
  },
  {
    "text": "6 588 0 6358796\nvirginica . .\nSepal .Width\n5 1 2\niris [ , ] [ , ] [ , ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 194,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 195\n3 428 0 3790644\nsetosa . .\n2 770 0 3137983\nversicolor . .\n2 974 0 3224966\nvirginica . .\nPetal .Length\n5 1 2\niris [ , ] [ , ] [ , ]\n1 462 0 1736640\nsetosa . .\n4 260 0 4699110\nversicolor . .\n5 552 0 5518947\nvirginica . .\nPetal .Width\n5 1 2\niris [ , ] [ , ] [ , ]\n0 246 0 1053856\nsetosa . .\n1 326 0 1977527\nversicolor . .\n2 026 0 2746501\nvirginica . .\nWe then call the prediction program to predict a single case, or to con-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 195,
      "chunk_index": 0
    }
  },
  {
    "text": "struct the “confusion matrix” as follows. The table gives the mean and\nstandard deviation of the variables.\n> predict(res , iris [ 3 , 1 : 4 ] ,type=\"raw\")\nsetosa versicolor virginica\n1 1 2 367113 18 7 240956 26\n[ ,] . e . e\n− −\n> out = table(predict(res , iris [ , 1 : 4 ]) , iris [ , 5 ])\n> print(out)\nsetosa versicolor virginica\n50 0 0\nsetosa\n0 47 3\nversicolor\n0 3 47\nvirginica\nThis in-sample prediction can be clearly seen to have a high level of ac-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 195,
      "chunk_index": 1
    }
  },
  {
    "text": "curacy. A test of the significance of this matrix may be undertaken using\nthe chisq.test function.\nThe basic Bayes calculation takes the following form.\nPr[a F = 1] Pr[b F = 1] Pr[c F = 1] Pr[d F = 1] Pr(F = 1)\nPr[F = 1 a,b,c,d] = | · | · | · | ·\n| Pr[a,b,c,d F = 1]+Pr[a,b,c,d F = 2]+Pr[a,b,c,d F = 3]\n| | |\nwhere F is the flower type, and a,b,c,d are the four attributes. Note\n{ }\nthat we do not need to compute the denominator, as it remains the same",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 195,
      "chunk_index": 2
    }
  },
  {
    "text": "for the calculation of Pr[F = 1 a,b,c,d], Pr[F = 2 a,b,c,d], or Pr[F =\n| |",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 195,
      "chunk_index": 3
    }
  },
  {
    "text": "196 data science: theories, models, algorithms, and analytics\n3 a,b,c,d].\n|\nThere are several seminal sources detailing the Bayes classifier and its\n1996 1997 1997\napplications—see Neal ( ), Mitchell ( ), Koller and Sahami ( ),\n1998\nand Chakrabarti, Dom, Agrawal and Raghavan ( )). These models\nhave many categories and are quite complex. But they do not discern\nemotive content—but factual content—which is arguably more amenable",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 196,
      "chunk_index": 0
    }
  },
  {
    "text": "to the use of statistical techniques. In contrast, news analytics are more\ncomplicated because the data comprises opinions, not facts, which are\nusually harder to interpret.\nThe Bayes classifier uses word-based probabilities, and is thus indiffer-\nent to the structure of language. Since it is language-independent, it has\nwide applicability.\nThe approach of the Bayes classifier is to use a set of pre-classified\nmessages to infer the category of new messages. It learns from past ex-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 196,
      "chunk_index": 1
    }
  },
  {
    "text": "perience. These classifiers are extremely efficient especially when the\nnumber of categories is small, e.g., in the classification of email into\nspam versus non-spam. Here is a brief mathematical exposition of Bayes\nclassification.\nSay we have hundreds of text messages (these are not instant mes-\nsages!) that we wish to classify rapidly into a number of categories. The\ntotal number of categories or classes is denoted C, and each category is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 196,
      "chunk_index": 2
    }
  },
  {
    "text": "denoted c ,i = 1...C. Each text message is denoted m ,j = 1...M, where\ni j\nM is the total number of messages. We denote M as the total number\ni\nof messages per class i, and ∑C M = M. Words in the messages are\ni=1 i\ndenoted as (w) and are indexed by k, and the total number of words is T.\nLet n(m,w) n(m ,w ) be the total number of times word w appears\nj k k\n≡\nin message m . Notation is kept simple by suppressing subscripts as far\nj",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 196,
      "chunk_index": 3
    }
  },
  {
    "text": "j\nas possible—the reader will be able to infer this from the context. We\nmaintain a count of the number of times each word appears in every\nmessage in the training data set. This leads naturally to the variable\nn(m), the total number of words in message m including duplicates. This\nis a simple sum, n(m ) = ∑T n(m ,w ).\nj k=1 j k\nWe also keep track of the frequency with which a word appears in a\ncategory. Hence, n(c,w) is the number of times word w appears in all\nm c. This is\n∈",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 196,
      "chunk_index": 4
    }
  },
  {
    "text": "m c. This is\n∈\nn(c ,w ) = ∑ n(m ,w ) ( 7 . 6 )\ni k j k\nm c\nj∈ i\nThis defines a corresponding probability: θ(c ,w ) is the probability with\ni k",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 196,
      "chunk_index": 5
    }
  },
  {
    "text": "more than words: extracting information from news 197\nwhich word w appears in all messages m in class c:\nθ(c,w) = ∑ m j∈ c i n(m j ,w k ) = n(c i ,w k ) ( 7 . 7 )\n∑ ∑ n(m ,w ) n(c )\nm c k j k i\nj∈ i\nEvery word must have some non-zero probability of occurrence, no mat-\nter how small, i.e., θ(c ,w ) = 0, c ,w . Hence, an adjustment is made to\ni k i k\n(cid:54) ∀\n77\nequation ( . ) via Laplace’s formula which is\nn(c ,w )+1\nθ(c ,w ) = i k\ni k n(c )+T\ni",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 197,
      "chunk_index": 0
    }
  },
  {
    "text": "n(c ,w )+1\nθ(c ,w ) = i k\ni k n(c )+T\ni\nThis probability θ(c ,w ) is unbiased and efficient. If n(c ,w ) = 0 and\ni k i k\nn(c ) = 0, k, then every word is equiprobable, i.e., 1. We now have\ni ∀ T\nthe required variables to compute the conditional probability of a text\nmessage j in category i, i.e. Pr[m c ]:\nj i\n|\n(cid:32) (cid:33)\nPr[m c ] =\nn(m\nj\n)\n∏\nT\nθ(c ,w )n(m j,w k )\nj i i k\n| n(m ,w )\n{ j k } k=1\nn(m )! T\n= j ∏ θ(c ,w )n(m j,w k )\nn(m ,w )! n(m ,w )! ... n(m ,w )! × i k",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 197,
      "chunk_index": 1
    }
  },
  {
    "text": "n(m ,w )! n(m ,w )! ... n(m ,w )! × i k\nj 1 × j 2 × × j T k=1\nPr[c ] is the proportion of messages in the prior (training corpus) pre-\ni\nclassified into class c . (Warning: Careful computer implementation of\ni\nthe multinomial probability above is required to avoid rounding error.)\nThe classification goal is to compute the most probable class c given\ni\nany message m . Therefore, using the previously computed values of\nj\nPr[m c ] and Pr[c ], we obtain the following conditional probability (ap-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 197,
      "chunk_index": 2
    }
  },
  {
    "text": "j i i\n|\nplying Bayes’ theorem):\nPr[m c ].Pr[c ]\nPr[c m ] = j | i i ( 7 . 8 )\ni | j ∑C Pr[m c ].Pr[c ]\ni=1 j | i i\n78\nFor each message, equation ( . ) delivers posterior probabilities,\nPr[c m ], i, one for each message category. The category with the high-\ni j\n| ∀\nest probability is assigned to the message.\nThe Bayesian classifier requires no optimization and is computable in\ndeterministic time. It is widely used in practice. There are free off-the-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 197,
      "chunk_index": 3
    }
  },
  {
    "text": "shelf programs that provide good software to run the Bayes classifier on\nlarge data sets. The one that is very widely used in finance applications\nis the Bow classifier, developed by Andrew McCallum when he was at\nCarnegie-Mellon University. This is an very fast classifier that requires\nalmost no additional programming by the user. The user only has to",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 197,
      "chunk_index": 4
    }
  },
  {
    "text": "198 data science: theories, models, algorithms, and analytics\nset up the training data set in a simple directory structure—each text\nmessage is a separate file, and the training corpus requires different sub-\ndirectories for the categories of text. Bow offers various versions of the\n1996\nBayes classifier—see McCallum ( ). The simple (naive) Bayes classifier\ndescribed above is available in R in the e1071 package—the function is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 198,
      "chunk_index": 0
    }
  },
  {
    "text": "called naiveBayes. The e1071 package is the machine learning library in\nR. There are also several more sophisticated variants of the Bayes classi-\nfier such as k-Means, kNN, etc.\nNews analytics begin with classification, and the Bayes classifier is the\nworkhorse of any news analytic system. Prior to applying the classifier\nit is important for the user to exercise judgment in deciding what cate-\ngories the news messages will be classified into. These categories might",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 198,
      "chunk_index": 1
    }
  },
  {
    "text": "be a simple flat list, or they may even be a hierarchical set—see Koller\n1997\nand Sahami ( ).\n7.5.2 Support Vector Machines\nA support vector machine or SVM is a classifier technique that is similar\nto cluster analysis but is applicable to very high-dimensional spaces. The\nidea may be best described by thinking of every text message as a vector\nin high-dimension space, where the number of dimensions might be,\nfor example, the number of words in a dictionary. Bodies of text in the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 198,
      "chunk_index": 2
    }
  },
  {
    "text": "same category will plot in the same region of the space. Given a training\ncorpus, the SVM finds hyperplanes in the space that best separate text of\none category from another.\nFor the seminal development of this method, see Vapnik and Lerner\n1963 1964 1995\n( ); Vapnik and Chervonenkis ( ); Vapnik ( ); and Smola and\n1998\nScholkopf ( ). I provide a brief summary of the method based on\nthese works.\nConsider a training data set given by the binary relation\n(x ,y ),...,(x ,y ) X .\n1 1 n n\n{ } ⊂ ×R",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 198,
      "chunk_index": 3
    }
  },
  {
    "text": "(x ,y ),...,(x ,y ) X .\n1 1 n n\n{ } ⊂ ×R\nThe set X d is the input space and set Y m is a set of categories.\n∈ R ∈ R\nWe define a function\nf : x y\n→\nwith the idea that all elements must be mapped from set X into set Y\nwith no more than an (cid:101)-deviation. A simple linear example of such a\nmodel would be\nf(x ) =< w,x > +b, w ,b\ni i\n∈ X ∈ R",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 198,
      "chunk_index": 4
    }
  },
  {
    "text": "more than words: extracting information from news 199\nThe notation < w,x > signifies the dot product of w and x. Note that the\nequation of a hyperplane is < w,x > +b = 0.\nThe idea in SVM regression is to find the flattest w that results in the\nmapping from x y. Thus, we minimize the Euclidean norm of w, i.e.,\n(cid:113) →\nw = ∑n w2. We also want to ensure that y f(x ) (cid:101), i. The\n|| || j=1 j | i − i | ≤ ∀\nobjective function (quadratic program) becomes\n1\nmin w 2\n2|| ||\nsubject to",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 199,
      "chunk_index": 0
    }
  },
  {
    "text": "1\nmin w 2\n2|| ||\nsubject to\ny < w,x > b (cid:101)\ni i\n− − ≤\ny + < w,x > +b (cid:101)\ni i\n− ≤\nThis is a (possibly infeasible) convex optimization problem. Feasibility\nis obtainable by introducing the slack variables (ξ,ξ ). We choose a con-\n∗\nstant C that scales the degree of infeasibility. The model is then modified\nto be as follows:\nmin 1 w 2+C ∑ n (ξ+ξ ∗ )\n2|| ||\ni=1\nsubject to\ny < w,x > b (cid:101)+ξ\ni i\n− − ≤\ny\ni\n+ < w,x\ni\n> +b (cid:101)+ξ\n∗\n− ≤\nξ,ξ ∗ 0\n≥",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 199,
      "chunk_index": 1
    }
  },
  {
    "text": "y\ni\n+ < w,x\ni\n> +b (cid:101)+ξ\n∗\n− ≤\nξ,ξ ∗ 0\n≥\nAs C increases, the model increases in sensitivity to infeasibility.\nWe may tune the objective function by introducing cost functions\nc(.),c (.). Then, the objective function becomes\n∗\nmin 1 w 2+C ∑ n [c(ξ)+c ∗ (ξ ∗ )]\n2|| ||\ni=1\nWe may replace the function [f(x) y] with a “kernel” K(x,y) introduc-\n−\ning nonlinearity into the problem. The choice of the kernel is a matter of\njudgment, based on the nature of the application being examined. SVMs",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 199,
      "chunk_index": 2
    }
  },
  {
    "text": "allow many different estimation kernels, e.g., the Radial Basis function\nkernel minimizes the distance between inputs (x) and targets (y) based\non\nf(x,y;γ) = exp( γ x y 2)\n− | − |\nwhere γ is a user-defined squashing parameter.\nThere are various SVM packages that are easily obtained in open-\nsource. An easy-to-use one is SVM Light—the package is available at",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 199,
      "chunk_index": 3
    }
  },
  {
    "text": "200 data science: theories, models, algorithms, and analytics\nthe following URL: http://svmlight.joachims.org/. SVM Light is an\nimplementation of Vapnik’s Support Vector Machine for the problem of\npattern recognition. The algorithm has scalable memory requirements\nand can handle problems with many thousands of support vectors effi-\nciently. The algorithm proceeds by solving a sequence of optimization\nproblems, lower-bounding the solution using a form of local search. It is\n1999",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 200,
      "chunk_index": 0
    }
  },
  {
    "text": "1999\nbased on work by Joachims ( ).\nAnother program is the University of London SVM. Interestingly, it\nis known as SVM Dark—evidently people who like hyperplanes have a\nsense of humor! See http://www.cs.ucl.ac.uk/staff/M.Sewell/svmdark/.\nFor a nice list of SVMs, see http://www.cs.ubc.ca/ murphyk/Software/svm.htm.\n∼\nIn R, see the machine learning library e1071—the function is, of course,\ncalled svm.\nAs an example, let’s use the svm function to analyze the same flower",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 200,
      "chunk_index": 1
    }
  },
  {
    "text": "data set that we used with naive Bayes.\n#USING SVMs\n1 4 5\n> res = svm( iris [ , : ] , iris [ , ])\n> out = table(predict(res , iris [ , 1 : 4 ]) , iris [ , 5 ])\n> print(out)\nsetosa versicolor virginica\n50 0 0\nsetosa\n0 48 2\nversicolor\n0 2 48\nvirginica\nSVMs are very fast and are quite generally applicable with many\ntypes of kernels. Hence, they may also be widely applied in news an-\nalytics.\n7.5.3 Word Count Classifiers\nThe simplest form of classifier is based on counting words that are of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 200,
      "chunk_index": 2
    }
  },
  {
    "text": "signed type. Words are the heart of any language inference system,\nand in a specialized domain, this is even more so. In the words of F.C.\nBartlett,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 200,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 201\n“Words ... can indicate the qualitative and relational features of a situ-\nation in their general aspect just as directly as, and perhaps even more\nsatisfactorily than, they can describe its particular individuality, This\nis, in fact, what gives to language its intimate relation to thought pro-\ncesses.”\nTo build a word-count classifier a user defines a lexicon of special",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 201,
      "chunk_index": 0
    }
  },
  {
    "text": "words that relate to the classification problem. For example, if the clas-\nsifier is categorizing text into optimistic versus pessimistic economic\nnews, then the user may want to create a lexicon of words that are use-\nful in separating the good news from bad. For example, the word “up-\nbeat” might be signed as optimistic, and the word “dismal” may be pes-\n300 500\nsimistic. In my experience, a good lexicon needs about – words.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 201,
      "chunk_index": 1
    }
  },
  {
    "text": "Domain knowledge is brought to bear in designing a lexicon. Therefore,\nin contrast to the Bayes classifier, a word-count algorithm is language-\ndependent.\nThis algorithm is based on a simple word count of lexical words. If\nthe number of words in a particular category exceeds that of the other\ncategories by some threshold then the text message is categorized to the\ncategory with the highest lexical count. The algorithm is of very low",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 201,
      "chunk_index": 2
    }
  },
  {
    "text": "complexity, extremely fast, and easy to implement. It delivers a baseline\napproach to the classification problem.\n7.5.4 Vector Distance Classifier\nThis algorithm treats each message as a word vector. Therefore, each\npre-classified, hand-tagged text message in the training corpus becomes\na comparison vector—we call this set the rule set. Each message in the\ntest set is then compared to the rule set and is assigned a classification\nbased on which rule comes closest in vector space.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 201,
      "chunk_index": 3
    }
  },
  {
    "text": "The angle between the message vector (M) and the vectors in the rule\nset (S) provides a measure of proximity.\nM S\ncos(θ) = ·\nM S\n|| ||·|| ||\nwhere A denotes the norm of vector A. Variations on this theme are\n|| ||\nmade possible by using sets of top-n closest rules, rather than only the\nclosest rule.\nWord vectors here are extremely sparse, and the algorithms may be\nbuilt to take the dot product and norm above very rapidly. This algo-\n2007",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 201,
      "chunk_index": 4
    }
  },
  {
    "text": "2007\nrithm was used in Das and Chen ( ) and was taken directly from",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 201,
      "chunk_index": 5
    }
  },
  {
    "text": "202 data science: theories, models, algorithms, and analytics\nideas used by search engines. The analogy is almost exact. A search en-\ngine essentially indexes pages by representing the text as a word vector.\nWhen a search query is presented, the vector distance cos(θ) (0,1) is\n∈\ncomputed for the search query with all indexed pages to find the pages\nwith which the angle is the least, i.e., where cos(θ) is the greatest. Sort-\ning all indexed pages by their angle with the search query delivers the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 202,
      "chunk_index": 0
    }
  },
  {
    "text": "best-match ordered list. Readers will remember in the early days of\nsearch engines how the list of search responses also provided a percent-\nage number along with the returned results—these numbers were the\nsame as the value of cos(θ).\nWhen using the vector distance classifier for news analytics, the classi-\nfication algorithm takes the new text sample and computes the angle of\nthe message with all the text pages in the indexes training corpus to find",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 202,
      "chunk_index": 1
    }
  },
  {
    "text": "the best matches. It then classifies pages with the same tag as the best\nmatches. This classifier is also very easy to implement as it only needs\nsimple linear algebra functions and sorting routines that are widely\navailable in almost any programming environment.\n7.5.5 Discriminant-Based Classifier\nAll the classifiers discussed above do not weight words differentially\nin a continuous manner. Either they do not weight them at all, as in",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 202,
      "chunk_index": 2
    }
  },
  {
    "text": "the case of the Bayes classifier or the SVM, or they focus on only some\nwords, ignoring the rest, as with the word count classifier. In contrast the\ndiscriminant-based classifier weights words based on their discriminant\nvalue.\nThe commonly used tool here is Fisher’s discriminant. Various imple-\nmentations of it, with minor changes in form are used. In the classifi-\ncation area, one of the earliest uses was in the Bow algorithm of McCal-\n1996",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 202,
      "chunk_index": 3
    }
  },
  {
    "text": "1996\nlum ( ), which reports the discriminant values; Chakrabarti, Dom,\n1998\nAgrawal and Raghavan ( ) also use it in their classification frame-\n2007\nwork, as do Das and Chen ( ). We present one version of Fisher’s\ndiscriminant here.\nLet the mean score (average number of times word w appears in a text\nmessage of category i) of each term for each category = µ , where i in-\ni\ndexes category. Let text messages be indexed by j. The number of times",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 202,
      "chunk_index": 4
    }
  },
  {
    "text": "word w appears in a message j of category i is denoted m . Let n be the\nij i\nnumber of times word w appears in category i. Then the discriminant",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 202,
      "chunk_index": 5
    }
  },
  {
    "text": "more than words: extracting information from news 203\nfunction might be expressed as:\n1 ∑ (µ µ )2\nF(w) = | C | i (cid:54) =k i − k\n∑ 1 ∑ (m µ )2\ni n i j ij − i\nIt is the ratio of the across-class (class i vs class k) variance to the average\nof within-class (class i C) variances. To get some intuition, consider\n∈\nthe case we looked at earlier, classifying the economic sentiment as op-\ntimistic or pessimistic. If the word “dismal” appears exactly once in text",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 203,
      "chunk_index": 0
    }
  },
  {
    "text": "that is pessimistic and never appears in text that is optimistic, then the\nwithin-class variation is zero, and the across-class variation is positive.\nIn such a case, where the denominator of the equation above is zero, the\nword “dismal” is an infinitely-powerful discriminant. It should be given\na very large weight in any word-count algorithm.\n2007\nIn Das and Chen ( ) we looked at stock message-board text and\ndetermined good discriminants using the Fisher metric. Here are some",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 203,
      "chunk_index": 1
    }
  },
  {
    "text": "words that showed high discriminant values (with values alongside) in\nclassifying optimistic versus pessimistic opinions.\nbad 0.0405\nhot 0.0161\nhype 0.0089\nimprove 0.0123\njoke 0.0268\njump 0.0106\nkilled 0.0160\nlead 0.0037\nlike 0.0037\nlong 0.0162\nlose 0.1211\nmoney 0.1537\novervalue 0.0160\nown 0.0031\ngood__n 0.0485\nThe last word in the list (“not good”) is an example of a negated word\nshowing a higher discriminant value than the word itself without a neg-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 203,
      "chunk_index": 2
    }
  },
  {
    "text": "ative connotation (recall the discussion of negative tagging earlier in Sec-\n732 00405\ntion . . ). Also see that the word “bad” has a score of . , whereas\n00485\nthe term “not good” has a higher score of . . This is an example",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 203,
      "chunk_index": 3
    }
  },
  {
    "text": "204 data science: theories, models, algorithms, and analytics\nwhere the structure and usage of language, not just the meaning of a\nword, matters.\nIn another example, using the Bow algorithm this time, examining\n20\na database of conference calls with analysts, the best discriminant\nwords were:\n0.030828516377649325 allowing\n0.094412331406551059 november\n0.044315992292870907 determined\n0.225433526011560692 general\n0.034682080924855488 seasonality\n0.123314065510597301 expanded",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 204,
      "chunk_index": 0
    }
  },
  {
    "text": "0.123314065510597301 expanded\n0.017341040462427744 rely\n0.071290944123314062 counsel\n0.044315992292870907 told\n0.015414258188824663 easier\n0.050096339113680152 drop\n0.028901734104046242 synergies\n0.025048169556840076 piece\n0.021194605009633910 expenditure\n0.017341040462427744 requirement\n0.090558766859344900 prospects\n0.019267822736030827 internationally\n0.017341040462427744 proper\n0.026974951830443159 derived\n0.001926782273603083 invited",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 204,
      "chunk_index": 1
    }
  },
  {
    "text": "0.001926782273603083 invited\nNot all these words would obviously connote bullishness or bearishness,\nbut some of them certainly do, such as “expanded”, “drop”, “prospects”,\netc. Why apparently unrelated words appear as good discriminants is\nuseful to investigate, and may lead to additional insights.\n7.5.6 Adjective-Adverb Classifier\nClassifiers may use all the text, as in the Bayes and vector-distance clas-\nsifiers, or a subset of the text, as in the word-count algorithm. They may",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 204,
      "chunk_index": 2
    }
  },
  {
    "text": "also weight words differentially as in discriminant-based word counts.\nAnother way to filter words in a word-count algorithm is to focus on\nthe segments of text that have high emphasis, i.e., in regions around\n2007\nadjectives and adverbs. This is done in Das and Chen ( ) using an",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 204,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 205\nadjective-adverb search to determine these regions.\nThis algorithm is language-dependent. In order to determine the ad-\njectives and adverbs in the text, parsing is required, and calls for the\nuse of a dictionary. The one I have used extensively is the CUVOALD\n((Computer Usable Version of the Oxford Advanced Learnerï£¡s Dic-\ntionary). It contains parts-of-speech tagging information, and makes",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 205,
      "chunk_index": 0
    }
  },
  {
    "text": "the parsing process very simple. There are other sources—a very well-\nknown one is WordNet from http://wordnet.princeton.edu/.\nUsing these dictionaries, it is easy to build programs that only extract\nthe regions of text around adjectives and adverbs, and then submit these\nto the other classifiers for analysis and classification. Counting adjectives\nand adverbs may also be used to score news text for “emphasis” thereby\nenabling a different qualitative metric of importance for the text.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 205,
      "chunk_index": 1
    }
  },
  {
    "text": "7.5.7 Scoring Optimism and Pessimism\nA very useful resource for scoring text is the General Inquirer,\nhttp://www.wjh.harvard.edu/ inquirer/, housed at Harvard Uni-\n∼\nversity. The Inquirer allows the user to assign “flavors” to words so as to\nscore text. In our case, we may be interested in counting optimistic and\npessimistic words in text. The Inquirer will do this online if needed, but\nthe dictionary may be downloaded and used offline as well. Words are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 205,
      "chunk_index": 2
    }
  },
  {
    "text": "tagged with attributes that may be easily used to undertake tagged word\ncounts.\nHere is a sample of tagged words from the dictionary that gives a\nflavor of its structure:\nABNORMAL H4Lvd Neg Ngtv Vice NEGAFF Modif |\nABOARD H4Lvd Space PREP LY |\nABOLITION Lvd TRANS Noun\nABOMINABLE H4 Neg Strng Vice Ovrst Eval IndAdj Modif |\nABORTIVE Lvd POWOTH POWTOT Modif POLIT\nABOUND H4 Pos Psv Incr IAV SUPV |\nThe words ABNORMAL and ABOMINABLE have “Neg” tags and the\nword ABOUND has a “Pos” tag.\n2007",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 205,
      "chunk_index": 3
    }
  },
  {
    "text": "word ABOUND has a “Pos” tag.\n2007\nDas and Chen ( ) used this dictionary to create an ambiguity score\nfor segmenting and filtering messages by optimism/pessimism in test-\ning news analytical algorithms. They found that algorithms performed\nbetter after filtering in less ambiguous text. This ambiguity score is dis-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 205,
      "chunk_index": 4
    }
  },
  {
    "text": "206 data science: theories, models, algorithms, and analytics\n759\ncussed later in Section . . .\n2007\nTetlock ( ) is the best example of the use of the General Inquirer\nin finance. Using text from the “Abreast of the Market\" column from the\n77\nWall Street Journal he undertook a principal components analysis of\ncategories from the GI and constructed a media pessimism score. High\npessimism presages lower stock prices, and extreme positive or negative",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 206,
      "chunk_index": 0
    }
  },
  {
    "text": "pessimism predicts volatility. Tetlock, Saar-Tsechansky and Macskassay\n2008\n( ) use news text related to firm fundamentals to show that negative\nwords are useful in predicting earnings and returns. The potential of this\ntool has yet to be fully realized, and I expect to see a lot more research\nundertaken using the General Inquirer.\n7.5.8 Voting among Classifiers\n2007\nIn Das and Chen ( ) we introduced a voting classifier. Given the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 206,
      "chunk_index": 1
    }
  },
  {
    "text": "highly ambiguous nature of the text being worked with, reducing the\n2002\nnoise is a major concern. Pang, Lee and Vaithyanathan ( ) found that\nstandard machine learning techniques do better than humans at classi-\nfication. Yet, machine learning methods such as naive Bayes, maximum\nentropy, and support vector machines do not perform as well on senti-\nment classification as on traditional topic-based categorization.\nTo mitigate error, classifiers are first separately applied, and then a",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 206,
      "chunk_index": 2
    }
  },
  {
    "text": "majority vote is taken across the classifiers to obtain the final category.\nThis approach improves the signal to noise ratio of the classification\nalgorithm.\n7.5.9 Ambiguity Filters\nSuppose we are building a sentiment index from a news feed. As each\ntext message comes in, we apply our algorithms to it and the result is\na classification tag. Some messages may be classified very accurately,\nand others with much lower levels of confidence. Ambiguity-filtering is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 206,
      "chunk_index": 3
    }
  },
  {
    "text": "a process by which we discard messages of high noise and potentially\nlow signal value from inclusion in the aggregate signal (for example, the\nsentiment index).\nOne may think of ambiguity-filtering as a sequential voting scheme.\nInstead of running all classifiers and then looking for a majority vote, we\nrun them sequentially, and discard messages that do not pass the hurdle\nof more general classifiers, before subjecting them to more particular",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 206,
      "chunk_index": 4
    }
  },
  {
    "text": "more than words: extracting information from news 207\nones. In the end, we still have a voting scheme. Ambiguity metrics are\ntherefore lexicographic.\n2007\nIn Das and Chen ( ) we developed an ambiguity filter for appli-\ncation prior to our classification algorithms. We applied the General\nInquirer to the training data to determine an “optimism” score. We com-\nputed this for each category of stock message type, i.e., buy, hold, and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 207,
      "chunk_index": 0
    }
  },
  {
    "text": "sell. For each type, we computed the mean optimism score, amounting\n0032 0026 0016\nto . , . , . , respectively, resulting in the expected rank order-\n0075 0069 0071\ning (the standard deviations around these means are . , . , . ,\nrespectively). We then filtered messages in based on how far they were\naway from the mean in the right direction. For example, for buy mes-\nsages, we chose for classification only those with one standard-deviation",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 207,
      "chunk_index": 1
    }
  },
  {
    "text": "higher than the mean. False positives in classification decline dramati-\ncally with the application of this ambiguity filter.\n7.6 Metrics\nDeveloping analytics without metrics is insufficient. It is important to\nbuild measures that examine whether the analytics are generating classi-\nfications that are statistically significant, economically useful, and stable.\nFor an analytic to be statistically valid, it should meet some criterion that",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 207,
      "chunk_index": 2
    }
  },
  {
    "text": "signifies classification accuracy and power. Being economically useful sets\na different bar—does it make money? And stability is a double-edged\nquality: one, does it perform well in-sample and out-of-sample? And\ntwo, is the behavior of the algorithm stable across training corpora?\nHere, we explore some of the metrics that have been developed, and\npropose others. No doubt, as the range of analytics grows, so will the\nrange of metrics.\n7.6.1 Confusion Matrix",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 207,
      "chunk_index": 3
    }
  },
  {
    "text": "range of metrics.\n7.6.1 Confusion Matrix\nThe confusion matrix is the classic tool for assessing classification accu-\nracy. Given n categories, the matrix is of dimension n n. The rows re-\n×\nlate to the category assigned by the analytic algorithm and the columns\nrefer to the correct category in which the text resides. Each cell (i,j) of\nthe matrix contains the number of text messages that were of type j and\nwere classified as type i. The cells on the diagonal of the confusion ma-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 207,
      "chunk_index": 4
    }
  },
  {
    "text": "trix state the number of times the algorithm got the classification right.\nAll other cells are instances of classification error. If an algorithm has no",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 207,
      "chunk_index": 5
    }
  },
  {
    "text": "208 data science: theories, models, algorithms, and analytics\nclassification ability, then the rows and columns of the matrix will be in-\ndependent of each other. Under this null hypothesis, the statistic that is\nexamined for rejection is as follows:\nχ2[dof = (n 1)2] = ∑ n ∑ n [A(i,j) − E(i,j)]2\n− E(i,j)\ni=1j=1\nwhere A(i,j) are the actual numbers observed in the confusion matrix,\nand E(i,j) are the expected numbers, assuming no classification ability",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 208,
      "chunk_index": 0
    }
  },
  {
    "text": "under the null. If T(i) represents the total across row i of the confusion\nmatrix, and T(j) the column total, then\nT(i) T(j) T(i) T(j)\nE(i,j) = × ×\n∑n T(i) ≡ ∑n T(j)\ni=1 j=1\nThe degrees of freedom of the χ2 statistic is (n 1)2. This statistic is very\n−\neasy to implement and may be applied to models for any n. A highly\nsignificant statistic is evidence of classification ability.\n7.6.2 Precision and Recall\nThe creation of the confusion matrix leads naturally to two measures",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 208,
      "chunk_index": 1
    }
  },
  {
    "text": "that are associated with it.\nPrecision is the fraction of positives identified that are truly positive,\nand is also known as positive predictive value. It is a measure of useful-\nness of prediction. So if the algorithm (say) was tasked with selecting\nthose account holders on LinkedIn who are actually looking for a job,\nand it identifies n such people of which only m were really looking for a\njob, then the precision would be m/n.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 208,
      "chunk_index": 2
    }
  },
  {
    "text": "job, then the precision would be m/n.\nRecall is the proportion of positives that are correctly identified, and is\nalso known as sensitivity. It is a measure of how complete the prediction\nis. If the actual number of people looking for a job on LinkedIn was M,\nthen recall would be n/M.\nFor example, suppose we have the following confusion matrix.\nActual\nPredicted Looking for Job Not Looking\n10 2 12\nLooking for Job\n1 16 17\nNot Looking\n11 18 29",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 208,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 209\nIn this case precision is 10/12 and recall is 10/11. Precision is related\nto the probability of false positives (Type I error), which is one minus\nprecision. Recall is related to the probability of false negatives (Type II\nerror), which is one minus recall.\n7.6.3 Accuracy\nAlgorithm accuracy over a classification scheme is the percentage of text\nthat is correctly classified. This may be done in-sample or out-of-sample.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 209,
      "chunk_index": 0
    }
  },
  {
    "text": "To compute this off the confusion matrix, we calculate\n∑n A(i,i)\nAccuracy = i=1\n∑n T(j)\nj=1\nWe should hope that this is at least greater than 1/n, which is the accu-\nracy level achieved on average from random guessing. In practice, I find\n60 70\nthat accuracy ratios of – % are reasonable for text that is non-factual\nand contains poor language and opinions.\n7.6.4 False Positives\nImproper classification is worse than a failure to classify. In a 2 2 (two\n×",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 209,
      "chunk_index": 1
    }
  },
  {
    "text": "×\ncategory, n = 2) scheme, every off-diagonal element in the confusion\nmatrix is a false positive. When n > 2, some classification errors are\n3\nworse than others. For example in a –way buy, hold, sell scheme, where\nwe have stock text for classification, classifying a buy as a sell is worse\nthan classifying it as a hold. In this sense an ordering of categories is\nuseful so that a false classification into a near category is not as bad as a",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 209,
      "chunk_index": 2
    }
  },
  {
    "text": "wrong classification into a far (diametrically opposed) category.\nThe percentage of false positives is a useful metric to work with. It\nmay be calculated as a simple count or as a weighted count (by nearness\nof wrong category) of false classifications divided by total classifications\nundertaken.\n2007\nIn our experiments on stock messages in Das and Chen ( ), we\nfound that the false positive rate for the voting scheme classifier was\n10",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 209,
      "chunk_index": 3
    }
  },
  {
    "text": "10\nabout %. This was reduced to below half that number after application\n759\nof an ambiguity filter (discussed in Section . . ) based on the General\nInquirer.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 209,
      "chunk_index": 4
    }
  },
  {
    "text": "210 data science: theories, models, algorithms, and analytics\n7.6.5 Sentiment Error\nWhen many articles of text are classified, an aggregate measure of senti-\nment may be computed. Aggregation is useful because it allows classifi-\ncation errors to cancel—if a buy was mistaken as a sell, and another sell\nas a buy, then the aggregate sentiment index is unaffected.\nSentiment error is the percentage difference between the computed\naggregate sentiment, and the value we would obtain if there were no",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 210,
      "chunk_index": 0
    }
  },
  {
    "text": "5 15\nclassification error. In our experiments this varied from - % across the\n2010\ndata sets that we used. Leinweber and Sisk ( ) show that sentiment\naggregation gives a better relation between news and stock returns.\n7.6.6 Disagreement\n2005\nIn Das, Martinez-Jerez and Tufano ( ) we introduced a disagreement\nmetric that allows us to gauge the level of conflict in the discussion.\nLooking at stock text messages, we used the number of signed buys",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 210,
      "chunk_index": 1
    }
  },
  {
    "text": "and sells in the day (based on a sentiment model) to determine how\nmuch disagreement of opinion there was in the market. The metric is\ncomputed as follows:\n(cid:12) (cid:12) (cid:12)(cid:12)\n(cid:12) (cid:12)B S(cid:12)(cid:12)\nDISAG = (cid:12)1 (cid:12) − (cid:12)(cid:12)\n(cid:12) −(cid:12)B+S(cid:12)(cid:12)\nwhere B,S are the numbers of classified buys and sells. Note that DISAG\nis bounded between zero and one. The quality of aggregate sentiment\ntends to be lower when DISAG is high.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 210,
      "chunk_index": 2
    }
  },
  {
    "text": "tends to be lower when DISAG is high.\n7.6.7 Correlations\nA natural question that arises when examining streaming news is: how\nwell does the sentiment from news correlate with financial time series?\nIs there predictability? An excellent discussion of these matters is pro-\n2010\nvided in Leinweber and Sisk ( ). They specifically examine invest-\nment signals derived from news.\nIn their paper, they show that there is a significant difference in cu-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 210,
      "chunk_index": 3
    }
  },
  {
    "text": "mulative excess returns between strong positive sentiment and strong\nnegative sentiment days over prediction horizons of a week or a quar-\nter. Hence, these event studies are based on point-in-time correlation\ntriggers. Their results are robust across countries.\nThe simplest correlation metrics are visual. In a trading day, we may\nplot the movement of a stock series, alongside the cumulative sentiment",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 210,
      "chunk_index": 4
    }
  },
  {
    "text": "more than words: extracting information from news 211\nseries. The latter is generated by taking all classified ‘buys’ as +1 and\n‘sells’ as 1, and the plot comprises the cumulative total of scores of the\n−\nmessages (‘hold’ classified messages are scored with value zero). See\n78\nFigure . for one example, where it is easy to see that the sentiment and\nstock series track each other quite closely. We coin the term “sents” for\nthe units of sentiment.\nFigure7.8: Plotofstockseries(up-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 211,
      "chunk_index": 0
    }
  },
  {
    "text": "Figure7.8: Plotofstockseries(up-\npergraph)versussentimentseries\n(lowergraph). Thecorrelation\nbetweentheseriesishigh. The\nplotisbasedonmessagesfrom\nYahoo! Financeandisforasingle\ntwenty-fourhourperiod.\n7.6.8 Aggregation Performance\n2010\nAs pointed out in Leinweber and Sisk ( ) aggregation of classified\nnews reduces noise and improves signal accuracy. One way to measure\nthis is to look at the correlations of sentiment and stocks for aggregated",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 211,
      "chunk_index": 1
    }
  },
  {
    "text": "versus disaggregated data. As an example, I examine daily sentiment\nfor individual stocks and an index created by aggregating sentiment\nacross stocks, i.e., a cross-section of sentiment. This is useful to examine",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 211,
      "chunk_index": 2
    }
  },
  {
    "text": "212 data science: theories, models, algorithms, and analytics\nwhether sentiment aggregates effectively in the cross-section.\n35\nI used all messages posted for stocks that comprise the Morgan\n35 1 27\nStanley High-Tech Index (MSH ) for the period June to August ,\n2001 88 397625\n. This results in calendar days and , messages, an average\n4500\nof about , messages per day. For each day I determine the sentiment\n4\nand stock return. Daily sentiment uses messages up to pm on each",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 212,
      "chunk_index": 0
    }
  },
  {
    "text": "trading day, coinciding with the stock return close.\nTicker STKRET(t C ) orre S l T at K io R n E s T o (t f + S 1 E ) NTY4pm(t)w S it T h KRET(t-1) Table7.1: CorrelationsofSentiment\nADP 0.086 0.138 -0.062 andStockReturnsfortheMSH35\nA\nA\nM\nM\nA\nZN\nT -0\n0\n.\n.\n0\n2\n0\n2\n8\n7\n-\n0\n0.\n.\n0\n1\n4\n6\n9\n7\n0\n0\n.\n.\n0\n1\n6\n6\n7\n1\nstocksandtheaggregatedMSH35\nAOL 0.386 -0.010 0.281 index. Stockreturns(STKRET)are\nBRCM 0.056 0.167 -0.007\nCA 0.023 0.127 0.035 computedfromclose-to-close. We",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 212,
      "chunk_index": 1
    }
  },
  {
    "text": "CPQ 0.260 0.161 0.239 computecorrelationsusingdata\nC\nD\nS\nE\nC\nL\nO\nL\n0\n0\n.\n.\n1\n4\n1\n9\n7\n3 -\n0\n0\n.\n.\n0\n0\n7\n2\n4\n4\n-\n0\n0.\n.\n0\n0\n2\n1\n5\n1\nfor88daysinthemonthsofJune,\nEDS -0.017 0.000 -0.078 JulyandAugust2001. Returndata\nEMC 0.111 0.010 0.193\nERTS 0.114 -0.223 0.225 overtheweekendislinearlyinter-\nHWP 0.315 -0.097 -0.114 polated,asmessagescontinueto\nIBM 0.071 -0.057 0.146\nINTC 0.128 -0.077 -0.007 bepostedoverweekends. Daily\nINTU -0.124 -0.099 -0.117 sentimentiscomputedfrommid-\nJ\nJ\nD\nN\nS\nP\nU\nR\n0\n0\n.\n.\n1\n4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 212,
      "chunk_index": 2
    }
  },
  {
    "text": "J\nJ\nD\nN\nS\nP\nU\nR\n0\n0\n.\n.\n1\n4\n2\n1\n6\n6\n0\n0\n.\n.\n0\n0\n5\n9\n6\n0 -\n0\n0\n.\n.\n0\n1\n4\n3\n7\n7\nnighttocloseoftradingat4pm\nLU 0.602 0.131 -0.027 (SENTY4pm).\nMOT -0.041 -0.014 -0.006\nMSFT 0.422 0.084 0.210\nMU 0.110 -0.087 0.030\nNT 0.320 0.068 0.288\nORCL 0.005 0.056 -0.062\nPALM 0.509 0.156 0.085\nPMTC 0.080 0.005 -0.030\nPSFT 0.244 -0.094 0.270\nSCMR 0.240 0.197 0.060\nSLR -0.077 -0.054 -0.158\nSTM -0.010 -0.062 0.161\nSUNW 0.463 0.176 0.276\nTLAB 0.225 0.250 0.283\nTXN 0.240 -0.052 0.117\nXLNX 0.261 -0.051 -0.217",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 212,
      "chunk_index": 3
    }
  },
  {
    "text": "TXN 0.240 -0.052 0.117\nXLNX 0.261 -0.051 -0.217\nYHOO 0.202 -0.038 0.222\nAveragecorrelationacross35stocks\n0.188 0.029 0.067\nCorrelationbetween35stockindexand35stocksentimentindex\n0.486 0.178 0.288\n35\nI also compute the average sentiment index of all stocks, i.e., a\n35\nproxy for the MSH sentiment. The corresponding equally weighted\n35\nreturn of stocks is also computed. These two time series permit an\nexamination of the relationship between sentiment and stock returns at\n71",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 212,
      "chunk_index": 4
    }
  },
  {
    "text": "71\nthe aggregate index level. Table . presents the correlations between\n35\nindividual stock returns and sentiment, and between the MSH index\n35\nreturn and MSH sentiment. We notice that there is positive contem-\nporaneous correlation between most stock returns and sentiment. The\n060 051\ncorrelations were sometimes as high as . (for Lucent), . (PALM)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 212,
      "chunk_index": 5
    }
  },
  {
    "text": "more than words: extracting information from news 213\n049\nand . (DELL). Only six stocks evidenced negative correlations, mostly\n0188\nsmall in magnitude. The average contemporaneous correlation is . ,\nwhich suggests that sentiment tracks stock returns in the high-tech sec-\ntor. (I also used full-day sentiment instead of only that till trading close\nand the results are almost the same—the correlations are in fact higher,\nas sentiment includes reactions to trading after the close).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 213,
      "chunk_index": 0
    }
  },
  {
    "text": "Average correlations for individual stocks are weaker when one lag\n0067 0029\n( . ) or lead ( . ) of the stock return are considered. More interest-\n35\ning is the average index of sentiment for all stocks. The contempo-\nraneous correlation of this index to the equally-weighted return index\n0486\nis as high as . . Here, cross-sectional aggregation helps in eliminat-\ning some of the idiosyncratic noise, and makes the positive relation-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 213,
      "chunk_index": 1
    }
  },
  {
    "text": "ship between returns and sentiment salient. This is also reflected in the\n0288\nstrong positive correlation of sentiment to lagged stock returns ( . )\n0178\nand leading returns ( . ). I confirmed the statistical contemporaneous\nrelationship of returns to sentiment by regressing returns on sentiment\n(T-statistics in brackets):\nSTKRET(t) = 0.1791+0.3866SENTY(t), R2 = 0.24\n−\n(0.93) (5.16)\n7.6.9 Phase-Lag Metrics\nCorrelation across sentiment and return time series is a special case of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 213,
      "chunk_index": 2
    }
  },
  {
    "text": "lead-lag analysis. This may be generalized to looking for pattern correla-\n78\ntions. As may be evident from Figure . , the stock and sentiment plots\nhave patterns. In the figure they appear contemporaneous, though the\nsentiment series lags the stock series.\nA graphical approach to lead-lag analysis is to look for graph patterns\nacross two series and to examine whether we may predict the patterns\nin one time series with the other. For example, can we use the senti-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 213,
      "chunk_index": 3
    }
  },
  {
    "text": "ment series to predict the high point of the stock series, or the low point?\nIn other words, is it possible to use the sentiment data generated from\nalgorithms to pick turning points in stock series? We call this type of\ngraphical examination “phase-lag” analysis.\nA simple approach I came up with involves decomposing graphs into\n79\neight types—see Figure . . On the left side of the figure, notice that\nthere are eight patterns of graphs based on the location of four salient",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 213,
      "chunk_index": 4
    }
  },
  {
    "text": "graph features: start, end, high, and low points. There are exactly eight",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 213,
      "chunk_index": 5
    }
  },
  {
    "text": "214 data science: theories, models, algorithms, and analytics\npossible graph patterns that may be generated from all positions of these\nfour salient points. It is also very easy to write software to take any time\nseries—say, for a trading day—and assign it to one of the patterns, keep-\ning track of the position of the maximum and minimum points. It is then\npossible to compare two graphs to see which one predicts the other in\nterms of pattern. For example, does the sentiment series maximum come",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 214,
      "chunk_index": 0
    }
  },
  {
    "text": "before that of the stock series? If so, how much earlier does it detect the\nturning point on average?\nUsing data from several stocks I examined whether the sentiment\ngraph pattern generated from a voting classification algorithm was pre-\ndictive of stock graph patterns. Phase-lags were examined in intervals of\nfive minutes through the trading day. The histogram of leads and lags is\n79\nshown on the right-hand side of Figure . . A positive value denotes that",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 214,
      "chunk_index": 1
    }
  },
  {
    "text": "the sentiment series lags the stock series; a negative value signifies that\nthe stock series lags sentiment. It is apparent from the histogram that the\nsentiment series lags stocks, and is not predictive of stock movements in\nthis case.\nFigure7.9: Phase-laganalysis. The\nleft-sideshowstheeightcanonical\nPhase-Lag graphpatternsthatarederived\nfromarrangementsofthestart,\nAnalysis\nend,high,andlowpointsofatime\nseries. Theright-sideshowsthe\nleadsandlagsofpatternsofthe\nstockseriesversusthesentiment",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 214,
      "chunk_index": 2
    }
  },
  {
    "text": "stockseriesversusthesentiment\nseries. Apositivevaluemeansthat\nthestockseriesleadsthesentiment\nseries.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 214,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 215\n7.6.10 Economic Significance\nNews analytics may be evaluated using economic yardsticks. Does the\nalgorithm deliver profitable opportunities? Does it help reduce risk?\n2005\nFor example, in Das and Sisk ( ) we formed a network with con-\nnections based on commonality of handles in online discussion. We de-\ntected communities using a simple rule based on connectedness beyond\na chosen threshold level, and separated all stock nodes into either one",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 215,
      "chunk_index": 0
    }
  },
  {
    "text": "giant community or into a community of individual singleton nodes. We\nthen examined the properties of portfolios formed from the community\nversus those formed from the singleton stocks.\nWe obtained several insights. We calculated the mean returns from\nan equally-weighted portfolio of the community stocks and an equally-\nweighted portfolio of singleton stocks. We also calculated the return\nstandard deviations of these portfolios. We did this month-by-month for",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 215,
      "chunk_index": 1
    }
  },
  {
    "text": "sixteen months. In fifteen of the sixteen months the mean returns were\nhigher for the community portfolio; the standard deviations were lower\nin thirteen of the sixteen months. The difference of means was significant\nfor thirteen of those months as well. Hence, community detection based\non news traffic leads to identifying a set of stocks that performs vastly\nbetter than the rest.\nThere is much more to be done in this domain of economic metrics\n2010",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 215,
      "chunk_index": 2
    }
  },
  {
    "text": "2010\nfor the performance of news analytics. Leinweber and Sisk ( ) have\nshown that there is exploitable alpha in news streams. The risk manage-\nment and credit analysis areas also offer economic metrics that may be\nused to validate news analytics.\n7.7 Grading Text\nIn recent years, the SAT exams added a new essay section. While the\ntest aimed at assessing original writing, it also introduced automated\ngrading. A goal of the test is to assess the writing level of the student.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 215,
      "chunk_index": 3
    }
  },
  {
    "text": "This is associated with the notion of readability.\n“Readability” is a metric of how easy it is to comprehend text. Given\na goal of efficient markets, regulators want to foster transparency by\nmaking sure financial documents that are disseminated to the investing\npublic are readable. Hence, metrics for readability are very important\nand are recently gaining traction.\n1952\nGunning ( ) developed the Fog index. The index estimates the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 215,
      "chunk_index": 4
    }
  },
  {
    "text": "216 data science: theories, models, algorithms, and analytics\nyears of formal education needed to understand text on a first reading.\n12\nA fog index of requires the reading level of a U.S. high school senior\n18\n(around years old). The index is based on the idea that poor read-\nability is associated with longer sentences and complex words. Complex\nwords are those that have more than two syllables. The formula for the\nFog index is\n(cid:20) (cid:18) (cid:19)(cid:21)\n#words #complex words\n0.4 +100",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 216,
      "chunk_index": 0
    }
  },
  {
    "text": "#words #complex words\n0.4 +100\n· #sentences · #words\nAlternative readability scores use similar ideas. The Flesch Reading\nEase Score and the Flesch-Kincaid Grade Level also use counts of words,\n2\nsyllables, and sentences. The Flesch Reading Ease Score is defined as 2Seehttp://en.wikipedia.org/wiki/\n(cid:18) (cid:19) (cid:18) (cid:19)\nFlesch-Kincaid_readability_tests.\n#words #syllables\n206.835 1.015 84.6\n− #sentences − #words\n90 100 11 60 70",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 216,
      "chunk_index": 1
    }
  },
  {
    "text": "− #sentences − #words\n90 100 11 60 70\nWith a range of - easily accessible by a -year old, - being\n13 15 0 30\neasy to understand for - year olds, and - for university graduates.\nThe Flesch-Kincaid Grade Level is defined as\n(cid:18) (cid:19) (cid:18) (cid:19)\n#words #syllables\n0.39 +11.8 15.59\n#sentences #words −\nwhich gives a number that corresponds to the grade level. As expected\nthese two measures are negatively correlated. Various other measures of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 216,
      "chunk_index": 2
    }
  },
  {
    "text": "readability use the same ideas as in the Fog index. For example the Cole-\n1975\nman and Liau ( ) index does not even require a count of syllables, as\nfollows:\nCLI = 0.0588L 0.296S 15.8\n− −\nwhere L is the average number of letters per hundred words and S is the\naverage number of sentences per hundred words.\nStandard readability metrics may not work well for financial text.\n2014\nLoughran and McDonald ( ) find that the Fog index is inferior to\n10\nsimply looking at -K file size.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 216,
      "chunk_index": 3
    }
  },
  {
    "text": "10\nsimply looking at -K file size.\n7.8 Text Summarization\nIt has become fairly easy to summarize text using statistical methods.\nThe simplest form of text summarizer works on a sentence-based model\nthat sorts sentences in a document in descending order of word over-\nlap with all other sentences in the text. The re-ordering of sentences ar-\nranges the document with the sentence that has most overlap with others\nfirst, then the next, and so on.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 216,
      "chunk_index": 4
    }
  },
  {
    "text": "more than words: extracting information from news 217\nAn article D may have m sentences s ,i = 1,2,...,m, where each s is a\ni i\nset of words. We compute the pairwise overlap between sentences using\n3\nthe similarity index: 3\ns s\nJ = J(s ,s ) = | i ∩ j | = J ( 7 . 9 )\nij i j ji\ns s\ni j\n| ∪ |\nThe overlap is the ratio of the size of the intersection of the two word\nsets in sentences s and s , divided by the size of the union of the two\ni j",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 217,
      "chunk_index": 0
    }
  },
  {
    "text": "i j\nsets. The similarity score of each sentence is computed as the row sums\nof the Jaccard similarity matrix.\nm\n= ∑ J ( 7 . 10 )\ni ij\nS\nj=1\nOnce the row sums are obtained, they are sorted and the summary is the\nfirst n sentences based on the values. We can then decide how many\ni\nS\nsentences we want in the summary.\nAnother approach to using row sums is to compute centrality using\nthe Jaccard matrix J, and then pick the n sentences with the highest cen-\ntrality scores.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 217,
      "chunk_index": 1
    }
  },
  {
    "text": "trality scores.\nWe illustrate the approach with a news article from the financial mar-\n21 2014\nkets. The sample text is taken from Bloomberg on April , , at the\nfollowing URL:\nhttp://www.bloomberg.com/news/print/2014-04-21/wall-street-\nbond-dealers-whipsawed-on-bearish-treasuries-bet-1-.html. The\n4\nfull text spans pages and is presented in an appendix to this chapter.\nThis article is read using a web scraper (as seen in preceding sections),",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 217,
      "chunk_index": 2
    }
  },
  {
    "text": "and converted into a text file with a separate line for each sentence. We\ncall this file summary_text.txt and this file is then read into R and pro-\ncessed with the following parsimonious program code. We first develop\nthe summarizer function.\n# FUNCTION TO RETURN n SENTENCE SUMMARY\n# Input : array of sentences ( text )\n# Output: n most common intersecting sentences\ntext_summary = function(text , n) {\nm = length( text ) # No of sentences in input\njaccard = matrix( 0 ,m,m) #Store match index",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 217,
      "chunk_index": 3
    }
  },
  {
    "text": "jaccard = matrix( 0 ,m,m) #Store match index\nfor ( i in 1 :m) {\nfor ( j in i :m) {\na = text [ i ]; aa = unlist( strsplit (a, \" \" ))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 217,
      "chunk_index": 4
    }
  },
  {
    "text": "218 data science: theories, models, algorithms, and analytics\nb = text [ j ]; bb = unlist( strsplit (b, \" \" ))\njaccard[i , j ] = length( intersect (aa ,bb)) /\nlength(union(aa ,bb))\njaccard[j , i ] = jaccard[i , j ]\n}\n}\nsimilarity_score = rowSums(jaccard)\nres = sort( similarity_score , index. return=TRUE,\ndecreasing=TRUE)\nidx = res$ix [ 1 :n]\nsummary = text [idx]\n}\nWe now read in the data and clean it into a single text array.\nurl = \"dstext_sample. txt\" #You can put any text file or URL here",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 218,
      "chunk_index": 0
    }
  },
  {
    "text": "text = read_web_page(url ,cstem= 0 ,cstop= 0 ,ccase= 0 ,cpunc= 0 ,cflat = 1 )\nprint(length( text [[ 1 ]]))\n1 1\n[ ]\nprint( text )\n1\n[ ] \"THERE HAVE BEEN murmurings that we are now in the\n\"trough of disillusionment\" of big data , the hype around it having\nsurpassed the reality of what it can deliver . Gartner suggested that\nthe \"gravitational pull of big data is now so strong that even people\nwho havenï£¡t a clue as to what it ’s all about report that they ’re running",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 218,
      "chunk_index": 1
    }
  },
  {
    "text": "big data projects . \" Indeed , their research with business decision\nmakers suggests that organisations are struggling to get value from\nbig data. Data scientists were meant . . . . .\n. . . . .\nNow we break the text into sentences using the period as a delimiter,\nand invoking the strsplit function in the stringr package.\ntext2 = strsplit(text ,\". \" ,fixed=TRUE) #Special handling of the period.\ntext2 = text2 [[1]]\nprint(text2)\n[1] \"THERE HAVE BEEN murmurings that we are now in the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 218,
      "chunk_index": 2
    }
  },
  {
    "text": "\"trough of disillusionment\" of big data, the hype around it having\nsurpassed the reality of what it can deliver\"\n[2] \" Gartner suggested that the \"gravitational pull of big data is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 218,
      "chunk_index": 3
    }
  },
  {
    "text": "more than words: extracting information from news 219\nnow so strong that even people who haven’t a clue as to what itï£¡s\nall about report that theyï£¡re running big data projects .\" Indeed,\ntheir research with business decision makers suggests that\norganisations are struggling to get value from big data\"\n[3] \"Data scientists were meant to be the answer to this issue\"\n[4] \"Indeed, Hal Varian, Chief Economist at Google famously",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 219,
      "chunk_index": 0
    }
  },
  {
    "text": "joked that \"The sexy job in the next 10 years will be statisticians .\"\nHe was clearly right as we are now used to hearing that data\nscientists are the key to unlocking the value of big data\"\n..... .....\nWe now call the text summarization function and produce the top five\nsentences that give the most overlap to all other sentences.\nres = text_summary(text2 ,5)\nprint(res)\n[1] \" Gartner suggested that the \"gravitational pull of big data is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 219,
      "chunk_index": 1
    }
  },
  {
    "text": "now so strong that even people who haven’t a clue as to what it ’s\nall about report that they’re running big data projects .\" Indeed,\ntheir research with business decision makers suggests that\norganisations are struggling to get value from big data\"\n[2] \"The focus on the data scientist often implies a centralized\napproach to analytics and decision making; we implicitly assume\nthat a small team of highly skilled individuals can meet the needs\nof the organisation as a whole\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 219,
      "chunk_index": 2
    }
  },
  {
    "text": "of the organisation as a whole\"\n[3] \"May be we are investing too much in a relatively small number\nof individuals rather than thinking about how we can design\norganisations to help us get the most from data assets\"\n[4] \"The problem with a centralized ’IT style ’ approach is that it\n−\nignores the human side of the process of considering how people\ncreate and use information i.e\"\n[5] \"Which probably means that data scientists ’ salaries will need\nto take a hit in the process.\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 219,
      "chunk_index": 3
    }
  },
  {
    "text": "to take a hit in the process.\"\nAs we can see, this generates an effective and clear summary of an\n42\narticle that originally had sentences.\n7.9 Discussion\nThe various techniques and metrics fall into two broad categories: su-\npervised and unsupervised learning methods. Supervised models use\nwell-specified input variables to the machine-learning algorithm, which\nthen emits a classification. One may think of this as a generalized regres-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 219,
      "chunk_index": 4
    }
  },
  {
    "text": "sion model. In unsupervised learning, there are no explicit input vari-\nables but latent ones, e.g., cluster analysis. Most of the news analytics we\nexplored relate to supervised learning, such as the various classification\nalgorithms. This is well-trodden research. It is the domain of unsuper-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 219,
      "chunk_index": 5
    }
  },
  {
    "text": "220 data science: theories, models, algorithms, and analytics\nvised learning, for example, the community detection algorithms and\ncentrality computation, that have been less explored and are potentially\nareas of greatest potential going forward.\nClassifying news to generate sentiment indicators has been well\nworked out. This is epitomized in many of the papers in this book. It\nis the networks on which financial information gets transmitted that",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 220,
      "chunk_index": 0
    }
  },
  {
    "text": "have been much less studied, and where I anticipate most of the growth\nin news analytics to come from. For example, how quickly does good\nnews about a tech company proliferate to other companies? We looked\n2005\nat issues like this in Das and Sisk ( ), discussed earlier, where we as-\nsessed whether knowledge of the network might be exploited profitably.\nInformation also travels by word of mouth and these information net-\nworks are also open for much further examination—see Godes, et. al.\n2005",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 220,
      "chunk_index": 1
    }
  },
  {
    "text": "2005\n( ). Inside (not insider) information is also transmitted in venture\ncapital networks where there is evidence now that better connected\nVCs perform better than unconnected VCs, as shown by Hochberg,\n2007\nLjungqvist and Lu ( ).\nWhether news analytics reside in the broad area of AI or not is under\ndebate. The advent and success of statistical learning theory in real-\nworld applications has moved much of news analytics out of the AI",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 220,
      "chunk_index": 2
    }
  },
  {
    "text": "domain into econometrics. There is very little natural language process-\ning (NLP) involved. As future developments shift from text methods to\ncontext methods, we may see a return to the AI paradigm. I believe that\ntools such as WolframAlphaTM will be the basis of context-dependent\nnews analysis.\nNews analytics will broaden in the toolkit it encompasses. Expect to\nsee greater use of dependency networks and collaborative filtering. We",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 220,
      "chunk_index": 3
    }
  },
  {
    "text": "will also see better data visualization techniques such as community\nviews and centrality diagrams. The number of tools keeps on growing.\nFor an almost exhaustive compendium of tools see the book by Koller\n2009\n( ) titled “Probabilistic Graphical Models.”\nIn the end, news analytics are just sophisticated methods for data min-\ning. For an interesting look at the top ten algorithms in data mining, see\n2008 10\nWu, et al. ( ). This paper discusses the top data mining algorithms",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 220,
      "chunk_index": 4
    }
  },
  {
    "text": "identified by the IEEE International Conference on Data Mining (ICDM)\nin December 2006 . 4 As algorithms improve in speed, they will expand to 4Thesealgorithmsare:C4.5,k-Means,\nSVM,Apriori,EM,PageRank,Ad-\nautomated decision-making, replacing human interaction—as noticed in\naBoost,kNN,NaiveBayes,andCART.\nthe marriage of news analytics with automated trading, and eventually, a",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 220,
      "chunk_index": 5
    }
  },
  {
    "text": "more than words: extracting information from news 221\nrebirth of XHAL.\n7.10 Appendix: Sample text from Bloomberg for summariza-\ntion\nSummarization is one of the major implementations in Big Text appli-\ncations. When faced with Big Text, there are three important stages\nthrough which analytics may proceed: (a) Indexation, (b) Summariza-\n5\ntion, and (c) Inference. Automatic summarization is a program that 5http://en.wikipedia.org/wiki/Automatic_summarization.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 221,
      "chunk_index": 0
    }
  },
  {
    "text": "reduces text while keeping mostly the salient points, accounting for\nvariables such as length, writing style, and syntax. There are two ap-\nproaches: (i) Extractive methods selecting a subset of existing words,\nphrases, or sentences in the original text to form the summary. (ii) Ab-\nstractive methods build an internal semantic representation and then\nuse natural language generation techniques to create a summary that is\ncloser to what a human might generate. Such a summary might contain",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 221,
      "chunk_index": 1
    }
  },
  {
    "text": "words not explicitly present in the original.\nThe following news article was used to demonstrate text summariza-\n78\ntion for the application in Section . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 221,
      "chunk_index": 2
    }
  },
  {
    "text": "222 data science: theories, models, algorithms, and analytics\n4/21/2014 Wall Street Bond Dealers Whipsawed on Bearish Treasuries Bet - Bloomberg\nWall Street Bond Dealers Whipsawed on Bearish\nTreasuries Bet\nBy Lisa Abramowicz and Daniel Kruger - Apr 21, 2014\nBetting against U.S. government debt this year is turning out to be a fool’s errand. Just ask Wall\nStreet’s biggest bond dealers.\nWhile the losses that their economists predicted have yet to materialize, JPMorgan Chase & Co.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 222,
      "chunk_index": 0
    }
  },
  {
    "text": "(JPM), Citigroup Inc. (C) and the 20 other firms that trade with the Federal Reserve began wagering\non a Treasuries selloff last month for the first time since 2011. The strategy was upended as Fed Chair\nJanet Yellen signaled she wasn’t in a rush to lift interest rates, two weeks after suggesting the opposite\nat the bank’s March 19 meeting.\nThe surprising resilience of Treasuries has investors re-calibrating forecasts for higher borrowing costs",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 222,
      "chunk_index": 1
    }
  },
  {
    "text": "as lackluster job growth and emerging-market turmoil push yields toward 2014 lows. That’s also made\nthe business of trading bonds, once more predictable for dealers when the Fed was buying trillions of\ndollars of debt to spur the economy, less profitable as new rules limit the risks they can take with their\nown money.\n“You have an uncertain Fed, an uncertain direction of the economy and you’ve got rates moving,”",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 222,
      "chunk_index": 2
    }
  },
  {
    "text": "Mark MacQueen, a partner at Sage Advisory Services Ltd., which oversees $10 billion, said by\ntelephone from Austin, Texas. In the past, “calling the direction of the market and what you should be\ndoing in it was a lot easier than it is today, particularly for the dealers.”\nTreasuries (USGG10YR) have confounded economists who predicted 10-year yields would approach\n3.4 percent by year-end as a strengthening economy prompts the Fed to pare its unprecedented bond\nbuying.\nCaught Short",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 222,
      "chunk_index": 3
    }
  },
  {
    "text": "buying.\nCaught Short\nAfter surging to a 29-month high of 3.05 percent at the start of the year, yields on the 10-year note\nhave declined and were at 2.72 percent at 7:42 a.m. in New York.\nOne reason yields have fallen is the U.S. labor market, which has yet to show consistent improvement.\nhttp://www.bloomberg.com/news/print/2014-04-21/wall-street-bond-dealers-whipsawed-on-bearish-treasuries-bet-1-.html 1/5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 222,
      "chunk_index": 4
    }
  },
  {
    "text": "more than words: extracting information from news 223\n4/21/2014 Wall Street Bond Dealers Whipsawed on Bearish Treasuries Bet - Bloomberg\nThe world’s largest economy added fewer jobs on average in the first three months of the year than in\nthe same period in the prior two years, data compiled by Bloomberg show. At the same time, a\nslowdown in China and tensions between Russia and Ukraine boosted demand for the safest assets.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 223,
      "chunk_index": 0
    }
  },
  {
    "text": "Wall Street firms known as primary dealers are getting caught short betting against Treasuries.\nThey collectively amassed $5.2 billion of wagers in March that would profit if Treasuries fell, the first\ntime they had net short positions on government debt since September 2011, data compiled by the Fed\nshow.\n‘Some Time’\nThe practice is allowed under the Volcker Rule that limits the types of trades that banks can make with",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 223,
      "chunk_index": 1
    }
  },
  {
    "text": "their own money. The wagers may include market-making, which is the business of using the firm’s\ncapital to buy and sell securities with customers while profiting on the spread and movement in prices.\nWhile the bets initially paid off after Yellen said on March 19 that the Fed may lift its benchmark rate\nsix months after it stops buying bonds, Treasuries have since rallied as her subsequent comments\nstrengthened the view that policy makers will keep borrowing costs low to support growth.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 223,
      "chunk_index": 2
    }
  },
  {
    "text": "On March 31, Yellen highlighted inconsistencies in job data and said “considerable slack” in labor\nmarkets showed the Fed’s accommodative policies will be needed for “some time.”\nThen, in her first major speech on her policy framework as Fed chair on April 16, Yellen said it will\ntake at least two years for the U.S. economy to meet the Fed’s goals, which determine how quickly the\ncentral bank raises rates.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 223,
      "chunk_index": 3
    }
  },
  {
    "text": "central bank raises rates.\nAfter declining as much as 0.6 percent following Yellen’s March 19 comments, Treasuries have\nrecouped all their losses, index data compiled by Bank of America Merrill Lynch show.\nYield Forecasts\n“We had that big selloff and the dealers got short then, and then we turned around and the Fed says,\n‘Whoa, whoa, whoa: it’s lower for longer again,’” MacQueen said in an April 15 telephone interview.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 223,
      "chunk_index": 4
    }
  },
  {
    "text": "“The dealers are really worried here. You get really punished if you take a lot of risk.”\nEconomists and strategists around Wall Street are still anticipating that Treasuries will underperform\nas yields increase, data compiled by Bloomberg show.\nWhile they’ve ratcheted down their forecasts this year, they predict 10-year yields will increase to 3.36\npercent by the end of December. That’s more than 0.6 percentage point higher than where yields are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 223,
      "chunk_index": 5
    }
  },
  {
    "text": "http://www.bloomberg.com/news/print/2014-04-21/wall-street-bond-dealers-whipsawed-on-bearish-treasuries-bet-1-.html 2/5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 223,
      "chunk_index": 6
    }
  },
  {
    "text": "224 data science: theories, models, algorithms, and analytics\n4/21/2014 Wall Street Bond Dealers Whipsawed on Bearish Treasuries Bet - Bloomberg\ntoday.\n“My forecast is 4 percent,” said Joseph LaVorgna, chief U.S. economist at Deutsche Bank AG, a\nprimary dealer. “It may seem like it’s really aggressive but it’s really not.”\nLaVorgna, who has the highest estimate among the 66 responses in a Bloomberg survey, said stronger",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 224,
      "chunk_index": 0
    }
  },
  {
    "text": "economic data will likely cause investors to sell Treasuries as they anticipate a rate increase from the\nFed.\nHistory Lesson\nThe U.S. economy will expand 2.7 percent this year from 1.9 percent in 2013, estimates compiled by\nBloomberg show. Growth will accelerate 3 percent next year, which would be the fastest in a decade,\nbased on those forecasts.\nDealers used to rely on Treasuries to act as a hedge against their holdings of other types of debt, such",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 224,
      "chunk_index": 1
    }
  },
  {
    "text": "as corporate bonds and mortgages. That changed after the credit crisis caused the failure of Lehman\nBrothers Holdings Inc. in 2008.\nThey slashed corporate-debt inventories by 76 percent from the 2007 peak through last March as they\nsought to comply with higher capital requirements from the Basel Committee on Banking Supervision\nand stockpiled Treasuries instead.\n“Being a dealer has changed over the years, and not least because you also have new balance-sheet",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 224,
      "chunk_index": 2
    }
  },
  {
    "text": "constraints that you didn’t have before,” Ira Jersey, an interest-rate strategist at primary dealer Credit\nSuisse Group AG (CSGN), said in a telephone interview on April 14.\nAlmost Guaranteed\nWhile the Fed’s decision to inundate the U.S. economy with more than $3 trillion of cheap money\nsince 2008 by buying Treasuries and mortgaged-backed bonds bolstered profits as all fixed-income\nassets rallied, yields are now so low that banks are struggling to make money trading government\nbonds.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 224,
      "chunk_index": 3
    }
  },
  {
    "text": "bonds.\nYields on 10-year notes have remained below 3 percent since January, data compiled by Bloomberg\nshow. In two decades before the credit crisis, average yields topped 6 percent.\nAverage daily trading has also dropped to $551.3 billion in March from an average $570.2 billion in\n2007, even as the outstanding amount of Treasuries has more than doubled since the financial crisis,\naccording data from the Securities Industry and Financial Markets Association.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 224,
      "chunk_index": 4
    }
  },
  {
    "text": "http://www.bloomberg.com/news/print/2014-04-21/wall-street-bond-dealers-whipsawed-on-bearish-treasuries-bet-1-.html 3/5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 224,
      "chunk_index": 5
    }
  },
  {
    "text": "more than words: extracting information from news 225\n4/21/2014 Wall Street Bond Dealers Whipsawed on Bearish Treasuries Bet - Bloomberg\n“During the crisis, the Fed went to great pains to save primary dealers,” Christopher Whalen, banker\nand author of “Inflated: How Money and Debt Built the American Dream,” said in a telephone\ninterview. “Now, because of quantitative easing and other dynamics in the market, it’s not just\ntreacherous, it’s almost a guaranteed loss.”\nTrading Revenue",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 225,
      "chunk_index": 0
    }
  },
  {
    "text": "Trading Revenue\nThe biggest dealers are seeing their earnings suffer. In the first quarter, five of the six biggest Wall\nStreet firms reported declines in fixed-income trading revenue.\nJPMorgan, the biggest U.S. bond underwriter, had a 21 percent decrease from its fixed-income trading\nbusiness, more than estimates from Moshe Orenbuch, an analyst at Credit Suisse, and Matt Burnell of\nWells Fargo & Co.\nCitigroup, whose bond-trading results marred the New York-based bank’s two prior quarterly",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 225,
      "chunk_index": 1
    }
  },
  {
    "text": "earnings, reported a 18 percent decrease in revenue from that business. Credit Suisse, the second-\nlargest Swiss bank, had a 25 percent drop as income from rates and emerging-markets businesses fell.\nDeclines in debt-trading last year prompted the Zurich-based firm to cut more than 100 fixed-income\njobs in London and New York.\nBank Squeeze\nChief Financial Officer David Mathers said in a Feb. 6 call that Credit Suisse has “reduced the capital",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 225,
      "chunk_index": 2
    }
  },
  {
    "text": "in this business materially and we’re obviously increasing our electronic trading operations in this\narea.” Jamie Dimon, chief executive officer at JPMorgan, also emphasized the decreased role of\nhumans in the rates-trading business on an April 11 call as the New York-based bank seeks to cut\ncosts.\nAbout 49 percent of U.S. government-debt trading was executed electronically last year, from 31\npercent in 2012, a Greenwich Associates survey of institutional money managers showed. That may",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 225,
      "chunk_index": 3
    }
  },
  {
    "text": "ultimately lead banks to combine their rates businesses or scale back their roles as primary dealers as\nfirms get squeezed, said Krishna Memani, the New York-based chief investment officer of\nOppenheimerFunds Inc., which oversees $79.1 billion in fixed-income assets.\n“If capital requirements were not as onerous as they are now, maybe they could have found a way of\nmaking it work, but they aren’t as such,” he said in a telephone interview.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 225,
      "chunk_index": 4
    }
  },
  {
    "text": "To contact the reporters on this story: Lisa Abramowicz in New York at labramowicz@bloomberg.net;\nDaniel Kruger in New York at dkruger1@bloomberg.net\nTo contact the editors responsible for this story: Dave Liedtka at dliedtka@bloomberg.net Michael\nhttp://www.bloomberg.com/news/print/2014-04-21/wall-street-bond-dealers-whipsawed-on-bearish-treasuries-bet-1-.html 4/5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 225,
      "chunk_index": 5
    }
  },
  {
    "text": "8\nVirulent Products: The Bass Model\n8.1 Introduction\n1969\nThe Bass ( ) product diffusion model is a classic one in the marketing\nliterature. It has been successfully used to predict the market shares of\nvarious newly introduced products, as well as mature ones.\nThe main idea of the model is that the adoption rate of a product\ncomes from two sources:\n1\n. The propensity of consumers to adopt the product independent of\nsocial influences to do so.\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 227,
      "chunk_index": 0
    }
  },
  {
    "text": "social influences to do so.\n2\n. The additional propensity to adopt the product because others have\nadopted it. Hence, at some point in the life cycle of a good product,\nsocial contagion, i.e. the influence of the early adopters becomes suffi-\nciently strong so as to drive many others to adopt the product as well.\nIt may be going too far to think of this as a “network” effect, because\nFrank Bass did this work well before the concept of network effect was",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 227,
      "chunk_index": 1
    }
  },
  {
    "text": "introduced, but essentially that is what it is.\nThe Bass model shows how the information of the first few periods of\nsales data may be used to develop a fairly good forecast of future sales.\nOne can easily see that whereas this model came from the domain of\nmarketing, it may just as easily be used to model forecasts of cashflows\nto determine the value of a start-up company.\n8.2 Historical Examples\nThere are some classic examples from the literature of the Bass model",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 227,
      "chunk_index": 2
    }
  },
  {
    "text": "providing a very good forecast of the ramp up in product adoption as a\nfunction of the two sources described above. See for example the actual",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 227,
      "chunk_index": 3
    }
  },
  {
    "text": "Empirical Generalization: Always (Almost)\nLooks Like a Bass Curve\n228 data science: theories, models, algorithms, and analytics\nAdoption of VCR’s\n80 81\nversus predicted market growth for VCRs in the s shown in Figure . .\n82\nCorrespondingly, Figure . shows the adoption of answering machines.\nActual and Fitted Adoption VCR's\n1980-1989\n12000\n10000\n8000\n6000\n4000\n2000\n0\n80 81 82 83 84 85 86 87 88 89\nYear\n(c) Frank M. Bass (1999)\nsdnasuohT\nni\nnoitpodA\nFigure8.1: ActualversusBassmodel",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 228,
      "chunk_index": 0
    }
  },
  {
    "text": "ni\nnoitpodA\nFigure8.1: ActualversusBassmodel\npredictionsforVCRs.\nActual Adoption\nFitted Adoption\n8.3 The Basic Idea\n1969 We follow the exposition in Bass ( ).\nDefine the cumulative probability of purchase of a product from time\nzero to time t by a single individual as F(t). Then, the probability of\npurchase at time t is the density function f(t) = F (t).\n(cid:48)\nThe rate of purchase at time t, given no purchase so far, logically fol-\nlows, i.e.\nf(t)\n.\n1 F(t)\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 228,
      "chunk_index": 1
    }
  },
  {
    "text": "lows, i.e.\nf(t)\n.\n1 F(t)\n−\nModeling this is just like modeling the adoption rate of the product at a\ngiven time t.\n1969\nBass ( ) suggested that this adoption rate be defined as\nf(t)\n= p+q F(t).\n1 F(t)\n−\nwhere we may think of p as defining the independent rate of a consumer\nadopting the product, and q as the imitation rate, because it modulates\nthe impact from the cumulative intensity of adoption, F(t).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 228,
      "chunk_index": 2
    }
  },
  {
    "text": "An Empirical Generalization\nvirulent products: the bass model 229\nFigure8.2: ActualversusBassmodel\nAdoption of Answering Machines predictionsforansweringmachines.\n1982-1993t\n14000\n12000\n10000\n8000\n6000\n4000\n2000\n0\n82 83 84 85 86 87 88 89 90 91 92 93\nYear\nadoption of answering machines Fitted Adoption\n(c) Frank M. Bass (1999)\nHence, if we can find p and q for a product, we can forecast its adop-\ntion over time, and thereby generate a time path of sales. To summarize:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 229,
      "chunk_index": 0
    }
  },
  {
    "text": "• p: coefficient of innovation.\n• q: coefficient of imitation.\n8.4 Solving the Model\nWe rewrite the Bass equation:\ndF/dt\n= p+q F.\n1 F\n−\nand note that F(0) = 0.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 229,
      "chunk_index": 1
    }
  },
  {
    "text": "230 data science: theories, models, algorithms, and analytics\nThe steps in the solution are:\ndF\n= (p+qF)(1 F) ( 8 . 1 )\ndt −\ndF\n= p+(q p)F qF2 ( 8 . 2 )\ndt − −\n(cid:90) 1 (cid:90)\ndF = dt ( 8 . 3 )\np+(q p)F qF2\n− −\nln(p+qF) ln(1 F)\n− − = t+c ( 8 . 4 )\np+q 1\nt = 0 F(0) = 0 ( 8 . 5 )\n⇒\nlnp\nt = 0 c = ( 8 . 6 )\n⇒ 1 p+q\np(e(p+q)t 1)\nF(t) = − ( 8 . 7 )\npe(p+q)t+q\n1\nAn alternative approach goes as follows. First, split the integral above 1Thiswassuggestedbystudents\nMuhammadSagarwallabasedonideas",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 230,
      "chunk_index": 0
    }
  },
  {
    "text": "MuhammadSagarwallabasedonideas\ninto partial fractions.\nfromAlexeyOrlovsky.\n(cid:90) 1 (cid:90)\ndF = dt ( 8 . 8 )\n(p+qF)(1 F)\n−\nSo we write\n1 A B\n= + ( 8 . 9 )\n(p+qF)(1 F) p+qF 1 F\n− −\nA AF+ pB+qFB\n= − ( 8 . 10 )\n(p+qF)(1 F)\n−\nA+ pB+F(qB A)\n= − ( 8 . 11 )\n(p+qF)(1 F)\n−\nThis implies that\nA+ pB = 1 ( 8 . 12 )\nqB A = 0 ( 8 . 13 )\n−\nSolving we get\nA = q/(p+q) ( 8 . 14 )\nB = 1/(p+q) ( 8 . 15 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 230,
      "chunk_index": 1
    }
  },
  {
    "text": "virulent products: the bass model 231\nso that\n(cid:90) 1 (cid:90)\ndF = dt ( 8 . 16 )\n(p+qF)(1 F)\n−\n(cid:90) (cid:18) A B (cid:19)\n+ dF = t+c ( 8 . 17 )\np+qF 1 F 1\n−\n(cid:90) (cid:18) q/(p+q) 1/(p+q) (cid:19)\n+ dF = t+c ( 8 . 18 )\np+qF 1 F 1\n−\n1 1\nln(p+qF) ln(1 F) = t+c ( 8 . 19 )\np+q − p+q − 1\nln(p+qF) ln(1 F)\n− − = t+c ( 8 . 20 )\np+q 1\n84\nwhich is the same as equation ( . ).\nWe may also solve for\ndF e(p+q)t p (p+q)2\nf(t) = = ( 8 . 21 )\ndt [pe(p+q)t+q]2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 231,
      "chunk_index": 0
    }
  },
  {
    "text": "f(t) = = ( 8 . 21 )\ndt [pe(p+q)t+q]2\nTherefore, if the target market is of size m, then at each t, the adop-\ntions are simply given by m f(t).\n×\nFor example, set m = 100,000, p = 0.01 and q = 0.2. Then the adop-\n83\ntion rate is shown in Figure . .\nFigure8.3: Exampleoftheadoption\nAdoptions rate:m=100,000,p=0.01andq=0.2.\nTime (years)\n8.4.1 Symbolic math in R\nThe preceding computation may also be undertaken in R, using it’s sym-\nbolic math capability.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 231,
      "chunk_index": 1
    }
  },
  {
    "text": "232 data science: theories, models, algorithms, and analytics\n> #BASS MODEL\n> FF = expression(p*(exp((p+q)*t) 1 )/ (p*exp((p+q)*t)+q))\n−\n>\n> #Take derivative\n> ff = D(FF, \"t\")\n> print( ff )\np * (exp((p + q) * t) * (p + q)) / (p * exp((p + q) * t) + q)\n−\np * (exp((p + q) * t) 1 ) * (p * (exp((p + q) * t) * (p +\n−\nq))) / (p * exp((p + q) * t) + q)^ 2\nWe may also plot the same as follows (note the useful tt eval function\nemployed in the next section of code):\n> #PLOT",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 232,
      "chunk_index": 0
    }
  },
  {
    "text": "employed in the next section of code):\n> #PLOT\n> m= 100000 ; p= 0 . 01 ; q= 0 . 2\n>\n> t=seq( 0 , 25 , 0 . 1 )\n> fn_f = eval( ff )\n> plot(t ,fn_f*m,type=\"l\")\n83 84\nAnd this results in a plot identical to that in Figure . . See Figure . .\n0 5 10 15 20 25\n0005\n0003\n0001\nt\ndlos\nstinU\nFigure8.4: Exampleoftheadoption\nrate:m=100,000,p=0.01andq=0.2.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 232,
      "chunk_index": 1
    }
  },
  {
    "text": "virulent products: the bass model 233\n8.5 Software\nThe ordinary differential equation here may be solved using free soft-\nware. One of the widely used open-source packages is called Maxima and\ncan be downloaded from many places. A very nice one page user-guide\nis available at\nhttp://www.math.harvard.edu/computing/maxima/\nHere is the basic solution of the differential equation in Maxima:\nMaxima 5.9.0 http://maxima.sourceforge.net\nDistributed under the GNU Public License. See the file COPYING.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 233,
      "chunk_index": 0
    }
  },
  {
    "text": "Dedicated to the memory of William Schelter.\nThis is a development version of Maxima. The function bug_report()\nprovides bug reporting information.\n(C1) depends(F,t);\n(D1) [F(t)]\n(C2) diff(F,t)=(1-F)*(p+q*F);\ndF\n(D2) -- = (1 - F) (F q + p)\ndt\n(C3) ode2(%,F,t);\nLOG(F q + p) - LOG(F - 1)\n(D3) ------------------------- = t + %C\nq + p\n3\nNotice that line (D ) of the program output does not correspond to\nequation ( 8 . 4 ). This is because the function 1 needs to be approached\n1 F\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 233,
      "chunk_index": 1
    }
  },
  {
    "text": "1 F\n−\nfrom the left, not the right as the software appears to be doing. Hence,\nsolving by partial fractions results in simple integrals that Maxima will\nhandle properly.\n(%i1) integrate((q/(p+q))/(p+q*F)+(1/(p+q))/(1-F),F);\nlog(q F + p) log(1 - F)\n(%o1) ------------ - ----------\nq + p q + p\nwhich is now the exact correct solution, which we use in the model.\nAnother good tool that is free for small-scale symbolic calculations is\n85",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 233,
      "chunk_index": 2
    }
  },
  {
    "text": "85\nWolframAlpha, available at www.wolframalpha.com. See Figure . for an\nexample of the basic Bass model integral.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 233,
      "chunk_index": 3
    }
  },
  {
    "text": "234 data science: theories, models, algorithms, and analytics\nFigure8.5: ComputingtheBass\nmodelintegralusingWolframAl-\npha.\n8.6 Calibration\nHow do we get coefficients p and q? Given we have the current sales\nhistory of the product, we can use it to fit the adoption curve.\n• Sales in any period are: s(t) = m f(t).\n• Cumulative sales up to time t are: S(t) = m F(t).\nSubstituting for f(t) and F(t) in the Bass equation gives:\ns(t)/m\n= p+q S(t)/m\n1 S(t)/m\n−\nWe may rewrite this as",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 234,
      "chunk_index": 0
    }
  },
  {
    "text": "= p+q S(t)/m\n1 S(t)/m\n−\nWe may rewrite this as\ns(t) = [p+q S(t)/m][m S(t)]\n−\nTherefore,\ns(t) = β +β S(t)+β S(t)2 ( 8 . 22 )\n0 1 2\nβ = pm ( 8 . 23 )\n0\nβ = q p ( 8 . 24 )\n1\n−\nβ = q/m ( 8 . 25 )\n2\n−\n822\nEquation . may be estimated by a regression of sales against cumu-\nlative sales. Once the coefficients in the regression β ,β ,β are ob-\n0 1 2\n{ }\ntained, the equations above may be inverted to determine the values of\nm,p,q . We note that since\n{ }\nβ\nβ = q p = mβ 0 ,\n1 2\n− − − m",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 234,
      "chunk_index": 1
    }
  },
  {
    "text": "virulent products: the bass model 235\nwe obtain a quadratic equation in m:\nβ m2+β m+β = 0\n2 1 0\nSolving we have\"\n(cid:113)\nβ β2 4β β\nm = − 1 ± 1− 0 2\n2β\n1\nand then this value of m may be used to solve for\nβ\np = 0 ; q = mβ\n2\nm −\nAs an example, let’s look at the trend for iPhone sales (we store the quar-\nterly sales in a file and read it in, and then undertook the Bass model\nanalysis). The R code for this computation is as follows:\n> #USING APPLE iPHONE SALES DATA",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 235,
      "chunk_index": 0
    }
  },
  {
    "text": "> #USING APPLE iPHONE SALES DATA\n> data = read. table(\"iphone_sales . txt\" ,header=TRUE)\n> isales = data[ , 2 ]\n> cum_isales = cumsum( isales )\n> cum_isales 2 = cum_isales^ 2\n> res = lm( isales ~ cum_isales+cum_isales 2 )\n> print(summary(res ))\nCall :\nlm(formula = isales ~ cum_isales + cum_isales 2 )\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax\n14 106 2 877 1 170 2 436 20 870\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error t value Pr(>|t|)\n3 220 00 2 194 00 1 468 0 1533\n( Intercept ) . e+ . e+ . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 235,
      "chunk_index": 1
    }
  },
  {
    "text": "( Intercept ) . e+ . e+ . .\ncum_isales 1 . 216 e 01 2 . 294 e 02 5 . 301 1 . 22 e 05 ***\n− − −\ncum_isales 2 6 . 893 e 05 3 . 906 e 05 1 . 765 0 . 0885 .\n− − − −\n−−−\nSignif . codes: 0 ?***? 0 . 001 ?**? 0 . 01 ?*? 0 . 05 ?.? 0 . 1 ? ? 1\nResidual standard error : 7 . 326 on 28 degrees of freedom\nMultiple R squared: 0 . 854 , Adjusted R squared: 0 . 8436\n− −\nF statistic : 81 . 89 on 2 and 28 DF, p value: 1 . 999 e 12\n− − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 235,
      "chunk_index": 2
    }
  },
  {
    "text": "236 data science: theories, models, algorithms, and analytics\nWe now proceed to fit the model and then plot it, with actual sales\noverlaid on the forecast.\n> #FIT THE MODEL\n> m 1 = ( b[ 2 ]+sqrt(b[ 2 ]^ 2 4 *b[ 1 ]*b[ 3 ])) / ( 2 *b[ 3 ])\n− −\n> m 2 = ( b[ 2 ] sqrt(b[ 2 ]^ 2 4 *b[ 1 ]*b[ 3 ])) / ( 2 *b[ 3 ])\n− − −\n> print(c(m 1 ,m 2 ))\ncum_isales cum_isales\n26 09855 1790 23321\n. .\n−\n> m = max(m 1 ,m 2 ); print(m)\n1 1790 233\n[ ] .\n> p = b[ 1 ] /m\n> q = m*b[ 3 ]\n−\n> print(c(p,q))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 236,
      "chunk_index": 0
    }
  },
  {
    "text": "> p = b[ 1 ] /m\n> q = m*b[ 3 ]\n−\n> print(c(p,q))\n( Intercept ) cum_isales 2\n0 00179885 0 12339235\n. .\n>\n> #PLOT THE FITTED MODEL\n100\n> nqtrs =\n> t=seq( 0 ,nqtrs)\n> fn_f = eval( ff )*m\n> plot(t ,fn_f ,type=\"l\")\n> n = length( isales )\n> lines ( 1 :n, isales , col=\"red\" ,lwd= 2 ,lty= 2 )\n>\n86\nThe outcome is plotted in Figure . . Indeed, it appears that Apple is\nready to peak out in sales.\n87\nFor several other products, Figure . shows the estimated coefficients\n1969",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 236,
      "chunk_index": 1
    }
  },
  {
    "text": "1969\nreported in Table I of the original Bass ( ) paper.\n8.7 Sales Peak\nIt is easy to calculate the time at which adoptions will peak out. Differ-\nentiate f(t) with respect to t, and set the result equal to zero, i.e.\nt ∗ = argmax t f(t)\nwhich is equivalent to the solution to f (t) = 0.\n(cid:48)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 236,
      "chunk_index": 2
    }
  },
  {
    "text": "virulent products: the bass model 237\n0 20 40 60 80 100\n04\n02\n0\nApple Inc Sales\nt\n)MM(\nstinU\nylrtQ\nFigure8.6: Bassmodelforecastof\nAppleInc’squarterlysales.Thecurrent\nsalesarealsooverlaidintheplot.\nFigure8.7: Empiricaladoptionrates\nandparametersfromtheBasspaper.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 237,
      "chunk_index": 0
    }
  },
  {
    "text": "238 data science: theories, models, algorithms, and analytics\nThe calculations are simple and give\n1\nt ∗ = − ln(p/q) ( 8 . 26 )\np+q\nHence, for the values p = 0.01 and q = 0.2, we have\n1\nt ∗ = − ln(0.01/0.2) = 14.2654 years.\n0.01+0.2\n83\nIf we examine the plot in Figure . we see this to be where the graph\npeaks out.\nFor the Apple data, here is the computation of the sales peak, reported\nin number of quarters from inception.\n> #PEAK SALES TIME POINT (IN QUARTERS)\n> tstar = 1/ (p+q)*log(p/q)\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 238,
      "chunk_index": 0
    }
  },
  {
    "text": "> tstar = 1/ (p+q)*log(p/q)\n−\n> print( tstar )\n( Intercept )\n33 77411\n.\n> length( isales )\n1 31\n[ ]\n31\nThe number of quarters that have already passed is . The peak arrives\nin a half a year!\n8.8 Notes\nThe Bass model has been extended to what is known as the generalized\n1994\nBass model in the paper by Bass, Krishnan, and Jain ( ). The idea is\nto extend the model to the following equation:\nf(t)\n= [p+q F(t)] x(t)\n1 F(t)\n−\nwhere x(t) stands for current marketing effort. This additional variable",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 238,
      "chunk_index": 1
    }
  },
  {
    "text": "allows (i) consideration of effort in the model, and (ii) given the function\nx(t), it may be optimized.\nThe Bass model comes from a deterministic differential equation. Ex-\ntensions to stochastic differential equations need to be considered.\nSee also the paper on Bayesian inference in Bass models by Boatwright\n2003\nand Kamakura ( ).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 238,
      "chunk_index": 2
    }
  },
  {
    "text": "virulent products: the bass model 239\nExercise\nIn the Bass model, if the coefficient of imitation increases relative to the\ncoefficient of innovation, then which of the following is the most valid?\n(a) the peak of the product life cycle occurs later.\n(b) the peak of the product life cycle occurs sooner.\n(c) there may be an increasing chance of two life-cycle peaks.\n(d) the peak may occur sooner or later, depending on the coefficient of\ninnovation.\nUsing peak time formula, substitute x = q/p:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 239,
      "chunk_index": 0
    }
  },
  {
    "text": "Using peak time formula, substitute x = q/p:\n1 ln(q/p) 1 ln(q/p) 1 ln(x)\nt\n∗\n= − ln(p/q) = = =\np+q p+q p 1+q/p p 1+x\nDifferentiate with regard to x (we are interested in the sign of the first\nderivative ∂t /∂q, which is the same as sign of ∂t /∂x):\n∗ ∗\n(cid:20) (cid:21)\n∂t 1 1 lnx 1+x xlnx\n∗\n= = −\n∂x p x(1+x) − (1+x)2 px(1+x)2\nFrom the Bass model we know that q > p > 0, i.e. x > 1, otherwise\nwe could get negative values of acceptance or shape without maximum",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 239,
      "chunk_index": 1
    }
  },
  {
    "text": "in the 0 F < 1 area. Therefore, the sign of ∂t /∂x is same as:\n∗\n≤\n(cid:18) (cid:19)\n∂t\n∗\nsign = sign(1+x xlnx), x > 1\n∂x −\nBut this non-linear equation\n1+x xlnx = 0, x > 1\n−\nhas a root x 3.59.\n≈\nIn other words, the derivative ∂t /∂x is negative when x > 3.59 and\n∗\npositive when x < 3.59. For low values of x = q/p, an increase in the\ncoefficient of imitation q increases the time to sales peak (illustrated in\nFigure 8 . 8 ), and for high values of q/p the time decreases with increas-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 239,
      "chunk_index": 2
    }
  },
  {
    "text": "ing q. So the right answer for the question appears to be “it depends on\nvalues of p and q”.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 239,
      "chunk_index": 3
    }
  },
  {
    "text": "240 data science: theories, models, algorithms, and analytics\n0 1 2 3 4 5\n511.0\n011.0\n501.0\n001.0\nt\n2ssab\nFigure8.8:Increaseinpeaktimewith\nq\n↑\np = .1, q = .22\np = .1, q = .20\nFigure 1: Increase in peak time with q\n↑\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 240,
      "chunk_index": 0
    }
  },
  {
    "text": "9\nExtracting Dimensions: Discriminant and Factor Analysis\n9.1 Overview\nIn this chapter we will try and understand two common approaches to\nanalyzing large data sets with a view to grouping the data and under-\nstanding the main structural components of the data. In discriminant\nanalysis (DA), we develop statistical models that differentiate two or\nmore population types, such as immigrants vs natives, males vs females,\netc. In factor analysis (FA), we attempt to collapse an enormous amount",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 241,
      "chunk_index": 0
    }
  },
  {
    "text": "of data about the population into a few common explanatory variables.\nDA is an attempt to explain categorical data, and FA is an attempt to\nreduce the dimensionality of the data that we use to explain both cate-\ngorical or continuous data. They are distinct techniques, related in that\nthey both exploit the techniques of linear algebra.\n9.2 Discriminant Analysis\nIn DA, what we are trying to explain is very often a dichotomous split",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 241,
      "chunk_index": 1
    }
  },
  {
    "text": "of our observations. For example, if we are trying to understand what\ndetermines a good versus a bad creditor. We call the good vs bad the\n“criterion” variable, or the “dependent” variable. The variables we use\nto explain the split between the criterion variables are called “predictor”\nor “explanatory” variables. We may think of the criterion variables as\nleft-hand side variables or dependent variables in the lingo of regression",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 241,
      "chunk_index": 2
    }
  },
  {
    "text": "analysis. Likewise, the explanatory variables are the right-hand side\nones.\nWhat distinguishes DA is that the left-hand side (lhs) variables are\nessentially qualitative in nature. They have some underlying numerical\nvalue, but are in essence qualitative. For example, when universities go",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 241,
      "chunk_index": 3
    }
  },
  {
    "text": "242 data science: theories, models, algorithms, and analytics\nthrough the admission process, they may have a cut off score for ad-\nmission. This cut off score discriminates the students that they want to\nadmit and the ones that they wish to reject. DA is a very useful tool for\ndetermining this cut off score.\nIn short, DA is the means by which quantitative explanatory variables\nare used to explain qualitative criterion variables. The number of qual-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 242,
      "chunk_index": 0
    }
  },
  {
    "text": "itative categories need not be restricted to just two. DA encompasses a\nlarger number of categories too.\n9.2.1 Notation and assumptions\n• Assume that there are N categories or groups indexed by i = 2...N.\n• Within each group there are observations y , indexed by j = 1...M.\nj i\nThe size of each group need not be the same, i.e., it is possible that\nM = M .\ni j\n(cid:54)\n• There are a set of predictor variables x = [x ,x ,...,x ] . Clearly,\n1 2 K (cid:48)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 242,
      "chunk_index": 1
    }
  },
  {
    "text": "1 2 K (cid:48)\nthere must be good reasons for choosing these so as to explain the\ngroups in which the y reside. Hence the value of the kth variable for\nj\ngroup i, observation j, is denoted as x .\nijk\n• Observations are mutually exclusive, i.e., each object can only belong\nto any one of the groups.\n• The K K covariance matrix of explanatory variables is assumed to be\n×\nthe same for all groups, i.e., Cov(x ) = Cov(x ).\ni j\n9.2.2 Discriminant Function",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 242,
      "chunk_index": 2
    }
  },
  {
    "text": "i j\n9.2.2 Discriminant Function\nDA involves finding a discriminant function D that best classifies the\nobservations into the chosen groups. The function may be nonlinear, but\nthe most common approach is to use linear DA. The function takes the\nfollowing form:\nK\n∑\nD = a x +a x +...+a x = a x\n1 1 2 2 K K k k\nk=1\nwhere the a coefficients are discriminant weights.\nk\nThe analysis requires the inclusion of a cut-off score C. For example,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 242,
      "chunk_index": 3
    }
  },
  {
    "text": "if N = 2, i.e., there are 2 groups, then if D > C the observation falls into\ngroup 1 , and if D C, then the observation falls into group 2 .\n≤",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 242,
      "chunk_index": 4
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 243\nHence, the objective function is to choose a ,C such that classifica-\nk\n{{ } }\ntion error is minimized. The equation C = D( x ; a ) is the equation\nk k\n{ } { }\n2\nof a hyperplane that cuts the space of the observations into parts if\nthere are only two groups. Note that if there are N groups then there\nwill be (N 1) cutoffs C ,C ,...,C , and a corresponding number\n1 2 N 1\n− { − }\nof hyperplanes.\nExercise\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 243,
      "chunk_index": 0
    }
  },
  {
    "text": "1 2 N 1\n− { − }\nof hyperplanes.\nExercise\n2\nDraw a diagram of the distribution of groups of observations and the\ncut off C. Shade the area under the distributions where observations for\n1 2\ngroup are wrongly classified as group ; and vice versa.\nThe variables x are also known as the “discriminants”. In the extrac-\nk\ntion of the discriminant function, better discriminants will have higher\nstatistical significance.\nExercise\n2 2\nDraw a diagram of DA with groups and discriminants. Make the dia-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 243,
      "chunk_index": 1
    }
  },
  {
    "text": "gram such that one of the variables is shown to be a better discriminant.\nHow do you show this diagrammatically?\n9.2.3 How good is the discriminant function?\nAfter fitting the discriminant function, the next question to ask is how\ngood the fit is. There are various measures that have been suggested for\nthis. All of them have the essential property that they best separate the\ndistribution of observations for different groups. There are many such",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 243,
      "chunk_index": 2
    }
  },
  {
    "text": "measures: (a) Point biserial correlation, (b) Mahalobis D, and (c) the con-\nfusion matrix. Each of the measures assesses the degree of classification\nerror.\nThe point biserial correlation is the R2 of a regression in which the\nclassified observations are signed as y = 1,i = 1 for group 1 and\nij\ny = 0,i = 2 for group 2 , and the rhs variables are the x values.\nij ijk\nThe Mahalanobis distance between any two characteristic vectors for\ntwo entities in the data is given by\n(cid:113)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 243,
      "chunk_index": 3
    }
  },
  {
    "text": "two entities in the data is given by\n(cid:113)\nD = (x x ) Σ 1(x x )\nM 1 2 (cid:48) − 1 2\n− −\nwhere x ,x are two vectors and Σ is the covariance matrix of character-\n1 2\nΣ\nistics of all observations in the data set. First, note that if is the identity",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 243,
      "chunk_index": 4
    }
  },
  {
    "text": "244 data science: theories, models, algorithms, and analytics\nmatrix, then D defaults to the Euclidean distance between two vec-\nM\ntors. Second, one of the vectors may be treated as the mean vector for a\ngiven category, in which case the Mahalanobis distance can be used to\nassess the distances within and across groups in a pairwise manner. The\nquality of the discriminant function is then gauged by computing the\nratio of the average distance across groups to the average distance within",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 244,
      "chunk_index": 0
    }
  },
  {
    "text": "groups. Such ratios are often called the Fisher’s discriminant value.\nThe confusion matrix is a cross-tabulation of the actual versus pre-\ndicted classification. For example, a n-category model will result in a\nn n confusion matrix. A comparison of this matrix with a matrix where\n×\nthe model is assumed to have no classification ability leads to a χ2 statis-\ntic that informs us about the statistical strength of the classification abil-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 244,
      "chunk_index": 1
    }
  },
  {
    "text": "ity of the model. We will examine this in more detail shortly.\n9.2.4 Caveats\nBe careful to not treat dependent variables that are actually better off\nremaining continuous as being artificially grouped in qualitative subsets.\n9.2.5 Implementation using R\n64\nWe implement a discriminant function model using data for the top\n2005 06\nteams in the - NCAA tournament. The data is as follows (averages\nper game):\n3\nGMS PTS REB AST TO A.T STL BLK PF FG FT X P",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 244,
      "chunk_index": 2
    }
  },
  {
    "text": "3\nGMS PTS REB AST TO A.T STL BLK PF FG FT X P\n1 6 84 2 41 5 17 8 12 8 1 39 6 7 3 8 16 7 0 514 0 664 0 417\n. . . . . . . . . . .\n2 6 74 5 34 0 19 0 10 2 1 87 8 0 1 7 16 5 0 457 0 753 0 361\n. . . . . . . . . . .\n3 5 77 4 35 4 13 6 11 0 1 24 5 4 4 2 16 6 0 479 0 702 0 376\n. . . . . . . . . . .\n4 5 80 8 37 8 13 0 12 6 1 03 8 4 2 4 19 8 0 445 0 783 0 329\n. . . . . . . . . . .\n5 4 79 8 35 0 15 8 14 5 1 09 6 0 6 5 13 3 0 542 0 759 0 397\n. . . . . . . . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 244,
      "chunk_index": 3
    }
  },
  {
    "text": ". . . . . . . . . . .\n6 4 72 8 32 3 12 8 13 5 0 94 7 3 3 5 19 5 0 510 0 663 0 400\n. . . . . . . . . . .\n7 4 68 8 31 0 13 0 11 3 1 16 3 8 0 8 14 0 0 467 0 753 0 429\n. . . . . . . . . . .\n8 4 81 0 28 5 19 0 14 8 1 29 6 8 3 5 18 8 0 509 0 762 0 467\n. . . . . . . . . . .\n9 3 62 7 36 0 8 3 15 3 0 54 8 0 4 7 19 7 0 407 0 716 0 328\n. . . . . . . . . . .\n10 3 65 3 26 7 13 0 14 0 0 93 11 3 5 7 17 7 0 409 0 827 0 377\n. . . . . . . . . . .\n11 3 75 3 29 0 16 0 13 0 1 23 8 0 0 3 17 7 0 483 0 827 0 476",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 244,
      "chunk_index": 4
    }
  },
  {
    "text": ". . . . . . . . . . .\n12 3 65 7 41 3 8 7 14 3 0 60 9 3 4 3 19 7 0 360 0 692 0 279\n. . . . . . . . . . .\n13 3 59 7 34 7 13 3 16 7 0 80 4 7 2 0 17 3 0 472 0 579 0 357\n. . . . . . . . . . .\n14 3 88 0 33 3 17 0 11 3 1 50 6 7 1 3 19 7 0 508 0 696 0 358\n. . . . . . . . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 244,
      "chunk_index": 5
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 245\n15 3 76 3 27 7 16 3 11 7 1 40 7 0 3 0 18 7 0 457 0 750 0 405\n. . . . . . . . . . .\n16 3 69 7 32 7 16 3 12 3 1 32 8 3 1 3 14 3 0 509 0 646 0 308\n. . . . . . . . . . .\n17 2 72 5 33 5 15 0 14 5 1 03 8 5 2 0 22 5 0 390 0 667 0 283\n. . . . . . . . . . .\n18 2 69 5 37 0 13 0 13 5 0 96 5 0 5 0 14 5 0 464 0 744 0 250\n. . . . . . . . . . .\n19 2 66 0 33 0 12 0 17 5 0 69 8 5 6 0 25 5 0 387 0 818 0 341\n. . . . . . . . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 245,
      "chunk_index": 0
    }
  },
  {
    "text": ". . . . . . . . . . .\n20 2 67 0 32 0 11 0 12 0 0 92 8 5 1 5 21 5 0 440 0 781 0 406\n. . . . . . . . . . .\n21 2 64 5 43 0 15 5 15 0 1 03 10 0 5 0 20 0 0 391 0 528 0 286\n. . . . . . . . . . .\n22 2 71 0 30 5 13 0 10 5 1 24 8 0 1 0 25 0 0 410 0 818 0 323\n. . . . . . . . . . .\n23 2 80 0 38 5 20 0 20 5 0 98 7 0 4 0 18 0 0 520 0 700 0 522\n. . . . . . . . . . .\n24 2 87 5 41 5 19 5 16 5 1 18 8 5 2 5 20 0 0 465 0 667 0 333\n. . . . . . . . . . .\n25 2 71 0 40 5 9 5 10 5 0 90 8 5 3 0 19 0 0 393 0 794 0 156",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 245,
      "chunk_index": 1
    }
  },
  {
    "text": ". . . . . . . . . . .\n26 2 60 5 35 5 9 5 12 5 0 76 7 0 0 0 15 5 0 341 0 760 0 326\n. . . . . . . . . . .\n27 2 79 0 33 0 14 0 10 0 1 40 3 0 1 0 18 0 0 459 0 700 0 409\n. . . . . . . . . . .\n28 2 74 0 39 0 11 0 9 5 1 16 5 0 5 5 19 0 0 437 0 660 0 433\n. . . . . . . . . . .\n29 2 63 0 29 5 15 0 9 5 1 58 7 0 1 5 22 5 0 429 0 767 0 283\n. . . . . . . . . . .\n30 2 68 0 36 5 14 0 9 0 1 56 4 5 6 0 19 0 0 398 0 634 0 364\n. . . . . . . . . . .\n31 2 71 5 42 0 13 5 11 5 1 17 3 5 3 0 15 5 0 463 0 600 0 241",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 245,
      "chunk_index": 2
    }
  },
  {
    "text": ". . . . . . . . . . .\n32 2 60 0 40 5 10 5 11 0 0 95 7 0 4 0 15 5 0 371 0 651 0 261\n. . . . . . . . . . .\n33 2 73 5 32 5 13 0 13 5 0 96 5 5 1 0 15 0 0 470 0 684 0 433\n. . . . . . . . . . .\n34 1 70 0 30 0 9 0 5 0 1 80 6 0 3 0 19 0 0 381 0 720 0 222\n. . . . . . . . . . .\n35 1 66 0 27 0 16 0 13 0 1 23 5 0 2 0 15 0 0 433 0 533 0 300\n. . . . . . . . . . .\n36 1 68 0 34 0 19 0 14 0 1 36 9 0 4 0 20 0 0 446 0 250 0 375\n. . . . . . . . . . .\n37 1 68 0 42 0 10 0 21 0 0 48 6 0 5 0 26 0 0 359 0 727 0 194",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 245,
      "chunk_index": 3
    }
  },
  {
    "text": ". . . . . . . . . . .\n38 1 53 0 41 0 8 0 17 0 0 47 9 0 1 0 18 0 0 333 0 600 0 217\n. . . . . . . . . . .\n39 1 77 0 33 0 15 0 18 0 0 83 5 0 0 0 16 0 0 508 0 250 0 450\n. . . . . . . . . . .\n40 1 61 0 27 0 12 0 17 0 0 71 8 0 3 0 16 0 0 420 0 846 0 400\n. . . . . . . . . . .\n41 1 55 0 42 0 11 0 17 0 0 65 6 0 3 0 19 0 0 404 0 455 0 250\n. . . . . . . . . . .\n42 1 47 0 35 0 6 0 17 0 0 35 9 0 4 0 20 0 0 298 0 750 0 160\n. . . . . . . . . . .\n43 1 57 0 37 0 8 0 24 0 0 33 9 0 3 0 12 0 0 418 0 889 0 250",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 245,
      "chunk_index": 4
    }
  },
  {
    "text": ". . . . . . . . . . .\n44 1 62 0 33 0 8 0 20 0 0 40 8 0 5 0 21 0 0 391 0 654 0 500\n. . . . . . . . . . .\n45 1 65 0 34 0 17 0 17 0 1 00 11 0 2 0 19 0 0 352 0 500 0 333\n. . . . . . . . . . .\n46 1 71 0 30 0 10 0 10 0 1 00 7 0 3 0 20 0 0 424 0 722 0 348\n. . . . . . . . . . .\n47 1 54 0 35 0 12 0 22 0 0 55 5 0 1 0 19 0 0 404 0 667 0 300\n. . . . . . . . . . .\n48 1 57 0 40 0 2 0 5 0 0 40 5 0 6 0 16 0 0 353 0 667 0 500\n. . . . . . . . . . .\n49 1 81 0 30 0 13 0 15 0 0 87 9 0 1 0 29 0 0 426 0 846 0 350",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 245,
      "chunk_index": 5
    }
  },
  {
    "text": ". . . . . . . . . . .\n50 1 62 0 37 0 14 0 18 0 0 78 7 0 0 0 21 0 0 453 0 556 0 333\n. . . . . . . . . . .\n51 1 67 0 37 0 12 0 16 0 0 75 8 0 2 0 16 0 0 353 0 867 0 214\n. . . . . . . . . . .\n52 1 53 0 32 0 15 0 12 0 1 25 6 0 3 0 16 0 0 364 0 600 0 368\n. . . . . . . . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 245,
      "chunk_index": 6
    }
  },
  {
    "text": "246 data science: theories, models, algorithms, and analytics\n53 1 73 0 34 0 17 0 19 0 0 89 3 0 3 0 20 0 0 520 0 750 0 391\n. . . . . . . . . . .\n54 1 71 0 29 0 16 0 10 0 1 60 10 0 6 0 21 0 0 344 0 857 0 393\n. . . . . . . . . . .\n55 1 46 0 30 0 10 0 11 0 0 91 3 0 1 0 23 0 0 365 0 500 0 333\n. . . . . . . . . . .\n56 1 64 0 35 0 14 0 17 0 0 82 5 0 1 0 20 0 0 441 0 545 0 333\n. . . . . . . . . . .\n57 1 64 0 43 0 5 0 11 0 0 45 6 0 1 0 20 0 0 339 0 760 0 294\n. . . . . . . . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 246,
      "chunk_index": 0
    }
  },
  {
    "text": ". . . . . . . . . . .\n58 1 63 0 34 0 14 0 13 0 1 08 5 0 3 0 15 0 0 435 0 815 0 091\n. . . . . . . . . . .\n59 1 63 0 36 0 11 0 20 0 0 55 8 0 2 0 18 0 0 397 0 643 0 381\n. . . . . . . . . . .\n60 1 52 0 35 0 8 0 8 0 1 00 4 0 2 0 15 0 0 415 0 500 0 235\n. . . . . . . . . . .\n61 1 50 0 19 0 10 0 17 0 0 59 12 0 2 0 22 0 0 444 0 700 0 300\n. . . . . . . . . . .\n62 1 56 0 42 0 3 0 20 0 0 15 2 0 2 0 17 0 0 333 0 818 0 200\n. . . . . . . . . . .\n63 1 54 0 22 0 13 0 10 0 1 30 6 0 1 0 20 0 0 415 0 889 0 222",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 246,
      "chunk_index": 1
    }
  },
  {
    "text": ". . . . . . . . . . .\n64 1 64 0 36 0 16 0 13 0 1 23 4 0 0 0 19 0 0 367 0 833 0 385\n. . . . . . . . . . .\nWe loaded in the data and ran the following commands (which are\nstored in the program file lda.R:\nncaa = read. table(\"ncaa. txt\" ,header=TRUE)\nx = as.matrix(ncaa[ 4 : 14 ])\n1 1 32\ny = :\n1 1 0 1\ny = y * +\n2 1 0\ny = y *\ny = c(y 1 ,y 2 )\nlibrary(MASS)\ndm = lda(y~x)\nHence the top 32 teams are category 1 (y = 1) and the bottom 32\nteams are category 2 (y = 0). The results are as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 246,
      "chunk_index": 2
    }
  },
  {
    "text": "> lda(y~x)\nCall:\nlda(y ~ x)\nPrior probabilities of groups:\n0 1\n0.5 0.5\nGroup means:\nxPTS xREB xAST xTO xA.T xSTL xBLK xPF\n0 62.10938 33.85938 11.46875 15.01562 0.835625 6.609375 2.375 18.84375\n1 72.09375 35.07500 14.02812 12.90000 1.120000 7.037500 3.125 18.46875\nxFG xFT xX3P\n0 0.4001562 0.6685313 0.3142187\n1 0.4464375 0.7144063 0.3525313\nCoefficients of linear discriminants:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 246,
      "chunk_index": 3
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 247\nLD1\nxPTS 0.02192489\n−\nxREB 0.18473974\nxAST 0.06059732\nxTO 0.18299304\n−\nxA.T 0.40637827\nxSTL 0.24925833\nxBLK 0.09090269\nxPF 0.04524600\nxFG 19.06652563\nxFT 4.57566671\nxX3P 1.87519768\nSome useful results can be extracted as follows:\n> result = lda(y~x)\n> result$prior\n0 1\n0.5 0.5\n> result$means\nxPTS xREB xAST xTO xA.T xSTL xBLK xPF\n0 62.10938 33.85938 11.46875 15.01562 0.835625 6.609375 2.375 18.84375",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 247,
      "chunk_index": 0
    }
  },
  {
    "text": "1 72.09375 35.07500 14.02812 12.90000 1.120000 7.037500 3.125 18.46875\nxFG xFT xX3P\n0 0.4001562 0.6685313 0.3142187\n1 0.4464375 0.7144063 0.3525313\n> result$call\nlda(formula = y ~ x)\n> result$N\n[1] 64\n> result$svd\n[1] 7.942264\nThe last line contains the singular value decomposition level, which\nis also the level of the Fischer discriminant, which gives the ratio of the\nbetween- and within-group standard deviations on the linear discrimi-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 247,
      "chunk_index": 1
    }
  },
  {
    "text": "nant variables. Their squares are the canonical F-statistics.\nWe can look at the performance of the model as follows:\n> result = lda(y~x)\n> predict(result)$class\n[1] 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0\nLevels: 0 1\nIf we want the value of the predicted normalized discriminant func-\ntion we simply do\n> predict( result )\nThe cut off is treated as being at zero.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 247,
      "chunk_index": 2
    }
  },
  {
    "text": "248 data science: theories, models, algorithms, and analytics\n9.2.6 Confusion Matrix\nAs we have seen before, the confusion matrix is a tabulation of actual\nand predicted values. To generate the confusion matrix for our basket-\nball example here we use the following commands in R:\n> result = lda(y~x)\n> y_pred = predict( result )$class\n> length(y_pred)\n1 64\n[ ]\n> table(y,y_pred)\ny_pred\n0 1\ny\n0 27 5\n1 5 27\n5 64\nWe can see that of the teams have been misclassified. Is this statisti-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 248,
      "chunk_index": 0
    }
  },
  {
    "text": "cally significant? In order to assess this, we compute the χ2 statistic for\nthe confusion matrix. Let’s define the confusion matrix as\n(cid:34) (cid:35)\n27 5\nA =\n5 27\nThis matrix shows some classification ability. Now we ask, what if the\nmodel has no classification ability, then what would the average confu-\nsion matrix look like? It’s easy to see that this would give a matrix that\nwould assume no relation between the rows and columns, and the num-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 248,
      "chunk_index": 1
    }
  },
  {
    "text": "bers in each cell would reflect the average number drawn based on row\nand column totals. In this case since the row and column totals are all\n32\n, we get the following confusion matrix of no classification ability:\n(cid:34) (cid:35)\n16 16\nE =\n16 16\nThe test statistic is the sum of squared normalized differences in the cells\nof both matrices, i.e.,\n[A E ]2\n∑ ij ij\nTest-Stat = −\nE\ni,j ij\nWe compute this in R.\n> A = matrix(c( 27 , 5 , 5 , 27 ) , 2 , 2 )\n> A",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 248,
      "chunk_index": 2
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 249\n1 2\n[ , ] [ , ]\n1 27 5\n[ ,]\n2 5 27\n[ ,]\n> E = matrix(c( 16 , 16 , 16 , 16 ) , 2 , 2 )\n> E\n1 2\n[ , ] [ , ]\n1 16 16\n[ ,]\n2 16 16\n[ ,]\n> test_stat = sum((A E)^ 2/E)\n−\n> test_stat\n1 30 25\n[ ] .\n> 1 pchisq( test_stat , 1 )\n−\n1 3 797912 08\n[ ] . e\n−\nThe χ2 distribution requires entering the degrees of freedom. In this\ncase, the degrees of freedom is 1 , i.e., equal to (r 1)(c 1), where r\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 249,
      "chunk_index": 0
    }
  },
  {
    "text": "− −\nis the number of rows and c is the number of columns. We see that the\nprobability of the A and E matrices being the same is zero. Hence, the\ntest suggests that the model has statistically significant classification\nability.\n9.2.7 Multiple groups\n4\nWhat if we wanted to discriminate the NCAA data into groups? Its\njust as simple:\n> y1 = rep(3,16)\n> y2 = rep(2,16)\n> y3 = rep(1,16)\n> y4 = rep(0,16)\n> y = c(y1,y2,y3,y4)\n> res = lda(y~x)\n> res\nCall:\nlda(y ~ x)\nPrior probabilities of groups:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 249,
      "chunk_index": 1
    }
  },
  {
    "text": "Call:\nlda(y ~ x)\nPrior probabilities of groups:\n0 1 2 3\n0.25 0.25 0.25 0.25\nGroup means:\nxPTS xREB xAST xTO xA.T xSTL xBLK xPF xFG\n0 61.43750 33.18750 11.93750 14.37500 0.888750 6.12500 1.8750 19.5000 0.4006875\n1 62.78125 34.53125 11.00000 15.65625 0.782500 7.09375 2.8750 18.1875 0.3996250\n2 70.31250 36.59375 13.50000 12.71875 1.094375 6.84375 3.1875 19.4375 0.4223750\n3 73.87500 33.55625 14.55625 13.08125 1.145625 7.23125 3.0625 17.5000 0.4705000\nxFT xX3P\n0 0.7174375 0.3014375",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 249,
      "chunk_index": 2
    }
  },
  {
    "text": "xFT xX3P\n0 0.7174375 0.3014375\n1 0.6196250 0.3270000\n2 0.7055625 0.3260625",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 249,
      "chunk_index": 3
    }
  },
  {
    "text": "250 data science: theories, models, algorithms, and analytics\n3 0.7232500 0.3790000\nCoefficients of linear discriminants:\nLD1 LD2 LD3\nxPTS 0.03190376 0.09589269 0.03170138\nxREB\n−0.16962627 −0.08677669 −0.11932275\nxAST 0.08820048 0.47175998 −0.04601283\nxTO 0.20264768 0.29407195 0.02550334\nxA.T\n−0.02619042 −3.28901817 −1.42081485\nxSTL 0.23954511 −0.26327278 −0.02694612\nxBLK 0.05424102 −0.14766348 −0.17703174\nxPF 0.03678799 −0.22610347 −0.09608475\nxFG 21.25583140 0.48722022 −9.50234314",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 250,
      "chunk_index": 0
    }
  },
  {
    "text": "xFG 21.25583140 0.48722022 −9.50234314\nxFT 5.42057568 6.39065311 2.72767409\nxX3P 1.98050128 2.74869782 0.90901853\n−\nProportion of trace:\nLD1 LD2 LD3\n0.6025 0.3101 0.0873\n> predict(res)$class\n[1] 3 3 3 3 3 3 3 3 1 3 3 2 0 3 3 3 0 3 2 3 2 2 3 2 2 0 2 2 2 2 2 2 3 1 1 1 0 1\n[39] 1 1 1 1 1 1 1 1 0 2 2 0 0 0 0 2 0 0 2 0 1 0 1 1 0 0\nLevels: 0 1 2 3\n> y\n[1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1\n[40] 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 250,
      "chunk_index": 1
    }
  },
  {
    "text": "> y_pred = predict(res)$class\n> table(y,y_pred)\ny_pred\ny 0 1 2 3\n0 10 3 3 0\n1 2 12 1 1\n2 2 0 11 3\n3 1 1 1 13\nExercise\nUse the spreadsheet titled default-analysis-data.xls and fit a model\nto discriminate firms that default from firms that do not. How good a fit\ndoes your model achieve?\n9.3 Eigen Systems\nWe now move on to understanding some properties of matrices that\nmay be useful in classifying data or deriving its underlying compo-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 250,
      "chunk_index": 2
    }
  },
  {
    "text": "nents. We download Treasury interest rate date from the FRED website,\nhttp://research.stlouisfed.org/fred2/. I have placed the data in a\nfile called tryrates.txt. Let’s read in the file.\n> rates = read. table(\"tryrates . txt\" ,header=TRUE)\n> names( rates )\n1 3 6 1 2 3 5 7\n[ ] \"DATE\" \"FYGM \" \"FYGM \" \"FYGT \" \"FYGT \" \"FYGT \" \"FYGT \" \"FYGT \"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 250,
      "chunk_index": 3
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 251\n9 10\n[ ] \"FYGT \"\nA M M matrix A has attendant M eigenvectors V and eigenvalue λ\n×\nif we can write\nλV = A V\nStarting with matrix A, the eigenvalue decomposition gives both V and\nλ. It turns out we can find M such eigenvalues and eigenvectors, as\nthere is no unique solution to this equation. We also require that λ = 0.\n(cid:54)\nWe may implement this in R as follows, setting matrix A equal to the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 251,
      "chunk_index": 0
    }
  },
  {
    "text": "covariance matrix of the rates of different maturities:\n> eigen(cov(rates))\n$values\n[1] 7.070996e+01 1.655049e+00 9.015819e 02 1.655911e 02 3.001199e 03\n[6] 2.145993e 03 1.597282e 03 8.562439e −04 − −\n− − −\n$vectors\n[,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 0.3596990 0.49201202 0.59353257 0.38686589 0.34419189 0.07045281\n[2,] −0.3581944 −0.40372601 0.06355170 −0.20153645 −0.79515713 −0.07823632\n[3,] −0.3875117 −0.28678312 0.30984414 0.61694982 0.45913099 0.20442661",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 251,
      "chunk_index": 1
    }
  },
  {
    "text": "[4,] −0.3753168 −0.01733899 −0.45669522 0.19416861 −0.03906518 0.46590654\n[5,] −0.3614653 −0.13461055 −0.36505588 −0.41827644 0.06076305 −0.14203743\n[6,] −0.3405515 0.31741378 −0.01159915 −0.18845999 −0.03366277 −0.72373049\n[7,] −0.3260941 0.40838395 −0.19017973 −0.05000002 −0.16835391 0.09196861\n[8,] −0.3135530 0.47616732 0.41174955 −0.42239432 0.06132982 0.42147082\n− [,7] [,8] − −\n[1,] 0.04282858 0.03645143\n[2,] 0.15571962 0.03744201\n[3,] −0.10492279 −0.16540673\n[4,] 0.30395044 −0.54916644",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 251,
      "chunk_index": 2
    }
  },
  {
    "text": "[4,] 0.30395044 −0.54916644\n[5,] 0.45521861 0.55849003\n[6,] −0.19935685 −0.42773742\n[7,] −0.70469469 0.39347299\n[8,] 0.35631546 −0.13650940\n−\n> rcorr = cor(rates)\n> rcorr\nFYGM3 FYGM6 FYGT1 FYGT2 FYGT3 FYGT5 FYGT7\nFYGM3 1.0000000 0.9975369 0.9911255 0.9750889 0.9612253 0.9383289 0.9220409\nFYGM6 0.9975369 1.0000000 0.9973496 0.9851248 0.9728437 0.9512659 0.9356033\nFYGT1 0.9911255 0.9973496 1.0000000 0.9936959 0.9846924 0.9668591 0.9531304",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 251,
      "chunk_index": 3
    }
  },
  {
    "text": "FYGT2 0.9750889 0.9851248 0.9936959 1.0000000 0.9977673 0.9878921 0.9786511\nFYGT3 0.9612253 0.9728437 0.9846924 0.9977673 1.0000000 0.9956215 0.9894029\nFYGT5 0.9383289 0.9512659 0.9668591 0.9878921 0.9956215 1.0000000 0.9984354\nFYGT7 0.9220409 0.9356033 0.9531304 0.9786511 0.9894029 0.9984354 1.0000000\nFYGT10 0.9065636 0.9205419 0.9396863 0.9680926 0.9813066 0.9945691 0.9984927\nFYGT10\nFYGM3 0.9065636\nFYGM6 0.9205419\nFYGT1 0.9396863\nFYGT2 0.9680926\nFYGT3 0.9813066\nFYGT5 0.9945691\nFYGT7 0.9984927",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 251,
      "chunk_index": 4
    }
  },
  {
    "text": "FYGT3 0.9813066\nFYGT5 0.9945691\nFYGT7 0.9984927\nFYGT10 1.0000000",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 251,
      "chunk_index": 5
    }
  },
  {
    "text": "252 data science: theories, models, algorithms, and analytics\nSo we calculated the eigenvalues and eigenvectors for the covariance\nmatrix of the data. What does it really mean? Think of the covariance\nmatrix as the summarization of the connections between the rates of dif-\nferent maturities in our data set. What we do not know is how many\ndimensions of commonality there are in these rates, and what is the rel-\native importance of these dimensions. For each dimension of commonal-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 252,
      "chunk_index": 0
    }
  },
  {
    "text": "ity, we wish to ask (a) how important is that dimension (the eigenvalue),\nand (b) the relative influence of that dimension on each rate (the values\nin the eigenvector). The most important dimension is the one with the\nhighest eigenvalue, known as the “principal” eigenvalue, corresponding\nto which we have the principal eigenvector. It should be clear by now\nthat the eigenvalue and its eigenvector are “eigen pairs”. It should also",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 252,
      "chunk_index": 1
    }
  },
  {
    "text": "be intuitive why we call this the eigenvalue “decomposition” of a matrix.\n9.4 Factor Analysis\nFactor analysis is the use of eigenvalue decomposition to uncover the\nunderlying structure of the data. Given a data set of observations and\nexplanatory variables, factor analysis seeks to achieve a decomposition\nwith these two properties:\n1\n. Obtain a reduced dimension set of explanatory variables, known as\nderived/extracted/discovered factors. Factors must be orthogonal, i.e.,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 252,
      "chunk_index": 2
    }
  },
  {
    "text": "uncorrelated with each other.\n2\n. Obtain data reduction, i.e., suggest a limited set of variables. Each\nsuch subset is a manifestation of an abstract underlying dimension.\nThese subsets are also ordered in terms of their ability to explain the\nvariation across observations.\nSee the article by Richard Darlington\nhttp://www.psych.cornell.edu/Darlington/factor.htm\nwhich is as good as any explanation one can get. See also the article\nby Statsoft:\nhttp://www.statsoft.com/textbook/stfacan.html",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 252,
      "chunk_index": 3
    }
  },
  {
    "text": "http://www.statsoft.com/textbook/stfacan.html\n9.4.1 Notation\n• Observations: y ,i = 1...N.\ni\n• Original explanatory variables: x ,k = 1...K.\nik",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 252,
      "chunk_index": 4
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 253\n• Factors: F,j = 1...M.\nj\n• M < K.\n9.4.2 The Idea\nAs you can see in the rates data, there are eight different rates. If we\nwanted to model the underlying drivers of this system of rates, we could\nassume a separate driver for each one leading to K = 8 underlying\nfactors. But the whole idea of factor analysis is to reduce the number\nof drivers that exist. So we may want to go with a smaller number of\nM < K factors.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 253,
      "chunk_index": 0
    }
  },
  {
    "text": "M < K factors.\nThe main concept here is to “project” the variables x RK onto the\n∈\nreduced factor set F RM such that we can explain most of the variables\n∈\nby the factors. Hence we are looking for a relation\nx = BF\nwhere B = b RK M is a matrix of factor “loadings” for the vari-\nkj ×\n{ } ∈\nables. Through matrix B, x may be represented in smaller dimension M.\nThe entries in matrix B may be positive or negative. Negative loadings",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 253,
      "chunk_index": 1
    }
  },
  {
    "text": "mean that the variable is negatively correlated with the factor. The whole\nidea is that we want to replace the relation of y to x with a relation of y\nto a reduced set F.\nOnce we have the set of factors defined, then the N observations y\nmay be expressed in terms of the factors through a factor “score” matrix\nA = a RN M as follows:\nij ×\n{ } ∈\ny = AF\nAgain, factor scores may be positive or negative. There are many ways\nin which such a transformation from variables to factors might be under-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 253,
      "chunk_index": 2
    }
  },
  {
    "text": "taken. We look at the most common one.\n9.4.3 Principal Components Analysis (PCA)\nIn PCA, each component (factor) is viewed as a weighted combination\nof the other variables (this is not always the way factor analysis is imple-\nmented, but is certainly one of the most popular).\nThe starting point for PCA is the covariance matrix of the data. Essen-\ntially what is involved is an eigenvalue analysis of this matrix to extract\nthe principal eigenvectors.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 253,
      "chunk_index": 3
    }
  },
  {
    "text": "254 data science: theories, models, algorithms, and analytics\nWe can do the analysis using the R statistical package. Here is the\nsample session:\n> ncaa = read. table(\"ncaa. txt\" ,header=TRUE)\n4 14\n> x = ncaa[ : ]\n> result = princomp(x)\n> screeplot ( result )\n> screeplot (result ,type=\"lines\")\nThe results are as follows:\n> summary(result)\nImportance of components:\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5\nStandard deviation 9.8747703 5.2870154 3.9577315 3.19879732 2.43526651",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 254,
      "chunk_index": 0
    }
  },
  {
    "text": "Proportion of Variance 0.5951046 0.1705927 0.0955943 0.06244717 0.03619364\nCumulative Proportion 0.5951046 0.7656973 0.8612916 0.92373878 0.95993242\nComp.6 Comp.7 Comp.8 Comp.9\nStandard deviation 2.04505010 1.53272256 0.1314860827 1.062179e 01\nProportion of Variance 0.02552391 0.01433727 0.0001055113 6.885489e −05\nCumulative Proportion 0.98545633 0.99979360 0.9998991100 9.999680e −01\nComp.10 Comp.11 −\nStandard deviation 6.591218e 02 3.007832e 02",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 254,
      "chunk_index": 1
    }
  },
  {
    "text": "Standard deviation 6.591218e 02 3.007832e 02\nProportion of Variance 2.651372e −05 5.521365e −06\nCumulative Proportion 9.999945e −01 1.000000e −00\n− −\nThe resultant “screeplot” shows the amount explained by each compo-\nnent.\nLets look at the loadings. These are the respective eigenvectors:\n> result$loadings\nLoadings:\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 Comp.10\nPTS 0.964 0.240",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 254,
      "chunk_index": 2
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 255\nREB 0.940 0.316\nAST 0.257 0.228 0.283 0.431 −0.778\nTO −0.194 −0.908 −0.116 −0.313 0.109\nA.T − − − 0.712 0.642 0.262\nSTL 0.194 0.205 0.816 0.498\nBLK − 0.516 0.849\nPF 0.110 0.223 0.862 0.364 0.228 −\n− − − −\nFG\nFT 0.619 0.762 0.175\nX3P 0.315 − 0.948\nComp.11 −\nPTS\nREB\nAST\nTO\nA.T\nSTL\nBLK\nPF\nFG 0.996\n−\nFT\nX3P\nWe can see that the main variable embedded in the first principal\ncomponent is PTS. (Not surprising!). We can also look at the standard",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 255,
      "chunk_index": 0
    }
  },
  {
    "text": "deviation of each component:\n> result$sdev\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7\n9.87477028 5.28701542 3.95773149 3.19879732 2.43526651 2.04505010 1.53272256\nComp.8 Comp.9 Comp.10 Comp.11\n0.13148608 0.10621791 0.06591218 0.03007832\nThe biplot shows the first two components and overlays the variables\nas well. This is a really useful visual picture of the results of the analysis.\n> biplot ( result )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 255,
      "chunk_index": 1
    }
  },
  {
    "text": "256 data science: theories, models, algorithms, and analytics\nThe alternative function prcomp returns the same stuff, but gives all\nthe factor loadings immediately.\n> prcomp(x)\nStandard deviations:\n[1] 9.95283292 5.32881066 3.98901840 3.22408465 2.45451793 2.06121675\n[7] 1.54483913 0.13252551 0.10705759 0.06643324 0.03031610\nRotation:\nPC1 PC2 PC3 PC4 PC5\nPTS 0.963808450 0.052962387 0.018398319 0.094091517 0.240334810\n− − −\nREB 0.022483140 0.939689339 0.073265952 0.026260543 0.315515827\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 256,
      "chunk_index": 0
    }
  },
  {
    "text": "− −\nAST 0.256799635 0.228136664 0.282724110 0.430517969 0.778063875\n− − −\nTO 0.061658120 0.193810802 0.908005124 0.115659421 0.313055838\n− − − −\nA.T 0.021008035 0.030935414 0.035465079 0.022580766 0.068308725\n− −\nSTL 0.006513483 0.081572061 0.193844456 0.205272135 0.014528901\n− −\nBLK 0.012711101 0.070032329 0.035371935 0.073370876 0.034410932\n− − −\nPF 0.012034143 0.109640846 0.223148274 0.862316681 0.364494150\n− −\nFG 0.003729350 0.002175469 0.001708722 0.006568270 0.001837634\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 256,
      "chunk_index": 1
    }
  },
  {
    "text": "− − − −\nFT 0.001210397 0.003852067 0.001793045 0.008110836 0.019134412\n− −\nX3P 0.003804597 0.003708648 0.001211492 0.002352869 0.003849550\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 256,
      "chunk_index": 2
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 257\nPC6 PC7 PC8 PC9 PC10\nPTS 0.029408534 0.0196304356 0.0026169995 0.004516521 0.004889708\n− −\nREB 0.040851345 0.0951099200 0.0074120623 0.003557921 0.008319362\n− − − −\nAST 0.044767132 0.0681222890 0.0359559264 0.056106512 0.015018370\n−\nTO 0.108917779 0.0864648004 0.0416005762 0.039363263 0.012726102\n− − −\nA.T 0.004846032 0.0061047937 0.7122315249 0.642496008 0.262468560\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 257,
      "chunk_index": 0
    }
  },
  {
    "text": "− − − −\nSTL 0.815509399 0.4981690905 0.0008726057 0.008845999 0.005846547\n− − − −\nBLK 0.516094006 0.8489313874 0.0023262933 0.001364270 0.008293758\n− −\nPF 0.228294830 0.0972181527 0.0005835116 0.001302210 0.001385509\n−\nFG 0.004118140 0.0041758373 0.0848448651 0.019610637 0.030860027\n−\nFT 0.005525032 0.0001301938 0.6189703010 0.761929615 0.174641147\n− − −\nX3P 0.001012866 0.0094289825 0.3151374823 0.038279107 0.948194531\n−\nPC11\nPTS 0.0037883918\nREB 0.0043776255\n−\nAST 0.0058744543\nTO 0.0001063247",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 257,
      "chunk_index": 1
    }
  },
  {
    "text": "−\nAST 0.0058744543\nTO 0.0001063247\n−\nA.T 0.0560584903\n−\nSTL 0.0062405867\n−\nBLK 0.0013213701\nPF 0.0043605809\n−\nFG 0.9956716097\n−\nFT 0.0731951151\n−\nX3P 0.0031976296\n−\n9.4.4 Application to Treasury Yield Curves\nWe had previously downloaded monthly data for constant maturity\n1976 2006 3\nyields from June to December . Here is the D plot. It shows\nthe change in the yield curve over time for a range of maturities.\n> persp(rates , theta= 30 ,phi= 0 ,xlab=\"years\" ,ylab=\"maturity\" ,zlab=\"rates\")",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 257,
      "chunk_index": 2
    }
  },
  {
    "text": "258 data science: theories, models, algorithms, and analytics\nAs before, we undertake a PCA of the system of Treasury rates. The\ncommands are the same as with the basketball data.\n> tryrates = read.table(\"tryrates.txt\",header=TRUE)\n> rates = as.matrix(tryrates[2:9])\n> result = princomp(rates)\n> result$loadings\nLoadings:\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8\nFYGM3 0.360 0.492 0.594 0.387 0.344\nFYGM6 −0.358 −0.404 −0.202 −0.795 0.156",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 258,
      "chunk_index": 0
    }
  },
  {
    "text": "FYGM6 −0.358 −0.404 −0.202 −0.795 0.156\nFYGT1 −0.388 −0.287 0.310 0.617 0.459 0.204 0.105 0.165\nFYGT2 −0.375 − −0.457 0.194 − 0.466 −0.304 −0.549\nFYGT3 −0.361 0.135 −0.365 −0.418 −0.142 −0.455 0.558\nFYGT5 −0.341 0.317 − −0.188 −0.724 0.199 −0.428\nFYGT7 −0.326 0.408 0.190 − 0.168 0.705 0.393\nFYGT10 −0.314 0.476 0.412 0.422 0.421 −0.356 −0.137\n− −\n> result$sdev\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7\n8.39745750 1.28473300 0.29985418 0.12850678 0.05470852 0.04626171 0.03991152\nComp.8",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 258,
      "chunk_index": 1
    }
  },
  {
    "text": "Comp.8\n0.02922175\n> summary(result)\nImportance of components:\nComp.1 Comp.2 Comp.3 Comp.4\nStandard deviation 8.397458 1.28473300 0.299854180 0.1285067846\nProportion of Variance 0.975588 0.02283477 0.001243916 0.0002284667\nCumulative Proportion 0.975588 0.99842275 0.999666666 0.9998951326\nComp.5 Comp.6 Comp.7 Comp.8\nStandard deviation 5.470852e 02 4.626171e 02 3.991152e 02 2.922175e 02\nProportion of Variance 4.140766e −05 2.960835e −05 2.203775e −05 1.181363e −05",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 258,
      "chunk_index": 2
    }
  },
  {
    "text": "Cumulative Proportion 9.999365e −01 9.999661e −01 9.999882e −01 1.000000e − +00\n− − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 258,
      "chunk_index": 3
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 259\nThe results are interesting. We see that the loadings are large in the\nfirst three component vectors for all maturity rates. The loadings corre-\nspond to a classic feature of the yield curve, i.e., there are three compo-\nnents: level, slope, and curvature. Note that the first component has al-\nmost equal loadings for all rates that are all identical in sign. Hence, this",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 259,
      "chunk_index": 0
    }
  },
  {
    "text": "is the level factor. The second component has negative loadings for the\nshorter maturity rates and positive loadings for the later maturity ones.\nTherefore, when this factor moves up, the short rates will go down, and\nthe long rates will go up, resulting in a steepening of the yield curve.\nIf the factor goes down, the yield curve will become flatter. Hence, the\nsecond principal component is clearly the slope factor. Examining the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 259,
      "chunk_index": 1
    }
  },
  {
    "text": "loadings of the third principal component should make it clear that the\neffect of this factor is to modulate the “curvature” or hump of the yield\n14 CLASSNOTES,S.DAS 17APRIL2007 97\ncurve. Still, from looking at the results, it is clear that % of the com-\nCumulative Proportion 0.975588 0.99842275 0.999666666 0.9998951326\nmon variation is explained by just the first factor, and a wee bit more by\nComp.5 Comp.6 Comp.7 Comp.8",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 259,
      "chunk_index": 2
    }
  },
  {
    "text": "Comp.5 Comp.6 Comp.7 Comp.8\nthe next tSwtaon.daTrdhedevrieastuiolntant “5.b4i7p08l5o2te”-0s2h4o.6w26s17t1hee-02do3.m99i1n15a2ne-c0e2o2.f9t2h21e75me-0a2in\nProportion of Variance 4.140766e-05 2.960835e-05 2.203775e-05 1.181363e-05\ncomponenCutm.ulative Proportion 9.999365e-01 9.999661e-01 9.999882e-01 1.000000e+00\nTheresultant“biplot”showsthedominanceofthemaincomponent.\nNoticethatthevariablesarealmostallequallyweightingonthefirstcomponent.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 259,
      "chunk_index": 3
    }
  },
  {
    "text": "Notice that the variables are almost all equally weighting on the first\n4.5. DifferencebetweenPCAandFA. ThedifferencebetweenPCAandFAis\ncomponenthta.tiTsthheatlfoernthgetphuropofsetshoefmvaetrcixtocormspcuotartiroensspPCoAnadsssumtoestthhaetafllavcatroiarncleoadings.\niscommon,withalluniquefactorssetequaltozero;whileFAassumesthatthere\nissomeuniquevariance. ThelevelofuniquevarianceisdictatedbytheFAmodel\nwhich is chosen. Accordingly, PCA is a model of a closed system, while FA is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 259,
      "chunk_index": 4
    }
  },
  {
    "text": "a model of an open system. FA tries to decompose the correlation matrix into\ncommonanduniqueportions.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 259,
      "chunk_index": 5
    }
  },
  {
    "text": "260 data science: theories, models, algorithms, and analytics\n9.4.5 Application: Risk Parity and Risk Disparity\nRisk parity – see Theirry Roncalli’s book\nRisk disparity – see Mark Kritzman’s paper.\n9.4.6 Difference between PCA and FA\nThe difference between PCA and FA is that for the purposes of matrix\ncomputations PCA assumes that all variance is common, with all unique\nfactors set equal to zero; while FA assumes that there is some unique",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 260,
      "chunk_index": 0
    }
  },
  {
    "text": "variance. Hence PCA may also be thought of as a subset of FA. The level\nof unique variance is dictated by the FA model which is chosen. Accord-\ningly, PCA is a model of a closed system, while FA is a model of an open\nsystem. FA tries to decompose the correlation matrix into common and\nunique portions.\n9.4.7 Factor Rotation\nFinally, there are some times when the variables would load better on\nthe factors if the factor system were to be rotated. This called factor rota-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 260,
      "chunk_index": 1
    }
  },
  {
    "text": "tion, and many times the software does this automatically.\nRemember that we decomposed variables x as follows:\nx = B F+e\nwhere x is dimension K, B RK M, F RM, and e is a K-dimension\n×\n∈ ∈\nvector. This implies that\nCov(x) = BB +ψ\n(cid:48)\nRecall that B is the matrix of factor loadings. The system remains un-\nchanged if B is replaced by BG, where G RM M, and G is orthogonal.\n×\n∈\nThen we call G a “rotation” of B.\nThe idea of rotation is easier to see with the following diagram. Two",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 260,
      "chunk_index": 2
    }
  },
  {
    "text": "conditions need to be satisfied: (a) The new axis (and the old one) should\nbe orthogonal. (b) The difference in loadings on the factors by each vari-\nable must increase. In the diagram below we can see that the rotation\nhas made the variables align better along the new axis system.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 260,
      "chunk_index": 3
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 261\nFactor Rotation\nFactor 2\nFactor 2\nFactor 1\nvariables\nFactor 1\n9.4.8 Using the factor analysis function\nTo illustrate, let’s undertake a factor analysis of the Treasury rates data.\nIn R, we can implement it generally with the factanal command.\n2\n> factanal (rates , )\nCall :\n2\nfactanal (x = rates , factors = )\nUniquenesses:\n3 6 1 2 3 5 7 10\nFYGM FYGM FYGT FYGT FYGT FYGT FYGT FYGT\n0 006 0 005 0 005 0 005 0 005 0 005 0 005 0 005",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 261,
      "chunk_index": 0
    }
  },
  {
    "text": "0 006 0 005 0 005 0 005 0 005 0 005 0 005 0 005\n. . . . . . . .\nLoadings:\n1 2\nFactor Factor\n3 0 843 0 533\nFYGM . .\n6 0 826 0 562\nFYGM . .\n1 0 793 0 608\nFYGT . .\n2 0 726 0 686\nFYGT . .\n3 0 681 0 731\nFYGT . .\n5 0 617 0 786\nFYGT . .\n7 0 579 0 814\nFYGT . .\n10 0 546 0 836\nFYGT . .\n1 2\nFactor Factor",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 261,
      "chunk_index": 1
    }
  },
  {
    "text": "262 data science: theories, models, algorithms, and analytics\n4 024 3 953\nSS loadings . .\n0 503 0 494\nProportion Var . .\n0 503 0 997\nCumulative Var . .\n2\nTest of the hypothesis that factors are sufficient .\nThe chi square statistic is 3556 . 38 on 13 degrees of freedom.\nThe p value is 0\n−\nNotice how the first factor explains the shorter maturities better and\nthe second factor explains the longer maturity rates. Hence, the two",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 262,
      "chunk_index": 0
    }
  },
  {
    "text": "factors cover the range of maturities. Note that the ability of the factors\nto separate the variables increases when we apply a factor rotation:\n2\n> factanal (rates , , rotation=\"promax\")\nCall :\n2\nfactanal (x = rates , factors = , rotation = \"promax\")\nUniquenesses:\n3 6 1 2 3 5 7 10\nFYGM FYGM FYGT FYGT FYGT FYGT FYGT FYGT\n0 006 0 005 0 005 0 005 0 005 0 005 0 005 0 005\n. . . . . . . .\nLoadings:\n1 2\nFactor Factor\n3 0 110 0 902\nFYGM . .\n6 0 174 0 846\nFYGM . .\n1 0 282 0 747\nFYGT . .\n2 0 477 0 560",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 262,
      "chunk_index": 1
    }
  },
  {
    "text": "FYGM . .\n1 0 282 0 747\nFYGT . .\n2 0 477 0 560\nFYGT . .\n3 0 593 0 443\nFYGT . .\n5 0 746 0 284\nFYGT . .\n7 0 829 0 194\nFYGT . .\n10 0 895 0 118\nFYGT . .\n1 2\nFactor Factor\n2 745 2 730\nSS loadings . .\n0 343 0 341\nProportion Var . .\n0 343 0 684\nCumulative Var . .\nThe factors have been reversed after the rotation. Now the first factor\nexplains long rates and the second factor explains short rates. If we want\nthe time series of the factors, use the following command:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 262,
      "chunk_index": 2
    }
  },
  {
    "text": "extracting dimensions: discriminant and factor analysis 263\n2\nresult = factanal (rates , , scores=\"regression\")\nts = result$scores\n> par(mfrow=c( 2 , 1 ))\n> plot( ts [ , 1 ] ,type=\"l\")\n> plot( ts [ , 2 ] ,type=\"l\")\nThe results are plotted here. The plot represents the normalized factor\ntime series.\nThus there appears to be a slow-moving first component and a fast mov-\ning second one.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 263,
      "chunk_index": 0
    }
  },
  {
    "text": "10\nBidding it Up: Auctions\n10.1 Theory\nAuctions comprise one of the oldest market forms, and are still a popu-\nlar mechanism for selling various assets and their related price discovery.\nIn this chapter we will study different auction formats, bidding theory,\nand revenue maximization principles.\n1 2002\nHal Varian, Chief Economist at Google (NYT, Aug , ) writes:\n“Auctions, one of the oldest ways to buy and sell, have been reborn\nand revitalized on the Internet.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 265,
      "chunk_index": 0
    }
  },
  {
    "text": "and revitalized on the Internet.\nWhen I say ”old,” I mean it. Herodotus described a Babylonian mar-\n500\nriage market, circa B.C., in which potential wives were auctioned off.\nNotably, some of the brides sold for a negative price.\nThe Romans used auctions for many purposes, including auctioning\n193\noff the right to collect taxes. In A.D. , the Praetorian Guards even\nauctioned off the Roman empire itself!\nWe don’t see auctions like this anymore (unless you count campaign",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 265,
      "chunk_index": 1
    }
  },
  {
    "text": "finance practices), but auctions are used for just about everything else.\nOnline, computer-managed auctions are cheap to run and have become\nincreasingly popular. EBay is the most prominent example, but other,\nless well-known companies use similar technology.”\n10.1.1 Overview\nAuctions have many features, but the key ingredient is information asym-\nmetry between seller and buyers. The seller may know more about the\nproduct than the buyers, and the buyers themselves might have differen-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 265,
      "chunk_index": 2
    }
  },
  {
    "text": "tial information about the item on sale. Moreover, buyers also take into",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 265,
      "chunk_index": 3
    }
  },
  {
    "text": "266 data science: theories, models, algorithms, and analytics\naccount imperfect information about the behavior of the other bidders.\nWe will examine how this information asymmetry plays into bidding\nstrategy in the mathematical analysis that follows.\nAuction market mechanisms are explicit, with the prices and revenue\na direct consequence of the auction design. In contrast, in other markets,\nthe interaction of buyers and sellers might be more implicit, as in the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 266,
      "chunk_index": 0
    }
  },
  {
    "text": "case of commodities, where the market mechanism is based on demand\nand supply, resulting in the implicit, proverbial invisible hand setting\nprices.\nThere are many examples of active auction markets, such as auctions\nof art and valuables, eBay, Treasury securities, Google ad auctions, and\neven the New York Stock Exchange, which is an example of a continuous\ncall auction market.\nAuctions may be for a single unit (e.g., art) or multiple units (e.g., Trea-\nsury securities).\n10.1.2 Auction types",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 266,
      "chunk_index": 1
    }
  },
  {
    "text": "sury securities).\n10.1.2 Auction types\nThe main types of auctions may be classified as follows:\n1\n. English (E): highest bid wins. The auction is open, i.e., bids are re-\nvealed to all participants as they occur. This is an ascending price\nauction.\n2\n. Dutch (D): auctioneer starts at a high price and calls out successively\nlower prices. First bidder accepts and wins the auction. Again, bids\nare open.\n3 1 1\n. st price sealed bid ( P): Bids are sealed. Highest bidder wins and\npays his price.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 266,
      "chunk_index": 2
    }
  },
  {
    "text": "pays his price.\n4 2 2 1\n. nd price sealed bid ( P): Same as P but the price paid by the winner\nis the second-highest price. Same as the auction analyzed by William\n1961\nVickrey in his seminal paper in that led to a Nobel prize. See\n1961\nVickrey ( ).\n5\n. Anglo-Dutch (AD): Open, ascending-price auction till only two bid-\nders remain, then it becomes sealed-bid.\n10.1.3 Value Determination\nThe eventual outcome of an auction is price/value discovery of the item",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 266,
      "chunk_index": 3
    }
  },
  {
    "text": "being sold. There are two characterizations of this value determination",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 266,
      "chunk_index": 4
    }
  },
  {
    "text": "bidding it up: auctions 267\nprocess, depending on the nature of the item being sold.\n1\n. Independent private values model: Each buyer bids his own indepen-\ndent valuation of the item at sale (as in regular art auctions).\n2\n. Common-values model: Bidders aim to discover a common price, as\nin Treasury auctions. This is because there is usually an after market\nin which common value is traded.\n10.1.4 Bidder Types\nThe assumptions made about the bidders impacts the revenue raised",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 267,
      "chunk_index": 0
    }
  },
  {
    "text": "in the auction and the optimal auction design chosen by the seller. We\nconsider two types of bidders.\n1\n. Symmetric: all bidders observe the same probability distribution of\nbids and stop-out (SP) prices. The stop out price is the price of the\nlowest winning bid for the last unit sold. This is a robust assumption\nwhen markets are competitive.\n2\n. Asymmetric or non-symmetric. Here the bidders may have different\ndistributions of value. This is often the case when markets are seg-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 267,
      "chunk_index": 1
    }
  },
  {
    "text": "mented. Example: bidding for firms in M&A deals.\n10.1.5 Benchmark Model (BM)\nWe begin by analyzing what is known as the benchmark model. It is the\n4\nsimplest framework in which we can analyze auctions. It is based on\nmain assumptions:\n1\n. Risk-neutrality of bidders. We do not need utility functions in the\nanalysis.\n2\n. Private-values model. Every bidder has her own value for the item.\nThere is a distribution of bidders’ private values.\n3",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 267,
      "chunk_index": 2
    }
  },
  {
    "text": "3\n. Symmetric bidders. Every bidder faces the same distribution of pri-\nvate values mentioned in the previous point.\n4\n. Payment by winners is a function of bids alone. For a counterexam-\nple, think of payment via royalties for a book contract which depends\non post auction outcomes. Or the bidding for movie rights, where the\nbuyer takes a part share of the movie with the seller.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 267,
      "chunk_index": 3
    }
  },
  {
    "text": "268 data science: theories, models, algorithms, and analytics\nThe following are the results and properties of the BM.\n1 1\n. D = P. That is, the Dutch auction and first price auction are equiva-\nlent to bidders. These two mechanisms are identical because in each\nthe bidder needs to choose how high to bid without knowledge of the\nother bids.\n2\n. In the BM, the optimal strategy is to bid one’s true valuation. This is\n1\neasy to see for D and P. In both auctions, you do not see any other",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 268,
      "chunk_index": 0
    }
  },
  {
    "text": "lower bids, so you bid up to your maximum value, i.e., one’s true\n2\nvalue, and see if the bid ends up winning. For P, if you bid too high\nyou overpay, bid too low you lose, so best to bid one’s valuation. For\nE, it’s best to keep bidding till price crosses your valuation (reserva-\ntion price).\n3\n. Equilibria types:\n• Dominant: A situation where bidders bid their true valuation irre-\n2\nspective of other bidders bids. Satisfied by E and P.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 268,
      "chunk_index": 1
    }
  },
  {
    "text": "• Nash: Bids are chosen based on the best guess of other bidders’\n1\nbids. Satisfied by D and P.\n10.2 Auction Math\nWe now get away from the abstract definition of different types of auc-\ntions and work out an example of an auctions equilibrium.\nLet F be the probability distribution of the bids. And define v as the\ni\ntrue value of the i-th bidder, on a continuum between 0 and 1 . Assume\nbidders are ranked in order of their true valuations v . How do we inter-\ni",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 268,
      "chunk_index": 2
    }
  },
  {
    "text": "i\npret F(v)? Think of the bids as being drawn from say, a beta distribution\nF on v (0,1), so that the probability of a very high or very low bid\n∈\nis lower than a bid around the mean of the distribution. The expected\ndifference between the first and second highest bids is, given v and v :\n1 2\nD = [1 F(v )](v v )\n2 1 2\n− −\nThat is, multiply the difference between the first and second bids by the\nprobability that v is the second-highest bid. Or think of the probability\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 268,
      "chunk_index": 3
    }
  },
  {
    "text": "2\nof there being a bid higher than v . Taking first-order conditions (from\n2\nthe seller’s viewpoint):\n∂D\n= [1 F(v 2 )] (v 1 v 2 )F (cid:48) (v 1 ) = 0\n∂v − − −\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 268,
      "chunk_index": 4
    }
  },
  {
    "text": "bidding it up: auctions 269\nNote that v d v , given bidders are symmetric in BM. The symbol d\n1 2\n≡ ≡\nmeans “equivalent in distribution”. This implies that\n1 F(v )\nv v = − 1\n1 − 2 f(v )\n1\n2\nThe expected revenue to the seller is the same as the expected nd price.\nThe second price comes from the following re-arranged equation:\n1 F(v )\nv = v − 1\n2 1 − f(v )\n1\n10.2.1 Optimization by bidders\nThe goal of bidder i is to find a function/bidding rule B that is a func-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 269,
      "chunk_index": 0
    }
  },
  {
    "text": "tion of the private value v such that\ni\nb = B(v )\ni i\nwhere b is the actual bid. If there are n bidders, then\ni\nPr[bidder i wins] = Pr[b > B(v )], j = i,\ni j\n∀ (cid:54)\n= [F(B 1(b ))]n 1\n− i −\nEach bidder tries to maximize her expected profit relative to her true\nvaluation, which is\nπ i = (v i b i )[F(B − 1(b i ))]n − 1 = (v i b i )[F(v i )]n − 1, ( 10 . 1 )\n− −\nagain invoking the notion of bidder symmetry. Optimize by taking ∂πi =\n∂b\ni",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 269,
      "chunk_index": 1
    }
  },
  {
    "text": "∂b\ni\n0. We can get this by taking first the total derivative of profit relative to\nthe bidder’s value as follows:\ndπ ∂π ∂π db ∂π\ni = i + i i = i\ndv ∂v ∂b dv ∂v\ni i i i i\nwhich reduces to the partial derivative of profit with respect to personal\nvaluation because ∂πi = 0. This useful first partial derivative is taken\n∂b\ni\n101\nfrom equation ( . ):\n∂π\ni = [F(B 1(b ))]n 1\n− i −\n∂v\ni\nNow, let v be the lowest bid. Integrate the previous equation to get\nl\n(cid:90) v\nπ i = i [F(x)]n − 1 dx ( 10 . 2 )\nv",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 269,
      "chunk_index": 2
    }
  },
  {
    "text": "l\n(cid:90) v\nπ i = i [F(x)]n − 1 dx ( 10 . 2 )\nv\nl",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 269,
      "chunk_index": 3
    }
  },
  {
    "text": "270 data science: theories, models, algorithms, and analytics\n101 102\nEquating ( . ) and ( . ) gives\n(cid:82)v\ni[F(x)]n 1 dx\nv −\nb = v l = B(v )\ni i − [F(v )]n 1 i\ni −\nwhich gives the bidding rule B(v ) entirely in terms of the personal valu-\ni\nation of the bidder. If, for example, F is uniform, then\n(n 1)v\nB(v) = −\nn\nHere we see that we “shade” our bid down slightly from our personal\nvaluation. We bid less than true valuation to leave some room for profit.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 270,
      "chunk_index": 0
    }
  },
  {
    "text": "The amount of shading of our bid depends on how much competition\nthere is, i.e., the number of bidders n. Note that\n∂B ∂B\n> 0, > 0\n∂v ∂n\ni\ni.e., you increase your bid as your personal value rises, and as the num-\nber of bidders increases.\n10.2.2 Example\nWe are bidding for a used laptop on eBay. Suppose we assume that the\n50\ndistribution of bids follows a beta distribution with minimum value $\n500\nand a maximum value of $ . Our personal value for the machine is\n300 10",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 270,
      "chunk_index": 1
    }
  },
  {
    "text": "300 10\n$ . Assume other bidders. How much should we bid?\nx = (\n1\n:\n1000\n)\n/1000\n450 50\ny = x* +\nprob_y = dbeta(x, 2 , 4 )\nprint(c(\"check=\" ,sum(prob_y) / 1000 ))\nprob_y = prob_y/sum(prob_y)\nplot(y,prob_y,type=\"l\")\n> print(c(\"check=\" ,sum(prob_y) / 1000 ))\n1 0 999998333334\n[ ] \"check=\" \" . \"\nNote that we have used the non-central Beta distribution, with shape\nparameters a = 2 and b = 4. Note that the Beta density function is\nΓ(a+b)\nBeta(x,a,b) =\nΓ(a)Γ(b)\nxa\n−\n1(1\n−\nx)b\n−\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 270,
      "chunk_index": 2
    }
  },
  {
    "text": "Γ(a+b)\nBeta(x,a,b) =\nΓ(a)Γ(b)\nxa\n−\n1(1\n−\nx)b\n−\n1\nfor x taking values between 0 and 1 . The distribution of bids from 50 to\n500 101\nis shown in Figure . . The mean and standard deviation are\ncomputed as follows.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 270,
      "chunk_index": 3
    }
  },
  {
    "text": "bidding it up: auctions 271\nFigure10.1: Probabilitydensity\nfunctionfortheBeta(a =2,b =4)\ndistribution.\n> print(c(\"mean=\" ,sum(y*prob_y)))\n1 200 000250000167\n[ ] \"mean=\" \" . \"\n> print(c(\"stdev=\" ,sqrt(sum(y^ 2 *prob_y) (sum(y*prob_y))^ 2 )))\n−\n1 80 1782055353774\n[ ] \"stdev=\" \" . \"\nWe can take a computational approach to solving this problem. We pro-\n101\ngram up equation . and then find the bid at which this is maximized.\n> x = (\n1\n:\n1000\n)\n/1000\n50 450\n> y = + *x\n> cumprob_y = pbeta(x, 2 , 4 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 271,
      "chunk_index": 0
    }
  },
  {
    "text": "50 450\n> y = + *x\n> cumprob_y = pbeta(x, 2 , 4 )\n> exp_profit = ( 300 y)*cumprob_y^ 10\n−\n> idx = which(exp_profit==max(exp_profit ))\n> y[idx]\n1 271 85\n[ ] .\n27185\nHence, the bid of . is slightly lower than the reservation price. It is\n10 5\n% lower. If there were only other bidders, then the bid would be:\n> exp_profit = ( 300 y)*cumprob_y^ 5\n−\n> idx = which(exp_profit==max(exp_profit ))\n> y[idx]\n1 254 3\n[ ] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 271,
      "chunk_index": 1
    }
  },
  {
    "text": "272 data science: theories, models, algorithms, and analytics\nNow, we shade the bid down much more, because there are fewer com-\npeting bidders, and so the chance of winning with a lower bid increases.\n10.3 Treasury Auctions\nThis section is based on the published paper by Das and Sundaram\n1996\n( ). We move on from single-unit auctions to a very common multi-\nunit auction. Treasury auctions are the mechanism by which the Federal",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 272,
      "chunk_index": 0
    }
  },
  {
    "text": "government issues its bills, notes, and bonds. Auctions are usually held\non Wednesdays. Bids are received up to early afternoon after which the\ntop bidders are given their quantities requested (up to prescribed ceil-\nings for any one bidder), until there is no remaining supply of securities.\nEven before the auction, Treasury securities trade in what is known as\na “when-issued” or pre-market. This market gives early indications of\nprice that may lead to tighter clustering of bids in the auction.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 272,
      "chunk_index": 1
    }
  },
  {
    "text": "There are two types of dealers in a Treasury auction, primary dealers,\ni.e., the big banks and investment houses, and smaller independent bid-\nders. The auction is really played out amongst the primary dealers. They\nplace what are known as competitive bids versus the others, who place\nnon-competitive bids.\nBidders also keep an eye on the secondary market that ensues right\nafter the auction. In many ways, the bidders are also influenced by the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 272,
      "chunk_index": 2
    }
  },
  {
    "text": "possible prices they expect the paper to be trading at in the secondary\nmarket, and indicators of these prices come from the when-issued mar-\nket.\nThe winner in an auction experiences regret, because he knows he\nbid higher than everyone else, and senses that he overpaid. This phe-\nnomenon is known as the “winner’s curse.” Treasury auction partici-\npants talk amongst each other to mitigate winner’s curse. The Fed also\ntalks to primary dealers to mitigate their winner’s curse and thereby",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 272,
      "chunk_index": 3
    }
  },
  {
    "text": "induce them to bid higher, because someone with lower propensity for\nregret is likely to bid higher.\n10.3.1 DPA or UPA?\nDPA stands for “discriminating price auction” and UPA for “uniform\nprice auction.” The former was the preferred format for Treasury auc-\ntions and the latter was introduced only recently.\nIn a DPA, the highest bidder gets his bid quantity at the price he bid.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 272,
      "chunk_index": 4
    }
  },
  {
    "text": "bidding it up: auctions 273\nThen the next highest bidder wins his quantity at the price he bid. And\nso on, until the supply of Treasury securities is exhausted. In this man-\nner the Treasury seeks to maximize revenue by filling each winning at\nthe price. Since the prices paid by each winning bidder are different,\nthe auction is called “discriminating” in price. Revenue maximization\n102\nis attempted by walking down the demand curve, see Figure . . The\nshaded area quantifies the revenue raised.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 273,
      "chunk_index": 0
    }
  },
  {
    "text": "shaded area quantifies the revenue raised.\nFigure10.2: RevenueintheDPA\nandUPAauctions.\nIn a UPA, the highest bidder gets his bid quantity at the price of the\nlast winning bid (this price is also known as the stop-out price). Then\nthe next highest bidder wins his quantity at the stop-out price. And so\non, until the supply of Treasury securities is exhausted. Thus, the UPA\n102\nis also known as a “single-price” auction. See Figure . , lower panel,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 273,
      "chunk_index": 1
    }
  },
  {
    "text": "where the shaded area quantifies the revenue raised.\nIt may intuitively appear that the DPA will raise more revenue, but\nin fact, empirically, the UPA has been more successful. This is because\nthe UPA incentivizes higher bids, as the winner’s curse is mitigated. In\na DPA, bids are shaded down on account of winner’s curse – winning\nmeans you paid higher than what a large number of other bidders were\nwilling to pay.\nSome countries like Mexico have used the UPA format. The U.S.,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 273,
      "chunk_index": 2
    }
  },
  {
    "text": "started with the DPA, and now runs both auction formats.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 273,
      "chunk_index": 3
    }
  },
  {
    "text": "274 data science: theories, models, algorithms, and analytics\nAn interesting study examined markups achieved over yields in the\nwhen-issued market as an indicator of the success of the two auction for-\n2 5 1991\nmats. They examined the auctions of - and -year notes from June\n1994\n- ). [from Mulvey, Archibald and Flynn, US Office of the Treasury].\n103\nSee Figure . . The results of a regression of the markups on bid dis-\npersion and duration of the auctioned securities shows that markups",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 274,
      "chunk_index": 0
    }
  },
  {
    "text": "Mulvey, Archibald, Flynn\ninc(Oreffiaces oef Uins Trtehaseurdy)ispersion of bids. If we think of bid dispersion as a\nproxy for the extent of winner’s curse, then we can see that the yields\nare pushed higher in the UPA than the DPA, therefore prices are lower\nin the UPA than the DPA. Markups are decreasing in the duration of the\n104\nsecurities. Bid dispersion is shown in Figure . .\nFigure10.3: Treasuryauction\nmarkups.\n10.4 Mechanism Design",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 274,
      "chunk_index": 1
    }
  },
  {
    "text": "markups.\n10.4 Mechanism Design\nWhat is a good auction mechanism? The following features might be\nconsidered.\n• It allows easy entry to the game.\n• It prevents collusion. For example, ascending bid auctions may be\nused to collude by signaling in the early rounds of bidding. Different\nauction formats may lead to various sorts of collusion.\n• It induces truthful value revelation (also known as “truthful” bid-\nding).\n• Efficient - maximizes utility of auctioneer and bidders.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 274,
      "chunk_index": 2
    }
  },
  {
    "text": "• Not costly to implement.\n• Fair to all parties, big and small.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 274,
      "chunk_index": 3
    }
  },
  {
    "text": "bidding it up: auctions 275\nFigure10.4: Bid-AskSpreadinthe\nAuction.\n10.4.1 Collusion\nHere are some examples of collusion in auctions, which can be explicit\nor implicit. Collusion amongst buyers results in mitigating the winner’s\ncurse, and may work to either raise revenues or lower revenues for the\nseller.\n1999\n• (Varian) : German phone spectrum auction. Bids had to be in\n10 1818\nminimum % increments for multiple units. A firm bid . and\n20 2 20",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 275,
      "chunk_index": 0
    }
  },
  {
    "text": "20 2 20\nmillion for lots. They signaled that everyone could center at\nmillion, which they believed was the fair price. This sort of implicit\ncollusion averts a bidding war.\n• In Treasury auctions, firms can discuss bids, which is encouraged by\nthe Treasury (why?). The restriction on cornering by placing a ceiling\non how much of the supply one party can obtain in the auction aids\ncollusion (why?). Repeated games in Treasury security auctions also\naids collusion (why?).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 275,
      "chunk_index": 1
    }
  },
  {
    "text": "aids collusion (why?).\n• Multiple units also allows punitive behavior, by firms bidding to raise\nprices on lots they do not want to signal others should not bid on lots\nthey do want.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 275,
      "chunk_index": 2
    }
  },
  {
    "text": "276 data science: theories, models, algorithms, and analytics\n10.4.2 Clicks (Advertising Auctions)\nThe Google AdWords program enables you to create advertisements\nwhich will appear on relevant Google search results pages and our net-\nwork of partner sites. See www.adwords.google.com.\nThe Google AdSense program differs in that it delivers Google Ad-\nWords ads to individuals’ websites. Google then pays web publishers\nfor the ads displayed on their site based on user clicks on ads or on ad",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 276,
      "chunk_index": 0
    }
  },
  {
    "text": "impressions, depending on the type of ad.\nThe material here refers to the elegant paper by Aggarwal, Goel, and\n2006\nMotwani ( ) on keyword auctions in AdWords. We first list some\nbasic features of search engine advertising models. Aggarwal went on\nto work for Google as they adopted this algorithm from her thesis at\nStanford.\n1\n. Search engine advertising uses three models: (a) CPM, cost per thou-\nsand views, (b) CPC, cost per click, and (c) CPA, cost per acquisition.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 276,
      "chunk_index": 1
    }
  },
  {
    "text": "These are all at different stages of the search page experience.\n2 2\n. CPC seems to be mostly used. There are models here:\n(a) Direct ranking: the Overture model.\n(b) Revenue ranking: the Google model.\n3\n. The merchant pays the price of the “next” click (different from “sec-\nond” price auctions). This is non-truthful in both revenue ranking\ncases as we will see in a subsequent example. That is, bidders will not\nbid their true private valuations.\n4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 276,
      "chunk_index": 2
    }
  },
  {
    "text": "bid their true private valuations.\n4\n. Asymmetric: there is an incentive to underbid, none to overbid.\n5\n. Iterative: by placing many bids and watching responses, a bidder can\nfigure out the bid ordering of other bidders for the same keywords, or\nbasket of keywords. However, this is not obvious or simple. Google\nused to provide the GBS or Google Bid Simulator so that sellers using\nAdWords can figure out their optimal bids. See the following for more\ndetails on Adwords: google.com/adwords/.\n6",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 276,
      "chunk_index": 3
    }
  },
  {
    "text": "details on Adwords: google.com/adwords/.\n6\n. If revenue ranking were truthful, it would maximize utility of auction-\neer and merchant. (Known as auction “efficiency”).\n7 . Innovation: the laddered auction. Randomized weights attached to bids.\n1\nIf weights are , then it’s direct ranking. If weights are CTR (click-\nthrough rate), i.e. revenue-based, it’s the revenue ranking.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 276,
      "chunk_index": 4
    }
  },
  {
    "text": "bidding it up: auctions 277\nTo get some insights about the process of optimal bidding in AdWords\nauctions, see http://www.thesearchagents.com/2009/09/optimal-bidding-part-1-behind-the\n-scenes-of-google-adwords-bidding-tutorial/. See the Hal Varian\nvideo: http://www.youtube.com/watch?v=jRx7AMb6rZ0.\nHere is a quick summary of Hal Varian’s video. A merchant can figure\nout what the maximum bid per click should be in the following steps:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 277,
      "chunk_index": 0
    }
  },
  {
    "text": "1 . Maximum profitable CPA: This is the profit margin on the product. For\n300 200\nexample, if the selling price is $ and cost is $ , then the profit\n100\nmargin is $ , which is also the maximum cost per acquisition (CPA)\na seller would pay.\n2 . Conversion Rate (CR): This is the number of times a click results in a\nsale. Hence, CR is equal to number of sales divided by clicks. So, if for\n100 5 5\nevery clicks, we get a sale every times, the CR is %.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 277,
      "chunk_index": 1
    }
  },
  {
    "text": "3 . Value per Click (VPC): Equal to the CR times the CPA. In the example,\nwe have VPC = 0.05 100 = $5.\n×\n4 . Determine the profit maximizing CPC bid: As the bid is lowered, the\nnumber of clicks falls, but the CPC falls as well, revenue falls, but\nthe profit after acquisition costs can rise until the sweet spot is deter-\nmined. To find the number of clicks expected at each bid price, use the\nGoogle Bid Simulator. See the table below (from Google) for the eco-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 277,
      "chunk_index": 2
    }
  },
  {
    "text": "nomics at different bid prices. Note that the price you bid is not the\nprice you pay for the click, because it is a “next-price” auction, based\non a revenue ranking model, so the exact price you pay is based on\nGoogle’s model, discussed in the next section. We see that the profit is\n4\nmaximized at a bid of $ .\nJust as an example, note that the profit is equal to\n(VPC CPC) #Clicks = (CPA CR CPC) #Clicks\n− × × − ×\n4\nHence, for a bid of $ , we have\n(5 407.02/154) 154 = $362.98\n− ×",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 277,
      "chunk_index": 3
    }
  },
  {
    "text": "278 data science: theories, models, algorithms, and analytics\nAs pointed out by Varian, the rule is to compute ICC (Incremental cost\nper click), and make sure that it equals the VPC. The ICC at a bid of\n500\n$ . is\n697.42 594.27\nICC(5.00) = − = 5.73 > 5\n208 190\n−\nThen\n594.27 407.02\nICC(4.50) = − = 5.20 > 5\n190 154\n−\n407.02 309.73\nICC(4.00) = − = 4.63 < 5\n154 133\n−\n400 450\nHence, the optimal bid lies between $ . and $ . .\n10.4.3 Next Price Auctions",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 278,
      "chunk_index": 0
    }
  },
  {
    "text": "10.4.3 Next Price Auctions\nIn a next-price auction, the CPC is based on the price of the click next\nafter your own bid. Thus, you do not pay your bid price, but the one in\nthe advertising slot just lower than yours. Hence, if your winning bid is\nfor position j on the search screen, the price paid is that of the winning\nbid at position j+1.\n2006\nSee the paper by Aggarwal, Goyal and Motwani ( ). Our discus-\nsion here is based on their paper. Let the true valuation (revenue) ex-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 278,
      "chunk_index": 1
    }
  },
  {
    "text": "pected by bidder/seller i be equal to v . The CPC is denoted p . Let the\ni i\nclick-through-rate (CTR) for seller/merchant i at a position j (where the\nad shows up on the search screen) be denoted CTR . CTR is the ratio of\nij\nthe number of clicks to the number of “impressions” i.e., the number of\ntimes the ad is shown.\n• The “utility” to the seller is given by\nUtility = CTR (v p )\nij i i\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 278,
      "chunk_index": 2
    }
  },
  {
    "text": "bidding it up: auctions 279\n• Example: 3 bidders A, B, C, with private values 200 , 180 , 100 . There\nare two slots or ad positions with CTRs 0 . 5 and 0 . 4 . If bidder A bids\n200 , pays 180 , utility is (200 180) 0.5 = 10. But why not bid 110 , for\n− ×\nutility of (200 100) 0.4 = 40? This simple example shows that the\n− ×\nnext price auction is not truthful. Also note that your bid determines\nyour ranking but not the price you pay (CPC).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 279,
      "chunk_index": 0
    }
  },
  {
    "text": "your ranking but not the price you pay (CPC).\n• Ranking of bids is based on w b in descending order of i. If w = 1,\ni i i\nthen we get the Overture direct ranking model. And if w = CTR\ni ij\nthen we have Google’s revenue ranking model. In the example below,\n0 100 0 1\nthe weights range from to , not to , but this is without any loss\nof generality. The weights assigned to each merchant bidder may be\nbased on some qualitative ranking such as the Quality Score (QS) of\nthe ad.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 279,
      "chunk_index": 1
    }
  },
  {
    "text": "the ad.\n• Price paid by bidder i is w i+1 b i+1.\nw\ni\n• Separable CTRs: CTRs of merchant i = 1 and i = 2 are the same for\nposition j. No bidder position dependence.\n10.4.4 Laddered Auction\n2006\nAGM denoted the revised auction as “laddered”. It gives a unique\ntruthful auction. The main idea is to set the CPC to\nK (cid:18) CTR CTR (cid:19) w b\n∑ i,j i,j+1 j+1 j+1\np = − , 1 i K\ni\nCTR w ≤ ≤\nj=i i,i i\nso that\n#Clicks i p = CTR p = ∑ K (cid:0) CTR CTR (cid:1) w j+1 b j+1\ni ii i i,j i,j+1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 279,
      "chunk_index": 2
    }
  },
  {
    "text": "i ii i i,j i,j+1\n#Impressions × × − w\ni j=i i\nThe lhs is the expected revenue to Google per ad impression. Make\nno mistake, the whole point of the model is to maximize Google’s rev-\nenue, while making the auction system more effective for merchants. If\nthis new model results in truthful equilibria, it is good for Google. The\nweights w are arbitrary and not known to the merchants.\ni\nHere is the table of CTRs for each slot by seller. These tables are the\n2006\nexamples in the AGM paper.\nA B C D",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 279,
      "chunk_index": 3
    }
  },
  {
    "text": "2006\nexamples in the AGM paper.\nA B C D\n1 040 035 030 020\nSlot . . . .\n2 030 025 025 018\nSlot . . . .\n3 018 020 020 015\nSlot . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 279,
      "chunk_index": 4
    }
  },
  {
    "text": "280 data science: theories, models, algorithms, and analytics\nThe assigned weights and the eventual allocations and prices are shown\nbelow.\nWeight Bid Score Rank Price\n60 25 1500 1 135\nMerchant A .\n40 30 1200 2 16\nMerchant B\n50 16 800 3 12\nMerchant C\n40 15 600 4 0\nMerchant D\nWe can verify these calculations as follows.\n> p 3 = ( 0 . 20 0 )/ 0 . 20 * 40/50 * 15 ; p 3\n−\n1 12\n[ ]\n> p 2 = ( 0 . 25 0 . 20 ) / 0 . 25 * 50/40 * 16 + ( 0 . 20 0 )/ 0 . 25 * 40/40 * 15 ; p 2\n− −\n1 16\n[ ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 280,
      "chunk_index": 0
    }
  },
  {
    "text": "− −\n1 16\n[ ]\n> p 1 = ( 0 . 40 0 . 30 ) / 0 . 40 * 40/60 * 30 + ( 0 . 30 0 . 18 ) / 0 . 40 * 50/60 * 16\n− −\n+ ( 0 . 18 0 )/ 0 . 40 * 40/60 * 15 ; p 1\n−\n1 13 5\n[ ] .\nSee the paper for more details, but this equilibrium is unique and truth-\nful.\nLooking at this model, examine the following questions:\n• What happens to the prices paid when the CTR drop rapidly as we go\ndown the slots versus when they drop slowly?\n• As a merchant, would you prefer that your weight be higher or lower?",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 280,
      "chunk_index": 1
    }
  },
  {
    "text": "• What is better for Google, a high dispersion in weights, or a low dis-\npersion in weights?\n• Can you see that by watching bidding behavior of the merchants,\nGoogle can adjust their weights to maximize revenue? By seeing a\nweek’s behavior Google can set weights for the next week. Is this le-\ngal?\n• Is Google better off if the bids are more dispersed than when they\nare close together? How would you use the data in the table above to\nanswer this question using R?\nExercise",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 280,
      "chunk_index": 2
    }
  },
  {
    "text": "answer this question using R?\nExercise\nWhereas Google clearly has modeled their AdWords auction to maxi-\nmize revenue, less is known about how merchants maximize their net",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 280,
      "chunk_index": 3
    }
  },
  {
    "text": "bidding it up: auctions 281\nrevenue per ad, by designing ads, and choosing keywords in an appro-\npriate manner. Google offers merchants a product called “Google Bid\nSimulator” so that the return from an adword (key word) may be deter-\nmined.\nIn this exercise, you will first take the time to role play a merchant\nwho is trying to explore and understand AdWords, and then come up\nwith an approach to maximize the return from a portfolio of AdWords.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 281,
      "chunk_index": 0
    }
  },
  {
    "text": "Here are some questions that will help in navigating the AdWords\nlandscape.\n1\n. What is the relation between keywords and cost-per-click (CPC)?\n2\n. What is the Quality Score (QS) of your ad, and how does it related to\nkeywords and CPC?\n3\n. What defines success in an ad auction? What are its determinants?\n4\n. What is AdRank. What does a higher AdRank buy for a merchant?\n5\n. What are AdGroups and how do they relate to keywords?\n6\n. What is automated CPC bidding?\n7",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 281,
      "chunk_index": 1
    }
  },
  {
    "text": "6\n. What is automated CPC bidding?\n7\n. What are the following tools? Keyword tool, Traffic estimator, Place-\nment tool, Contextual targeting tool?\n8\n. What is the incremental cost-per-click (ICC)?\nSketch a brief outline of how you might go about optimizing a port-\nfolio of AdWords. Use the concepts we studied in Markowitz portfolio\noptimization for this.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 281,
      "chunk_index": 2
    }
  },
  {
    "text": "11\nTruncate and Estimate: Limited Dependent Variables\n11.1 Introduction\nUsually we run regressions using continuous variables for the dependent\n(y) variables, such as, for example, when we regress income on educa-\ntion. Sometimes however, the dependent variable may be discrete, and\ncould be binomial or multinomial. That is, the dependent variable is\n“limited”. In such cases, we need a different approach.\nDiscrete dependent variables are a special case of limited dependent vari-\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 283,
      "chunk_index": 0
    }
  },
  {
    "text": "1\nables. The Logit and Probit models we look at here are examples of 1Thesearecommonusageanddonot\nneedtobecapitalized,sowewilluse\ndiscrete dependent variable models. Such models are also often called\nlowercasesubsequently.\nqualitative response (QR) models.\nIn particular, when the variable is binary, i.e., takes values of 0,1 ,\n{ }\nthen we get a probability model. If we just regressed left hand side vari-\nables of ones and zeros on a suite of right hand side variables we could",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 283,
      "chunk_index": 1
    }
  },
  {
    "text": "of course fit a linear regression. Then if we took another observation\nwith values for the right hand side, i.e., x = x ,x ,...,x , we could\n1 2 k\n{ }\ncompute the value of the y variable using the fitted coefficients. But of\n0 1\ncourse, this value will not be exactly or , except by unlikely coinci-\ndence. Nor will this value lie in the range (0,1).\nThere is also a relationship to classifier models. In classifier models,\nwe are interested in allocating observations to categories. In limited de-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 283,
      "chunk_index": 2
    }
  },
  {
    "text": "pendent models we also want to explain the reasons (i.e., find explana-\ntory variables) for what results in the allocation across categories.\nSome examples of such models are to explain whether a person is\nemployed or not, whether a firm is syndicated or not, whether a firm is\nsolvent or not, which field of work is chosen by graduates, where con-\nsumers shop, whether they choose Coke versus Pepsi, etc.\n0 1\nThese fitted values might not even lie between and with a linear",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 283,
      "chunk_index": 3
    }
  },
  {
    "text": "284 data science: theories, models, algorithms, and analytics\nregression. However, if we used a carefully chosen nonlinear regression\nfunction, then we could ensure that the fitted values of y are restricted\nto the range (0,1), and then we would get a model where we fitted a\nprobability. There are two such model forms that are widely used: (a)\nLogit, also known as a logistic regression, and (b) Probit models. We\nlook at each one in turn.\n11.2 Logit\nA logit model takes the following form:\nef(x)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 284,
      "chunk_index": 0
    }
  },
  {
    "text": "A logit model takes the following form:\nef(x)\ny = , f(x) = β +β x +...β x\n1+ef(x) 0 1 1 k k\nWe are interested in fitting the coefficients β ,β ,...,β . Note that,\n0 1 k\n{ }\nirrespective of the coefficients, f(x) ( ∞ ,+∞), but y (0,1). When\n∈ − ∈\nf(x) ∞ , y 0, and when f(x) +∞ , y 1. We also write this\n→ − → → →\nmodel as\neβ(cid:48)x\ny = Λ(β x)\n(cid:48)\n1+eβ (cid:48) x ≡\nΛ\nwhere (lambda) is for logit.\nThe model generates a S-shaped curve for y, and we can plot it as\nfollows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 284,
      "chunk_index": 1
    }
  },
  {
    "text": "follows:\nThe fitted value of y is nothing but the probability that y = 1.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 284,
      "chunk_index": 2
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 285\n32\nFor the NCAA data, take the top teams and make their dependent\n1 32\nvariable , and that of the bottom teams zero.\n1 1 32\n> y = :\n1 1 0 1\n> y = y * +\n1\n> y\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[ ]\n2 1 0\n> y = y *\n2\n> y\n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[ ]\n> y = c(y 1 ,y 2 )\n> y\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n[ ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 285,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ]\n39 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[ ]\n> x = as.matrix(ncaa[ 4 : 14 ])\nThen running the model is pretty easy as follows:\n> h = glm(y~x, family=binomial(link=\" logit \" ))\n> logLik(h)\n’log Lik. ’ 21 . 44779 (df= 12 )\n−\n> summary(h)\nCall :\nglm(formula = y ~ x, family = binomial(link = \" logit \" ))\nDeviance Residuals :\nMin\n1Q\nMedian\n3Q\nMax\n1 80174 0 40502 0 00238 0 37584 2 31767\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error z value Pr(>|z|)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 285,
      "chunk_index": 1
    }
  },
  {
    "text": "Estimate Std. Error z value Pr(>|z|)\n45 83315 14 97564 3 061 0 00221\n( Intercept ) . . . . **\n− −\n0 06127 0 09549 0 642 0 52108\nxPTS . . . .\n− −\n0 49037 0 18089 2 711 0 00671\nxREB . . . . **\n0 16422 0 26804 0 613 0 54010\nxAST . . . .\n0 38405 0 23434 1 639 0 10124\nxTO . . . .\n− −\n1 56351 3 17091 0 493 0 62196\nxA.T . . . .\n0 78360 0 32605 2 403 0 01625\nxSTL . . . . *\n0 07867 0 23482 0 335 0 73761\nxBLK . . . .\n0 02602 0 13644 0 191 0 84874\nxPF . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 285,
      "chunk_index": 2
    }
  },
  {
    "text": "286 data science: theories, models, algorithms, and analytics\n46 21374 17 33685 2 666 0 00768\nxFG . . . . **\n10 72992 4 47729 2 397 0 01655\nxFT . . . . *\n3 5 41985 5 77966 0 938 0 34838\nxX P . . . .\n−−−\nSignif . codes: 0 L’ 0 . 001 L’ 0 . 01 L’ 0 . 05 L’ 0 . 1 L’ 1\n(Dispersion parameter for binomial family taken to be 1 )\nNull deviance: 88 . 723 on 63 degrees of freedom\nResidual deviance: 42 . 896 on 52 degrees of freedom\n66 896\nAIC: .\n6\nNumber of Fisher Scoring iterations :",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 286,
      "chunk_index": 0
    }
  },
  {
    "text": "AIC: .\n6\nNumber of Fisher Scoring iterations :\nSuppose we ran this just with linear regression (this is also known as\nrunning a linear probability model):\n> h = lm(y~x)\n> summary(h)\nCall :\nlm(formula = y ~ x)\nResiduals :\nMin\n1Q\nMedian\n3Q\nMax\n0 65982 0 26830 0 03183 0 24712 0 83049\n. . . . .\n− −\nCoefficients :\nEstimate Std. Error t value Pr(>|t|)\n4 114185 1 174308 3 503 0 000953\n( Intercept ) . . . . ***\n− −\n0 005569 0 010263 0 543 0 589709\nxPTS . . . .\n− −\n0 046922 0 015003 3 128 0 002886",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 286,
      "chunk_index": 1
    }
  },
  {
    "text": "xPTS . . . .\n− −\n0 046922 0 015003 3 128 0 002886\nxREB . . . . **\n0 015391 0 036990 0 416 0 679055\nxAST . . . .\n0 046479 0 028988 1 603 0 114905\nxTO . . . .\n− −\n0 103216 0 450763 0 229 0 819782\nxA.T . . . .\n0 063309 0 028015 2 260 0 028050\nxSTL . . . . *\n0 023088 0 030474 0 758 0 452082\nxBLK . . . .\n0 011492 0 018056 0 636 0 527253\nxPF . . . .\n4 842722 1 616465 2 996 0 004186\nxFG . . . . **\n1 162177 0 454178 2 559 0 013452\nxFT . . . . *",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 286,
      "chunk_index": 2
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 287\n3 0 476283 0 712184 0 669 0 506604\nxX P . . . .\n−−−\nSignif . codes: 0 L’ 0 . 001 L’ 0 . 01 L’ 0 . 05 L’ 0 . 1 L’ 1\nResidual standard error : 0 . 3905 on 52 degrees of freedom\nMultiple R Squared: 0 . 5043 , Adjusted R squared: 0 . 3995\n− −\nF statistic : 4 . 81 on 11 and 52 DF, p value: 4 . 514 e 05\n− − −\n11.3 Probit\nProbit has essentially the same idea as the logit except that the prob-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 287,
      "chunk_index": 0
    }
  },
  {
    "text": "ability function is replaced by the normal distribution. The nonlinear\nregression equation is as follows:\ny = Φ[f(x)], f(x) = β +β x +...β x\n0 1 1 k k\nwhere\nΦ(.)\nis the cumulative normal probability function. Again, irre-\nspective of the coefficients, f(x) ( ∞ ,+∞), but y (0,1). When\n∈ − ∈\nf(x) ∞ , y 0, and when f(x) +∞ , y 1.\n→ − → → →\nWe can redo the same previous logit model using a probit instead:\n> h = glm(y~x, family=binomial(link=\"probit\" ))\n> logLik(h)\n’log Lik. ’ 21 . 27924 (df= 12 )\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 287,
      "chunk_index": 1
    }
  },
  {
    "text": "> logLik(h)\n’log Lik. ’ 21 . 27924 (df= 12 )\n−\n> summary(h)\nCall :\nglm(formula = y ~ x, family = binomial(link = \"probit\" ))\nDeviance Residuals :\nMin\n1Q\nMedian\n3Q\nMax\n1 7635295 0 4121216 0 0003102 0 3499560 2 2456825\n. . . . .\n− − −\nCoefficients :\nEstimate Std. Error z value Pr(>|z|)\n26 28219 8 09608 3 246 0 00117\n( Intercept ) . . . . **\n− −\n0 03463 0 05385 0 643 0 52020\nxPTS . . . .\n− −\n0 28493 0 09939 2 867 0 00415\nxREB . . . . **\n0 10894 0 15735 0 692 0 48874\nxAST . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 287,
      "chunk_index": 2
    }
  },
  {
    "text": "0 10894 0 15735 0 692 0 48874\nxAST . . . .\n0 23742 0 13642 1 740 0 08180\nxTO . . . . .\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 287,
      "chunk_index": 3
    }
  },
  {
    "text": "288 data science: theories, models, algorithms, and analytics\n0 71485 1 86701 0 383 0 70181\nxA.T . . . .\n0 45963 0 18414 2 496 0 01256\nxSTL . . . . *\n0 03029 0 13631 0 222 0 82415\nxBLK . . . .\n0 01041 0 07907 0 132 0 89529\nxPF . . . .\n26 58461 9 38711 2 832 0 00463\nxFG . . . . **\n6 28278 2 51452 2 499 0 01247\nxFT . . . . *\n3 3 15824 3 37841 0 935 0 34988\nxX P . . . .\n−−−\nSignif . codes: 0 L’ 0 . 001 L’ 0 . 01 L’ 0 . 05 L’ 0 . 1 L’ 1\n(Dispersion parameter for binomial family taken to be 1 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 288,
      "chunk_index": 0
    }
  },
  {
    "text": "Null deviance: 88 . 723 on 63 degrees of freedom\nResidual deviance: 42 . 558 on 52 degrees of freedom\n66 558\nAIC: .\n8\nNumber of Fisher Scoring iterations :\n11.4 Analysis\nBoth these models are just settings in which we are computing binary\nprobabilities, i.e.\nPr[y = 1] = F(β\n(cid:48)\nx)\nwhere β is a vector of coefficients, and x is a vector of explanatory vari-\nables. F is the logit/probit function.\nyˆ = F(β x)\n(cid:48)\nwhere yˆ is the fitted value of y for a given x. In each case the function",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 288,
      "chunk_index": 1
    }
  },
  {
    "text": "takes the logit or probit form that we provided earlier. Of course,\nPr[y = 0] = 1 F(β (cid:48) x)\n−\nNote that the model may also be expressed in conditional expectation\nform, i.e.\nE[y x] = F(β\n(cid:48)\nx)(y = 1)+[1 F(β\n(cid:48)\nx)](y = 0) = F(β\n(cid:48)\nx)\n| −\n11.4.1 Slopes\nIn a linear regression, it is easy to see how the dependent variable changes\nwhen any right hand side variable changes. Not so with nonlinear mod-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 288,
      "chunk_index": 2
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 289\nels. A little bit of pencil pushing is required (add some calculus too).\nRemember that y lies in the range (0,1). Hence, we may be interested\nin how E(y x) changes as any of the explanatory variables changes in\n|\nvalue, so we can take the derivative:\n∂E(y x)\n| = F (β x)β f(β x)β\n(cid:48) (cid:48) (cid:48)\n∂x ≡\nFor each model we may compute this at the means of the regressors:\n• In the logit model this is as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 289,
      "chunk_index": 0
    }
  },
  {
    "text": "• In the logit model this is as follows:\n(C 1 ) F: exp(b*x) /( 1 +exp(b*x));\nb x\n%E\n1\n(D )\n−−−−−−−−−\nb x\n1\n%E +\n(C 2 ) diff(F,x);\n2\nb x b x\nb %E b %E\n2\n(D )\n−−−−−−−−− − −−−−−−−−−−−−\n2\nb x b x\n1 1\n%E + (%E + )\nTherefore, we may write this as:\n(cid:32) (cid:33)(cid:32) (cid:33)\n∂E(y x) eβ(cid:48)x eβ(cid:48)x\n| = β 1\n∂x 1+eβ (cid:48) x − 1+eβ (cid:48) x\nwhich may be re-written as\n∂E(y x)\n| = β Λ(β\n(cid:48)\nx) [1 Λ(β\n(cid:48)\nx)]\n∂x · · −\n> h = glm(y~x, family=binomial(link=\" logit \" ))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 289,
      "chunk_index": 1
    }
  },
  {
    "text": "> h = glm(y~x, family=binomial(link=\" logit \" ))\n> beta = h$coefficients\n> beta\n( Intercept ) xPTS xREB xAST xTO\n45 83315262 0 06127422 0 49037435 0 16421685 0 38404689\n. . . . .\n− − −\nxA.T xSTL xBLK xPF xFG\n1 56351478 0 78359670 0 07867125 0 02602243 46 21373793\n. . . . .\n3\nxFT xX P\n10 72992472 5 41984900\n. .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 289,
      "chunk_index": 2
    }
  },
  {
    "text": "290 data science: theories, models, algorithms, and analytics\n> dim(x)\n1 64 11\n[ ]\n> beta = as.matrix(beta)\n> dim(beta)\n1 12 1\n[ ]\n> wuns = matrix( 1 , 64 , 1 )\n> x = cbind(wuns,x)\n> dim(x)\n1 64 12\n[ ]\n> xbar = as.matrix(colMeans(x))\n> dim(xbar)\n1 12 1\n[ ]\n> xbar\n1\n[ , ]\n1 0000000\n.\n67 1015625\nPTS .\n34 4671875\nREB .\n12 7484375\nAST .\n13 9578125\nTO .\n0 9778125\nA.T .\n6 8234375\nSTL .\n2 7500000\nBLK .\n18 6562500\nPF .\n0 4232969\nFG .\n0 6914687\nFT .\n3 0 3333750\nX P .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 290,
      "chunk_index": 0
    }
  },
  {
    "text": "0 4232969\nFG .\n0 6914687\nFT .\n3 0 3333750\nX P .\n> logitfunction = exp(t(beta) %*% xbar) /( 1 +exp(t(beta) %*% xbar))\n> logitfunction\n1\n[ , ]\n1 0 5139925\n[ ,] .\n> slopes = beta * logitfunction [ 1 ] * ( 1 logitfunction [ 1 ])\n−\n> slopes\n1\n[ , ]\n11 449314459\n( Intercept ) .\n−\n0 015306558\nxPTS .\n−\n0 122497576\nxREB .\n0 041022062\nxAST .\n0 095936529\nxTO .\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 290,
      "chunk_index": 1
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 291\n0 390572574\nxA.T .\n0 195745753\nxSTL .\n0 019652410\nxBLK .\n0 006500512\nxPF .\n11 544386272\nxFG .\n2 680380362\nxFT .\n3 1 353901094\nxX P .\n• In the probit model this is\n∂E(y x)\n| = φ(β x)β\n(cid:48)\n∂x\nwhere φ(.) is the normal density function (not the cumulative proba-\nbility).\n> h = glm(y~x, family=binomial(link=\"probit\" ))\n> beta = h$coefficients\n> beta\n( Intercept ) xPTS xREB xAST xTO\n26 28219202 0 03462510 0 28493498 0 10893727 0 23742076",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 291,
      "chunk_index": 0
    }
  },
  {
    "text": ". . . . .\n− − −\nxA.T xSTL xBLK xPF xFG\n0 71484863 0 45963279 0 03029006 0 01040612 26 58460638\n. . . . .\n3\nxFT xX P\n6 28277680 3 15823537\n. .\n> x = as.matrix(cbind(wuns,x))\n> xbar = as.matrix(colMeans(x))\n> dim(xbar)\n1 12 1\n[ ]\n> dim(beta)\nNULL\n> beta = as.matrix(beta)\n> dim(beta)\n1 12 1\n[ ]\n> slopes = dnorm(t(beta) %*% xbar)[ 1 ]*beta\n> slopes\n1\n[ , ]\n10 470181164\n( Intercept ) .\n−\n0 013793791\nxPTS .\n−\n0 113511111\nxREB .\n0 043397939\nxAST .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 291,
      "chunk_index": 1
    }
  },
  {
    "text": "292 data science: theories, models, algorithms, and analytics\n0 094582613\nxTO .\n−\n0 284778174\nxA.T .\n0 183106438\nxSTL .\n0 012066819\nxBLK .\n0 004145544\nxPF .\n10 590655632\nxFG .\n2 502904294\nxFT .\n3 1 258163568\nxX P .\n11.4.2 Maximum-Likelihood Estimation (MLE)\nEstimation in the models above, using the glm function is done by R\nusing MLE. Lets write this out a little formally. Since we have say n ob-\nservations, and each LHS variable is y = 0,1 , we have the likelihood\n{ }\nfunction as follows:\nn",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 292,
      "chunk_index": 0
    }
  },
  {
    "text": "{ }\nfunction as follows:\nn\nL = ∏ F(β (cid:48) x)y i[1 F(β (cid:48) x)]1 − y i\n−\ni=1\nThe log-likelihood will be\nn\n∑(cid:2) (cid:3)\nlnL = y\ni\nlnF(β\n(cid:48)\nx)+(1 y\ni\n)ln[1 F(β\n(cid:48)\nx)]\n− −\ni=1\nTo maximize the log-likelihood we take the derivative:\n∂lnL ∑ n (cid:20) f(β (cid:48) x) f(β (cid:48) x) (cid:21)\n= y (1 y ) β = 0\n∂β i F(β x) − − i 1 F(β x)\ni=1 (cid:48) − (cid:48)\nwhich gives a system of equations to be solved for β. This is what the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 292,
      "chunk_index": 1
    }
  },
  {
    "text": "software is doing. The system of first-order conditions are collectively\ncalled the “likelihood equation”.\nYou may well ask, how do we get the t-statistics of the parameter es-\ntimates β? The formal derivation is beyond the scope of this class, as it\nrequires probability limit theorems, but let’s just do this a little heuristi-\ncally, so you have some idea of what lies behind it.\nThe t-stat for a coefficient is its value divided by its standard devia-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 292,
      "chunk_index": 2
    }
  },
  {
    "text": "tion. We get some idea of the standard deviation by asking the question:\nhow does the coefficient set β change when the log-likelihood changes?\nThat is, we are interested in ∂β/∂lnL. Above we have computed the\nreciprocal of this, as you can see. Lets define\n∂lnL\ng =\n∂β",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 292,
      "chunk_index": 3
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 293\nWe also define the second derivative (also known as the Hessian matrix)\n∂2lnL\nH =\n∂β∂β\n(cid:48)\nNote that the following are valid:\nE(g) = 0 (this is a vector)\nVar(g) = E(gg ) E(g)2 = E(gg )\n(cid:48) (cid:48)\n−\n= E(H) (this is a non-trivial proof)\n−\nWe call\nI(β) = E(H)\n−\nthe information matrix. Since (heuristically) the variation in log-likelihood\nwith changes in beta is given by Var(g) = E(H) = I(β), the inverse\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 293,
      "chunk_index": 0
    }
  },
  {
    "text": "−\ngives the variance of β. Therefore, we have\nVar(β) I(β) 1\n−\n→\nWe take the square root of the diagonal of this matrix and divide the\nvalues of β by that to get the t-statistics.\n11.5 Multinomial Logit\nYou will need the nnet package for this. This model takes the following\nform:\nexp(β x)\n(cid:48)j\nProb[y = j] = p =\nj 1+∑J\nexp(β x)\nj=1 (cid:48)j\nWe usually set\n1\nProb[y = 0] = p =\n0 1+∑J\nexp(β x)\nj=1 (cid:48)j\nTo run this we set up as follows:\n> ncaa = read. table(\"ncaa. txt\" ,header=TRUE)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 293,
      "chunk_index": 1
    }
  },
  {
    "text": "> ncaa = read. table(\"ncaa. txt\" ,header=TRUE)\n> x = as.matrix(ncaa[ 4 : 14 ])\n1 1 16 0 1\n> w = ( : )* +\n0 1 16 0\n> w = ( : )*\n> y 1 = c(w 1 ,w 0 ,w 0 ,w 0 )\n> y 2 = c(w 0 ,w 1 ,w 0 ,w 0 )\n> y 3 = c(w 0 ,w 0 ,w 1 ,w 0 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 293,
      "chunk_index": 2
    }
  },
  {
    "text": "294 data science: theories, models, algorithms, and analytics\n> y 4 = c(w 0 ,w 0 ,w 0 ,w 1 )\n> y = cbind(y 1 ,y 2 ,y 3 ,y 4 )\n> library(nnet)\n> res = multinom(y~x)\n# weights : 52 (36 variable )\n88 722839\ninitial value .\n10 71 177975\niter value .\n20 60 076921\niter value .\n30 51 167439\niter value .\n40 47 005269\niter value .\n50 45 196280\niter value .\n60 44 305029\niter value .\n70 43 341689\niter value .\n80 43 260097\niter value .\n90 43 247324\niter value .\n100 43 141297\niter value .\n43 141297",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 294,
      "chunk_index": 0
    }
  },
  {
    "text": "iter value .\n100 43 141297\niter value .\n43 141297\nfinal value .\n100\nstopped after iterations\n> res\nCall :\nmultinom(formula = y ~ x)\nCoefficients :\n( Intercept ) xPTS xREB xAST xTO xA.T\n2 8 847514 0 1595873 0 3134622 0 6198001 0 2629260 2 1647350\ny . . . . . .\n− − − −\n3 65 688912 0 2983748 0 7309783 0 6059289 0 9284964 0 5720152\ny . . . . . .\n− − −\n4 31 513342 0 1382873 0 2432960 0 2887910 0 2204605 2 6409780\ny . . . . . .\n− − −\n3\nxSTL xBLK xPF xFG xFT xX P",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 294,
      "chunk_index": 1
    }
  },
  {
    "text": "y . . . . . .\n− − −\n3\nxSTL xBLK xPF xFG xFT xX P\n2 0 813519 0 01472506 0 6521056 13 77579 10 374888 3 436073\ny . . . . . .\n− − −\n3 1 310701 0 63038878 0 1788238 86 37410 24 769245 4 897203\ny . . . . . .\n− − − − −\n4 1 470406 0 31863373 0 5392835 45 18077 6 701026 7 841990\ny . . . . . .\n− − − −\n86 2826\nResidual Deviance: .\n158 2826\nAIC: .\n> names(res)\n1\n[ ] \"n\" \"nunits\" \"nconn\" \"conn\"\n5\n[ ] \"nsunits\" \"decay\" \"entropy\" \"softmax\"\n9\n[ ] \"censored\" \"value\" \"wts\" \"convergence\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 294,
      "chunk_index": 2
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 295\n13\n[ ] \" fitted . values\" \"residuals\" \" call \" \"terms\"\n17\n[ ] \"weights\" \"deviance\" \"rank\" \"lab\"\n21\n[ ] \"coefnames\" \"vcoefnames\" \"xlevels\" \"edf\"\n25\n[ ] \"AIC\"\n> res$fitted . values\n1 2 3 4\ny y y y\n1 6 785454 01 3 214178 01 7 032345 06 2 972107 05\n. e . e . e . e\n− − − −\n2 6 168467 01 3 817718 01 2 797313 06 1 378715 03\n. e . e . e . e\n− − − −\n3 7 784836 01 1 990510 01 1 688098 02 5 584445 03\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 295,
      "chunk_index": 0
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n4 5 962949 01 3 988588 01 5 018346 04 4 344392 03\n. e . e . e . e\n− − − −\n5 9 815286 01 1 694721 02 1 442350 03 8 179230 05\n. e . e . e . e\n− − − −\n6 9 271150 01 6 330104 02 4 916966 03 4 666964 03\n. e . e . e . e\n− − − −\n7 4 515721 01 9 303667 02 3 488898 02 4 205023 01\n. e . e . e . e\n− − − −\n8 8 210631 01 1 530721 01 7 631770 03 1 823302 02\n. e . e . e . e\n− − − −\n9 1 567804 01 9 375075 02 6 413693 01 1 080996 01\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 295,
      "chunk_index": 1
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n10 8 403357 01 9 793135 03 1 396393 01 1 023186 02\n. e . e . e . e\n− − − −\n11 9 163789 01 6 747946 02 7 847380 05 1 606316 02\n. e . e . e . e\n− − − −\n12 2 448850 01 4 256001 01 2 880803 01 4 143463 02\n. e . e . e . e\n− − − −\n13 1 040352 01 1 534272 01 1 369554 01 6 055822 01\n. e . e . e . e\n− − − −\n14 8 468755 01 1 506311 01 5 083480 04 1 985036 03\n. e . e . e . e\n− − − −\n15 7 136048 01 1 294146 01 7 385294 02 8 312770 02\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 295,
      "chunk_index": 2
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n16 9 885439 01 1 114547 02 2 187311 05 2 887256 04\n. e . e . e . e\n− − − −\n17 6 478074 02 3 547072 01 1 988993 01 3 816127 01\n. e . e . e . e\n− − − −\n18 4 414721 01 4 497228 01 4 716550 02 6 163956 02\n. e . e . e . e\n− − − −\n19 6 024508 03 3 608270 01 7 837087 02 5 547777 01\n. e . e . e . e\n− − − −\n20 4 553205 01 4 270499 01 3 614863 04 1 172681 01\n. e . e . e . e\n− − − −\n21 1 342122 01 8 627911 01 1 759865 03 1 236845 03\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 295,
      "chunk_index": 3
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n22 1 877123 02 6 423037 01 5 456372 05 3 388705 01\n. e . e . e . e\n− − − −\n23 5 620528 01 4 359459 01 5 606424 04 1 440645 03\n. e . e . e . e\n− − − −\n24 2 837494 01 7 154506 01 2 190456 04 5 809815 04\n. e . e . e . e\n− − − −\n25 1 787749 01 8 037335 01 3 361806 04 1 715541 02\n. e . e . e . e\n− − − −\n26 3 274874 02 3 484005 02 1 307795 01 8 016317 01\n. e . e . e . e\n− − − −\n27 1 635480 01 3 471676 01 1 131599 01 3 761245 01\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 295,
      "chunk_index": 4
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n28 2 360922 01 7 235497 01 3 375018 02 6 607966 03\n. e . e . e . e\n− − − −\n29 1 618602 02 7 233098 01 5 762083 06 2 604984 01\n. e . e . e . e\n− − − −\n30 3 037741 02 8 550873 01 7 487804 02 3 965729 02\n. e . e . e . e\n− − − −\n31 1 122897 01 8 648388 01 3 935657 03 1 893584 02\n. e . e . e . e\n− − − −\n32 2 312231 01 6 607587 01 4 770775 02 6 031045 02\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 295,
      "chunk_index": 5
    }
  },
  {
    "text": "296 data science: theories, models, algorithms, and analytics\n33 6 743125 01 2 028181 02 2 612683 01 4 413746 02\n. e . e . e . e\n− − − −\n34 1 407693 01 4 089518 02 7 007541 01 1 175815 01\n. e . e . e . e\n− − − −\n35 6 919547 04 4 194577 05 9 950322 01 4 233924 03\n. e . e . e . e\n− − − −\n36 8 051225 02 4 213965 03 9 151287 01 1 450423 04\n. e . e . e . e\n− − − −\n37 5 691220 05 7 480549 02 5 171594 01 4 079782 01\n. e . e . e . e\n− − − −\n38 2 709867 02 3 808987 02 6 193969 01 3 154145 01",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 296,
      "chunk_index": 0
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n39 4 531001 05 2 248580 08 9 999542 01 4 626258 07\n. e . e . e . e\n− − − −\n40 1 021976 01 4 597678 03 5 133839 01 3 798208 01\n. e . e . e . e\n− − − −\n41 2 005837 02 2 063200 01 5 925050 01 1 811166 01\n. e . e . e . e\n− − − −\n42 1 829028 04 1 378795 03 6 182839 01 3 801544 01\n. e . e . e . e\n− − − −\n43 1 734296 01 9 025284 04 7 758862 01 4 978171 02\n. e . e . e . e\n− − − −\n44 4 314938 05 3 131390 06 9 997892 01 1 645004 04\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 296,
      "chunk_index": 1
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n45 1 516231 02 2 060325 03 9 792594 01 3 517926 03\n. e . e . e . e\n− − − −\n46 2 917597 01 6 351166 02 4 943818 01 1 503468 01\n. e . e . e . e\n− − − −\n47 1 278933 04 1 773509 03 1 209486 01 8 771500 01\n. e . e . e . e\n− − − −\n48 1 320000 01 2 064338 01 6 324904 01 2 907578 02\n. e . e . e . e\n− − − −\n49 1 683221 02 4 007848 01 1 628981 03 5 807540 01\n. e . e . e . e\n− − − −\n50 9 670085 02 4 314765 01 7 669035 03 4 641536 01\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 296,
      "chunk_index": 2
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n51 4 953577 02 1 370037 01 9 882004 02 7 146405 01\n. e . e . e . e\n− − − −\n52 1 787927 02 9 825660 02 2 203037 01 6 635604 01\n. e . e . e . e\n− − − −\n53 1 174053 02 4 723628 01 2 430072 03 5 134666 01\n. e . e . e . e\n− − − −\n54 2 053871 01 6 721356 01 4 169640 02 8 078090 02\n. e . e . e . e\n− − − −\n55 3 060369 06 1 418623 03 1 072549 02 9 878528 01\n. e . e . e . e\n− − − −\n56 1 122164 02 6 566169 02 3 080641 01 6 150525 01\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 296,
      "chunk_index": 3
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n57 8 873716 03 4 996907 01 8 222034 03 4 832136 01\n. e . e . e . e\n− − − −\n58 2 164962 02 2 874313 01 1 136455 03 6 897826 01\n. e . e . e . e\n− − − −\n59 5 230443 03 6 430174 04 9 816825 01 1 244406 02\n. e . e . e . e\n− − − −\n60 8 743368 02 6 710327 02 4 260116 01 4 194514 01\n. e . e . e . e\n− − − −\n61 1 913578 01 6 458463 04 3 307553 01 4 772410 01\n. e . e . e . e\n− − − −\n62 6 450967 07 5 035697 05 7 448285 01 2 551205 01\n. e . e . e . e\n− − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 296,
      "chunk_index": 4
    }
  },
  {
    "text": ". e . e . e . e\n− − − −\n63 2 400365 04 4 651537 03 8 183390 06 9 951002 01\n. e . e . e . e\n− − − −\n64 1 515894 04 2 631451 01 1 002332 05 7 366933 01\n. e . e . e . e\n− − − −\n1\nYou can see from the results that the probability for category is the\nsame as p . What this means is that we compute the other three prob-\n0\nabilities, and the remaining is for the first category. We check that the\n1\nprobabilities across each row for all four categories add up to :\n> rowSums(res$fitted.values)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 296,
      "chunk_index": 5
    }
  },
  {
    "text": "> rowSums(res$fitted.values)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 296,
      "chunk_index": 6
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 297\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n51 52 53 54 55 56 57 58 59 60 61 62 63 64\n1 1 1 1 1 1 1 1 1 1 1 1 1 1\n11.6 Truncated Variables\nHere we provide some basic results that we need later. And of course,\nwe need to revisit our Bayesian ideas again!\n• Given a probability density f(x),\nf(x)\nf(x x > a) =\n| Pr(x > a)\nIf we are using the normal distribution then this is:\nφ(x)\nf(x x > a) =\n| 1 Φ(a)\n−\n• If x N(µ,σ2), then\n∼\nφ(c) a µ",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 297,
      "chunk_index": 0
    }
  },
  {
    "text": "| 1 Φ(a)\n−\n• If x N(µ,σ2), then\n∼\nφ(c) a µ\nE(x x > a) = µ+σ , c = −\n| 1 Φ(c) σ\n−\nNote that this expectation is provided without proof, as are the next\nfew ones. For example if we let x be standard normal and we want\nE([x x > 1], we have\n| −\n> dnorm( 1 )/( 1 pnorm( 1 ))\n− − −\n1 0 2876000\n[ ] .\n• For the same distribution\nφ(c) a µ\nE(x x < a) = µ+σ − , c = −\n| Φ(c) σ\nFor example, E[x x < 1] is\n|\n> dnorm( 1 ) /pnorm( 1 )\n−\n1 0 2876000\n[ ] .\n−\nφ(c) φ(c)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 297,
      "chunk_index": 1
    }
  },
  {
    "text": "−\n1 0 2876000\n[ ] .\n−\nφ(c) φ(c)\n• Inverse Mills Ratio: The values 1 Φ(c) or −Φ(c) as the case may be\n−\nis often shortened to the variable λ(c), which is also known as the\nInverse Mills Ratio.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 297,
      "chunk_index": 2
    }
  },
  {
    "text": "298 data science: theories, models, algorithms, and analytics\n• If y and x are correlated (with correlation ρ), and y N(µ ,σ2), then\n∼ y y\nf(y,x)\nPr(y,x x > a) =\n| Pr(x > a)\na µ\nE(y x > a) = µ +σ ρλ(c), c = −\ny y\n| σ\nThis leads naturally to the truncated regression model. Suppose we have\nthe usual regression model where\ny = β\n(cid:48)\nx+e, e N(0,σ2)\n∼\nBut suppose we restrict attention in our model to values of y that are\ngreater than a cut off a. We can then write down by inspection the fol-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 298,
      "chunk_index": 0
    }
  },
  {
    "text": "lowing correct model (no longer is the simple linear regression valid)\nφ[(a β x)/σ]\n(cid:48)\nE(y y > a) = β x+σ −\n| (cid:48) 1 Φ[(a β x)/σ]\n(cid:48)\n− −\nTherefore, when the sample is truncated, then we need to run the regres-\nsion above, i.e., the usual right-hand side β x with an additional variable,\n(cid:48)\ni.e., the Inverse Mill’s ratio. We look at this in a real-world example.\nAn Example: Limited Dependent Variables in VC Syndications",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 298,
      "chunk_index": 1
    }
  },
  {
    "text": "Not all venture-backed firms end up making a successful exit, either\nvia an IPO, through a buyout, or by means of another exit route. By\nexamining a large sample of firms, we can measure the probability of a\nfirm making a successful exit. By designating successful exits as S = 1,\nand setting S = 0 otherwise, we use matrix X of explanatory variables\nand fit a Probit model to the data. We define S to be based on a latent\nthreshold variable S such that\n∗\n(cid:40)\n1 if S > 0\nS = ∗ ( 11 . 1 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 298,
      "chunk_index": 2
    }
  },
  {
    "text": "∗\n(cid:40)\n1 if S > 0\nS = ∗ ( 11 . 1 )\n0 if S 0.\n∗\n≤\nwhere the latent variable is modeled as\nS ∗ = γ (cid:48) X+u, u ∼ N(0,σ u 2) ( 11 . 2 )\nThe fitted model provides us the probability of exit, i.e., E(S), for all\nfinancing rounds.\nE(S) = E(S ∗ > 0) = E(u > γ (cid:48) X) = 1 Φ( γ (cid:48) X) = Φ(γ (cid:48) X), ( 11 . 3 )\n− − −\nwhere γ is the vector of coefficients fitted in the Probit model, using\nstandard likelihood methods. The last expression in the equation above",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 298,
      "chunk_index": 3
    }
  },
  {
    "text": "follows from the use of normality in the Probit specification.\nΦ(.)\nde-\nnotes the cumulative normal distribution.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 298,
      "chunk_index": 4
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 299\n11.6.1 Endogeneity\nSuppose we want to examine the role of syndication in venture success.\nSuccess in a syndicated venture comes from two broad sources of VC\nexpertise. First, VCs are experienced in picking good projects to invest\nin, and syndicates are efficient vehicles for picking good firms; this is the\n1994\nselection hypothesis put forth by Lerner ( ). Amongst two projects",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 299,
      "chunk_index": 0
    }
  },
  {
    "text": "that appear a-priori similar in prospects, the fact that one of them is\nselected by a syndicate is evidence that the project is of better quality\n(ex-post to being vetted by the syndicate, but ex-ante to effort added by\nthe VCs), since the process of syndication effectively entails getting a\nsecond opinion by the lead VC. Second, syndicates may provide better\nmonitoring as they bring a wide range of skills to the venture, and this is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 299,
      "chunk_index": 1
    }
  },
  {
    "text": "suggested in the value-added hypothesis of Brander, Amit and Antweiler\n2002\n( ).\nA regression of venture returns on various firm characteristics and a\ndummy variable for syndication allows a first pass estimate of whether\nsyndication impacts performance. However, it may be that syndicated\nfirms are simply of higher quality and deliver better performance, whether\nor not they chose to syndicate. Better firms are more likely to syndicate",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 299,
      "chunk_index": 2
    }
  },
  {
    "text": "because VCs tend to prefer such firms and can identify them. In this\ncase, the coefficient on the dummy variable might reveal a value-add\nfrom syndication, when indeed, there is none. Hence, we correct the\nspecification for endogeneity, and then examine whether the dummy\nvariable remains significant.\n2011\nGreene ( ) provides the correction for endogeneity required here.\nWe briefly summarize the model required. The performance regression\nis of the form:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 299,
      "chunk_index": 3
    }
  },
  {
    "text": "is of the form:\nY = β (cid:48) X+δS+(cid:101), (cid:101) ∼ N(0,σ (cid:101) 2) ( 11 . 4 )\nwhere Y is the performance variable; S is, as before, the dummy variable\n1\ntaking a value of if the firm is syndicated, and zero otherwise, and\nδ is a coefficient that determines whether performance is different on\naccount of syndication. If it is not, then it implies that the variables X\nare sufficient to explain the differential performance across firms, or that",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 299,
      "chunk_index": 4
    }
  },
  {
    "text": "there is no differential performance across the two types of firms.\nHowever, since these same variables determine also, whether the firm\nsyndicates or not, we have an endogeneity issue which is resolved by\nadding a correction to the model above. The error term (cid:101) is affected\nby censoring bias in the subsamples of syndicated and non-syndicated",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 299,
      "chunk_index": 5
    }
  },
  {
    "text": "300 data science: theories, models, algorithms, and analytics\nfirms. When S = 1, i.e. when the firm’s financing is syndicated, then the\nresidual (cid:101) has the following expectation\n(cid:20) (cid:21)\nφ(γ X)\nE((cid:101) | S = 1) = E((cid:101) | S ∗ > 0) = E((cid:101) | u > − γ (cid:48) X) = ρσ (cid:101) Φ(γ (cid:48) X) . ( 11 . 5 )\n(cid:48)\nwhere ρ = Corr((cid:101),u), and σ is the standard deviation of (cid:101). This implies\n(cid:101)\nthat\n(cid:20) (cid:21)\nφ(γ X)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 300,
      "chunk_index": 0
    }
  },
  {
    "text": "(cid:101)\nthat\n(cid:20) (cid:21)\nφ(γ X)\nE(Y | S = 1) = β (cid:48) X+δ+ρσ (cid:101) Φ(γ (cid:48) X) . ( 11 . 6 )\n(cid:48)\nNote that φ( γ X) = φ(γ X), and 1 Φ( γ X) = Φ(γ X). For es-\n(cid:48) (cid:48) (cid:48) (cid:48)\n− − −\ntimation purposes, we write this as the following regression equation:\nY = δ+β (cid:48) X+β m m(γ (cid:48) X) ( 11 . 7 )\nwhere m(γ X) =\nφ(γ(cid:48)X)\nand β = ρσ . Thus, δ,β,β are the coeffi-\n(cid:48) Φ(γ (cid:48) X) m (cid:101) { m }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 300,
      "chunk_index": 1
    }
  },
  {
    "text": "(cid:48) Φ(γ (cid:48) X) m (cid:101) { m }\ncients estimated in the regression. (Note here that m(γ X) is also known\n(cid:48)\nas the inverse Mill’s ratio.)\nLikewise, for firms that are not syndicated, we have the following\nresult\n(cid:20) (cid:21)\nφ(γ X)\nE(Y | S = 0) = β (cid:48) X+ρσ (cid:101) 1 − Φ(γ (cid:48) X) . ( 11 . 8 )\n(cid:48)\n−\nThis may also be estimated by linear cross-sectional regression.\nY = β (cid:48) X+β m m (cid:48) (γ (cid:48) X) ( 11 . 9 )\nwhere m (cid:48) = 1 −\nφ\nΦ\n(\n(\nγ\nγ",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 300,
      "chunk_index": 2
    }
  },
  {
    "text": "where m (cid:48) = 1 −\nφ\nΦ\n(\n(\nγ\nγ\n(cid:48)X\nX\n)\n) and β m = ρσ (cid:101) .\n− (cid:48)\nThe estimation model will take the form of a stacked linear regression\ncomprising both equations ( 11 . 7 ) and ( 11 . 9 ). This forces β to be the same\nacross all firms without necessitating additional constraints, and allows\nthe specification to remain within the simple OLS form. If δ is significant\nafter this endogeneity correction, then the empirical evidence supports",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 300,
      "chunk_index": 3
    }
  },
  {
    "text": "the hypothesis that syndication is a driver of differential performance.\nIf the coefficients δ,β are significant, then the expected difference in\nm\n{ }\nperformance for each syndicated financing round (i,j) is\n(cid:104) (cid:105)\nδ+β m m(γ i(cid:48)j X ij )\n−\nm (cid:48) (γ i(cid:48)j X ij ) ,\n∀\ni,j. ( 11 . 10 )\nThe method above forms one possible approach to addressing treatment\neffects. Another approach is to estimate a Probit model first, and then",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 300,
      "chunk_index": 4
    }
  },
  {
    "text": "to set m(γ X) = Φ(γ X). This is known as the instrumental variables\n(cid:48) (cid:48)\napproach.\nThe regression may be run using the sampleSelection package in R.\nSample selection models correct for the fact that two subsamples may be\ndifferent because of treatment effects.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 300,
      "chunk_index": 5
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 301\n11.6.2 Example: Women in the Labor Market\nAfter loading in the package sampleSelection we can use the data set\ncalled Mroz87. This contains labour market participation data for women\nas well as wage levels for women. If we are explaining what drives\nwomen’s wages we can simply run the following regression.\n> library(sampleSelection)\n> data(Mroz87)\n> summary(Mroz87)\nlfp hours kids5 kids618\nMin. :0.0000 Min. : 0.0 Min. :0.0000 Min. :0.000",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 301,
      "chunk_index": 0
    }
  },
  {
    "text": "Min. :0.0000 Min. : 0.0 Min. :0.0000 Min. :0.000\n1st Qu.:0.0000 1st Qu.: 0.0 1st Qu.:0.0000 1st Qu.:0.000\nMedian :1.0000 Median : 288.0 Median :0.0000 Median :1.000\nMean :0.5684 Mean : 740.6 Mean :0.2377 Mean :1.353\n3rd Qu.:1.0000 3rd Qu.:1516.0 3rd Qu.:0.0000 3rd Qu.:2.000\nMax. :1.0000 Max. :4950.0 Max. :3.0000 Max. :8.000\nage educ wage repwage\nMin. :30.00 Min. : 5.00 Min. : 0.000 Min. :0.000\n1st Qu.:36.00 1st Qu.:12.00 1st Qu.: 0.000 1st Qu.:0.000",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 301,
      "chunk_index": 1
    }
  },
  {
    "text": "Median :43.00 Median :12.00 Median : 1.625 Median :0.000\nMean :42.54 Mean :12.29 Mean : 2.375 Mean :1.850\n3rd Qu.:49.00 3rd Qu.:13.00 3rd Qu.: 3.788 3rd Qu.:3.580\nMax. :60.00 Max. :17.00 Max. :25.000 Max. :9.980\nhushrs husage huseduc huswage\nMin. : 175 Min. :30.00 Min. : 3.00 Min. : 0.4121\n1st Qu.:1928 1st Qu.:38.00 1st Qu.:11.00 1st Qu.: 4.7883\nMedian :2164 Median :46.00 Median :12.00 Median : 6.9758\nMean :2267 Mean :45.12 Mean :12.49 Mean : 7.4822",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 301,
      "chunk_index": 2
    }
  },
  {
    "text": "Mean :2267 Mean :45.12 Mean :12.49 Mean : 7.4822\n3rd Qu.:2553 3rd Qu.:52.00 3rd Qu.:15.00 3rd Qu.: 9.1667\nMax. :5010 Max. :60.00 Max. :17.00 Max. :40.5090\nfaminc mtr motheduc fatheduc\nMin. : 1500 Min. :0.4415 Min. : 0.000 Min. : 0.000\n1st Qu.:15428 1st Qu.:0.6215 1st Qu.: 7.000 1st Qu.: 7.000\nMedian :20880 Median :0.6915 Median :10.000 Median : 7.000\nMean :23081 Mean :0.6789 Mean : 9.251 Mean : 8.809\n3rd Qu.:28200 3rd Qu.:0.7215 3rd Qu.:12.000 3rd Qu.:12.000",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 301,
      "chunk_index": 3
    }
  },
  {
    "text": "Max. :96000 Max. :0.9415 Max. :17.000 Max. :17.000\nunem city exper nwifeinc\nMin. : 3.000 Min. :0.0000 Min. : 0.00 Min. : 0.02906\n1st Qu.: 7.500 1st Qu.:0.0000 1st Qu.: 4.00 1st Qu.:1−3.02504\nMedian : 7.500 Median :1.0000 Median : 9.00 Median :17.70000\nMean : 8.624 Mean :0.6428 Mean :10.63 Mean :20.12896\n3rd Qu.:11.000 3rd Qu.:1.0000 3rd Qu.:15.00 3rd Qu.:24.46600\nMax. :14.000 Max. :1.0000 Max. :45.00 Max. :96.00000\nwifecoll huscoll kids\nTRUE:212 TRUE:295 Mode :logical",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 301,
      "chunk_index": 4
    }
  },
  {
    "text": "TRUE:212 TRUE:295 Mode :logical\nFALSE:541 FALSE:458 FALSE:229\nTRUE :524\n> res = lm(wage ~ age + I(age^2) + educ + city , data=Mroz87)\n> summary(res)\nCall:\nlm(formula = wage ~ age + I(age^2) + educ + city , data = Mroz87)\nResiduals:\nMin 1Q Median 3Q Max\n4.6805 2.1919 0.4575 1.3588 22.6903\n− − −\nCoefficients:\nEstimate Std. Error t value Pr(>|t|)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 301,
      "chunk_index": 5
    }
  },
  {
    "text": "302 data science: theories, models, algorithms, and analytics\n(Intercept) 8.499373 3.296628 2.578 0.0101 *\nage −0.252758 0.152719 −1.655 0.0983 .\nI(age^2) 0.002918 0.001761 1.657 0.0980 .\neduc −0.450873 0.050306 −8.963 <2e 16 ***\ncity 0.080852 0.238852 0.339 0.73−51\n−−−\nSignif. codes: 0 Ô***Õ 0.001 Ô**Õ 0.01 Ô*Õ 0.05 Ô.Õ 0.1 Ô Õ 1\nResidual standard error: 3.075 on 748 degrees of freedom\nMultiple R squared: 0.1049, Adjusted R squared: 0.1001",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 302,
      "chunk_index": 0
    }
  },
  {
    "text": "F statisti − c : 21.91 on 4 and 748 DF, p val − ue: < 2.2e 16\n− − −\nSo, education matters. But since education also determines labor force\nparticipation (variable lfp) it may just be that we can use lfp instead.\nLet’s try that.\n> res = lm(wage ~ age + I(age^2) + lfp + city , data=Mroz87)\n> summary(res)\nCall:\nlm(formula = wage ~ age + I(age^2) + lfp + city , data = Mroz87)\nResiduals:\nMin 1Q Median 3Q Max\n4.1808 0.9884 0.1615 0.3090 20.6810\n− − −\nCoefficients:\nEstimate Std. Error t value Pr(>|t|)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 302,
      "chunk_index": 1
    }
  },
  {
    "text": "Estimate Std. Error t value Pr(>|t|)\n(Intercept) 4.558e 01 2.606e+00 0.175 0.8612\nage −3.052e −03 1.240e 01 −0.025 0.9804\nI(age^2) 1.288e −05 1.431e −03 0.009 0.9928\nlfp 4.186e − +00 1.845e −01 22.690 <2e 16 ***\ncity 4.622e 01 1.905e −01 2.426 0.01−55 *\n− −\n−−−\nSignif. codes: 0 Ô***Õ 0.001 Ô**Õ 0.01 Ô*Õ 0.05 Ô.Õ 0.1 Ô Õ 1\nResidual standard error: 2.491 on 748 degrees of freedom\nMultiple R squared: 0.4129, Adjusted R squared: 0.4097\nF statisti − c : 131.5 on 4 and 748 DF, p val − ue: < 2.2e 16",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 302,
      "chunk_index": 2
    }
  },
  {
    "text": "− − −\n> res = lm(wage ~ age + I(age^2) + lfp + educ + city , data=Mroz87)\n> summary(res)\nCall:\nlm(formula = wage ~ age + I(age^2) + lfp + educ + city , data = Mroz87)\nResiduals:\nMin 1Q Median 3Q Max\n4.9895 1.1034 0.1820 0.4646 21.0160\n− − −\nCoefficients:\nEstimate Std. Error t value Pr(>|t|)\n(Intercept) 4.7137850 2.5882435 1.821 0.069 .\nage −0.0395656 0.1200320 −0.330 0.742\nI(age^2) 0.0002938 0.0013849 0.212 0.832\nlfp −3.9439552 0.1815350 2−1.726 < 2e 16 ***",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 302,
      "chunk_index": 3
    }
  },
  {
    "text": "lfp −3.9439552 0.1815350 2−1.726 < 2e 16 ***\neduc 0.2906869 0.0400905 7.251 1.04e −12 ***\ncity 0.2219959 0.1872141 1.186 0.2−36\n−−−\nSignif. codes: 0 Ô***Õ 0.001 Ô**Õ 0.01 Ô*Õ 0.05 Ô.Õ 0.1 Ô Õ 1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 302,
      "chunk_index": 4
    }
  },
  {
    "text": "truncate and estimate: limited dependent variables 303\nResidual standard error: 2.409 on 747 degrees of freedom\nMultiple R squared: 0.4515, Adjusted R squared: 0.4478\nF statisti − c : 123 on 5 and 747 DF, p val − ue: < 2.2e 16\n− − −\nIn fact, it seems like both matter, but we should use the selection equa-\ntion approach of Heckman, in two stages.\n> res = selection(lfp ~ age + I(age^2) + faminc + kids + educ,\nwage ~ exper + I(exper^2) + educ + city , data=Mroz87, method = \"2step\" )\n> summary(res)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 303,
      "chunk_index": 0
    }
  },
  {
    "text": "> summary(res)\n−\nT\n−\no\n−\nb\n−\ni\n−\nt\n−−2−−\nm\n−\no\n−\nd\n−\ne\n−\nl\n−−\n(\n−\ns\n−\na\n−\nm\n−\np\n−\nl\n−\ne\n−−\ns\n−\ne\n−\nl\n−\ne\n−\nc\n−\nt\n−\nio\n−−\nn\n−−\nm\n−\no\n−\nd\n−\ne\n−\nl\n−\n)\n−−−−−−\n2 step Heckman / heckit estimation\n7−53 observations (325 censored and 428 observed)\nand 14 free parameters (df = 740)\nProbit selection equation:\nEstimate Std. Error t value Pr(>|t|)\n(Intercept) 4.157e+00 1.402e+00 2.965 0.003127 **\nage −1.854e 01 6.597e 02 −2.810 0.005078 **\nI(age^2) 2.426e −03 7.735e −04 3.136 0.001780\n**",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 303,
      "chunk_index": 1
    }
  },
  {
    "text": "I(age^2) 2.426e −03 7.735e −04 3.136 0.001780\n**\nfaminc −4.580e −06 4.206e −06 −1.089 0.276544\nkidsTRUE 4.490e −01 1.309e −01 3.430 0.000638 ***\neduc −9.818e −02 2.298e −02 −4.272 2.19e 05 ***\n− − −\nOutcome equation:\nEstimate Std. Error t value Pr(>|t|)\n(Intercept) 0.9712003 2.0593505 0.472 0.637\nexper −0.0210610 0.0624646 −0.337 0.736\nI(exper^2) 0.0001371 0.0018782 0.073 0.942\neduc 0.4170174 0.1002497 4.160 3.56e 05 ***\ncity 0.4438379 0.3158984 1.405 0.1−60",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 303,
      "chunk_index": 2
    }
  },
  {
    "text": "city 0.4438379 0.3158984 1.405 0.1−60\nMultiple R Squared:0.1264, Adjusted R Squared:0.116\n− −\nError terms:\nEstimate Std. Error t value Pr(>|t|)\ninvMillsRatio 1.098 1.266 0.867 0.386\nsigma\n−3.200\nNA\n−\nNA NA\nrho 0.343 NA NA NA\n−\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n11.6.3 Endogeity – Some Theory to Wrap Up\nEndogeneity may be technically expressed as arising from a correlation\nof the independent variables and the error term in a regression. This can\nbe stated as:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 303,
      "chunk_index": 3
    }
  },
  {
    "text": "be stated as:\nY = β (cid:48) X+u, E(X u) = 0\n· (cid:54)\nThis can happen in many ways:\n1 . Measurement error: If X is measured in error, we have X˜ = X+e. The\nthe regression becomes\nY = β +β (X˜ e)+u = β +β X˜ +(u β e) = β +β X˜ +v\n0 1 0 1 1 0 1\n− −\nWe see that\nE(X˜ v) = E[(X+e)(u β e)] = β E(e2) = β Var(e) = 0\n1 1 1\n· − − − (cid:54)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 303,
      "chunk_index": 4
    }
  },
  {
    "text": "304 data science: theories, models, algorithms, and analytics\n2 . Omitted variables: Suppose the true model is\nY = β +β X +β X +u\n0 1 1 2 2\nbut we do not have X , which happens to be correlated with X , then\n2 1\nit will be subsumed in the error term and no longer will E(X u) =\ni\n·\n0, i.\n∀\n3 . Simultaneity: This occurs when Y and X are jointly determined. For\nexample, high wages and high education go together. Or, advertising",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 304,
      "chunk_index": 0
    }
  },
  {
    "text": "and sales coincide. Or that better start-up firms tend to receive syndi-\ncation. The structural form of these settings may be written as:\nY = β +β X+u, X = α +α Y+v\n0 1 0 1\nThe solution to these equations gives the reduced-form version of the\nmodel.\nβ +β α βv+u α +α β v+α u\nY = 0 1 0 + , X = 0 1 0 + 1\n1 α β 1 α β 1 α β 1 α β\n1 1 1 1 1 1 1 1\n− − − −\nFrom which we can compute the endogeneity result.\n(cid:18) (cid:19)\nv+α u α\nCov(X,u) = Cov 1 ,u = 1 Var(u)\n1 α β 1 α β ·\n1 1 1 1\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 304,
      "chunk_index": 1
    }
  },
  {
    "text": "12\nRiding the Wave: Fourier Analysis\n12.1 Introduction\nFourier analysis comprises many different connnections between infinite\nseries, complex numbers, vector theory, and geometry. We may think\nof different applications: (a) fitting economic time series, (b) pricing op-\ntions, (c) wavelets, (d) obtaining risk-neutral pricing distributions via\nFourier inversion.\n12.2 Fourier Series\n12.2.1 Basic stuff\nFourier series are used to represent periodic time series by combinations",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 305,
      "chunk_index": 0
    }
  },
  {
    "text": "of sine and cosine waves. The time it takes for one cycle of the wave is\ncalled the “period” T of the wave. The “frequency” f of the wave is the\nnumber of cycles per second, hence,\n1\nf =\nT\n12.2.2 The unit circle\nWe need some basic geometry on the unit circle.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 305,
      "chunk_index": 1
    }
  },
  {
    "text": "306 data science: theories, models, algorithms, and analytics\na\na sin!\n!\na cos!\nThis circle is the unit circle if a = 1. There is a nice link between the unit\ncircle and the sine wave. See the next figure for this relationship.\n+1\nf(!)\n!\n\" 2\"\n-1\nHence, as we rotate through the angles, the height of the unit vector on\nthe circle traces out the sine wave. In general for radius a, we get a sine\nwave with amplitude a, or we may write:\nf(θ) = asin(θ) ( 12 . 1 )\n12.2.3 Angular velocity",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 306,
      "chunk_index": 0
    }
  },
  {
    "text": "f(θ) = asin(θ) ( 12 . 1 )\n12.2.3 Angular velocity\nVelocity is distance per time (in a given direction). For angular velocity\nwe measure distance in degrees, i.e. degrees per unit of time. The usual\nsymbol for angular velocity is ω. We can thus write\nθ\nω = , θ = ωT\nT\n121\nHence, we can state the function in equation ( . ) in terms of time as\nfollows\nf(t) = asinωt",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 306,
      "chunk_index": 1
    }
  },
  {
    "text": "riding the wave: fourier analysis 307\n12.2.4 Fourier series\nA Fourier series is a collection of sine and cosine waves, which when\nsummed up, closely approximate any given waveform. We can express\nthe Fourier series in terms of sine and cosine waves\n∞\n∑\nf(θ) = a + (a cosnθ+b sinnθ)\n0 n n\nn=1\n∞\n∑\nf(t) = a + (a cosnωt+b sinnωt)\n0 n n\nn=1\nThe a is needed since the waves may not be symmetric around the x-\n0\naxis.\n12.2.5 Radians\nDegrees are expressed in units of radians. A radian is an angle defined",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 307,
      "chunk_index": 0
    }
  },
  {
    "text": "in the following figure.\na a\na\n572958\nThe angle here is a radian which is equal to . degrees (approxi-\n60\nmately). This is slightly less than degrees as you would expect to get\nwith an equilateral triangle. Note that (since the circumference is 2πa)\n57.2958π = 57.2958 3.142 = 180 degrees.\n×\nSo now for the unit circle\n2π = 360 (degrees)\n360\nω =\nT\n2π\nω =\nT\nHence, we may rewrite the Fourier series equation as:\n∞\n∑\nf(t) = a + (a cosnωt+b sinnωt)\n0 n n\nn=1\n∞ (cid:18) (cid:19)\n∑ 2πn 2πn",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 307,
      "chunk_index": 1
    }
  },
  {
    "text": "0 n n\nn=1\n∞ (cid:18) (cid:19)\n∑ 2πn 2πn\n= a + a cos t+b sin t\n0 n n\nT T\nn=1\nSo we now need to figure out how to get the coefficients a ,a ,b .\n0 n n\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 307,
      "chunk_index": 2
    }
  },
  {
    "text": "308 data science: theories, models, algorithms, and analytics\n12.2.6 Solving for the coefficients\nWe start by noting the interesting phenomenon that sines and cosines are\northogonal, i.e. their inner product is zero. Hence,\n(cid:90) T\nsin(nt).cos(mt) dt = 0, n,m ( 12 . 2 )\n∀\n0\n(cid:90) T\nsin(nt).sin(mt) dt = 0, n = m ( 12 . 3 )\n∀ (cid:54)\n0\n(cid:90) T\ncos(nt).cos(mt) dt = 0, n = m ( 12 . 4 )\n∀ (cid:54)\n0\nWhat this means is that when we multiply one wave by another, and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 308,
      "chunk_index": 0
    }
  },
  {
    "text": "then integrate the resultant wave from 0 to T (i.e. over any cycle, so we\ncould go from say T/2 to +T/2 also), then we get zero, unless the two\n−\nwaves have the same frequency. Hence, the way we get the coefficients\nof the Fourier series is as follows. Integrate both sides of the series in\nequation ( 12 . 2 ) from 0 to T, i.e.\n(cid:90) T (cid:90) T (cid:90) T (cid:34) ∞ (cid:35)\n∑\nf(t) = a dt+ (a cosnωt+b sinnωt) dt\n0 n n\n0 0 0 n=1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 308,
      "chunk_index": 1
    }
  },
  {
    "text": "0 n n\n0 0 0 n=1\nExcept for the first term all the remaining terms are zero (integrating a\nsine or cosine wave over its cycle gives net zero). So we get\n(cid:90) T\nf(t) dt = a T\n0\n0\nor\n1 (cid:90) T\na = f(t) dt\n0\nT\n0\nNow lets try another integral, i.e.\n(cid:90) T (cid:90) T\nf(t)cos(ωt) = a cos(ωt) dt\n0\n0 0\n(cid:90) T (cid:34) ∞ (cid:35)\n∑\n+ (a cosnωt+b sinnωt)cos(ωt) dt\nn n\n0 n=1\nHere, all terms are zero except for the term in a cos(ωt)cos(ωt), be-\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 308,
      "chunk_index": 2
    }
  },
  {
    "text": "1\ncause we are multiplying two waves (pointwise) that have the same fre-\nquency. So we get\n(cid:90) T (cid:90) T\nf(t)cos(ωt) = a cos(ωt)cos(ωt) dt\n1\n0 0\nT\n= a\n1\n2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 308,
      "chunk_index": 3
    }
  },
  {
    "text": "riding the wave: fourier analysis 309\nHow? Note here that for unit amplitude, integrating cos(ωt) over one\ncycle will give zero. If we multiply cos(ωt) by itself, we flip all the wave\nsegments from below to above the zero line. The product wave now fills\nout half the area from 0 to T, so we get T/2. Thus\n2 (cid:90) T\na = f(t)cos(ωt)\n1\nT\n0\nWe can get all a this way - just multiply by cos(nωt) and integrate. We\nn\ncan also get all b this way - just multiply by sin(nωt) and integrate.\nn",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 309,
      "chunk_index": 0
    }
  },
  {
    "text": "n\nThis forms the basis of the following summary results that give the\ncoefficients of the Fourier series.\n1 (cid:90) T/2 1 (cid:90) T\na = f(t) dt = f(t) dt ( 12 . 5 )\n0\nT T\nT/2 0\n−\n1 (cid:90) T/2 2 (cid:90) T\na = f(t)cos(nωt) dt = f(t)cos(nωt) dt ( 12 . 6 )\nn\nT/2\nT/2\nT\n0\n−\n1 (cid:90) T/2 2 (cid:90) T\nb = f(t)sin(nωt) dt = f(t)sin(nωt) dt ( 12 . 7 )\nn\nT/2\nT/2\nT\n0\n−\n12.3 Complex Algebra\nJust for fun, recall that\n∞\n∑ 1\ne = .\nn!\nn=0\nand\n∞\neiθ = ∑ 1 (iθ)n\nn!\nn=0\n1 1\ncos(θ) = 1+0.θ θ2+0.θ3+ θ2+...",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 309,
      "chunk_index": 1
    }
  },
  {
    "text": "n!\nn=0\n1 1\ncos(θ) = 1+0.θ θ2+0.θ3+ θ2+...\n− 2! 4!\n1\nisin(θ) = 0+iθ+0.θ2 iθ3+0.θ4+...\n− 3!\nWhich leads into the famous Euler’s formula:\neiθ = cosθ+isinθ ( 12 . 8 )\nand the corresponding\ne − iθ = cosθ isinθ ( 12 . 9 )\n−\nRecall also that cos( θ) = cos(θ). And sin( θ) = sin(θ). Note also\n− − −\nthat if θ = π, then\ne\n−\niπ = cos(π) isin(π) = 1+0\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 309,
      "chunk_index": 2
    }
  },
  {
    "text": "310 data science: theories, models, algorithms, and analytics\nwhich can be written as\ne − iπ +1 = 0\nan equation that contains five fundamental mathematical constants:\ni,π,e,0,1 , and three operators +, ,= .\n{ } { − }\n12.3.1 From Trig to Complex\n128 129\nUsing equations ( . ) and ( . ) gives\n1\ncosθ = (eiθ +e − iθ) ( 12 . 10 )\n2\n1\nsinθ = i(eiθ e − iθ) ( 12 . 11 )\n2 −\nNow, return to the Fourier series,\n∞\nf(t) = a + ∑ (a cosnωt+b sinnωt) ( 12 . 12 )\n0 n n\nn=1\n∞ (cid:18) (cid:19)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 310,
      "chunk_index": 0
    }
  },
  {
    "text": "0 n n\nn=1\n∞ (cid:18) (cid:19)\n= a 0 + ∑ a n 1 (einωt+e − inωt)+b n 1 (einωt e − inωt) ( 12 . 13 )\n2 2i −\nn=1\n∞\n(cid:16) (cid:17)\n= a 0 + ∑ A n einωt+B n e − inωt ( 12 . 14 )\nn=1\nwhere\n1 (cid:90) T\nA = f(t)e inωt dt\nn −\nT\n0\n1 (cid:90) T\nB = f(t)einωt dt\nn\nT\n0\nHow? Start with\n∞ (cid:18) (cid:19)\nf(t) = a + ∑ a 1 (einωt+e inωt)+b 1 (einωt e inωt)\n0 n − n −\n2 2i −\nn=1\nThen\n∞ (cid:18) (cid:19)\nf(t) = a + ∑ a 1 (einωt+e inωt)+b i (einωt e inωt)\n0 n 2 − n 2i2 − −\nn=1\n∞ (cid:18) (cid:19)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 310,
      "chunk_index": 1
    }
  },
  {
    "text": "0 n 2 − n 2i2 − −\nn=1\n∞ (cid:18) (cid:19)\n= a + ∑ a 1 (einωt+e inωt)+b i (einωt e inωt)\n0 n − n −\n2 2 −\nn=1 −\n∞ (cid:18) (cid:19)\nf(t) = a 0 + ∑ 1 (a n ib n )einωt+ 1 (a n +ib n )e − inωt ( 12 . 15 )\n2 − 2\nn=1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 310,
      "chunk_index": 2
    }
  },
  {
    "text": "riding the wave: fourier analysis 311\n128 129\nNote that from equations ( . ) and ( . ),\n2 (cid:90) T\na = f(t)cos(nωt) dt ( 12 . 16 )\nn\nT\n0\n2 (cid:90) T 1\n= f(t) [einωt+e − inωt] dt ( 12 . 17 )\nT 2\n0\n1 (cid:90) T\na n = f(t)[einωt+e − inωt] dt ( 12 . 18 )\nT\n0\nIn the same way, we can handle b , to get\nn\n2 (cid:90) T\nb = f(t)sin(nωt) dt ( 12 . 19 )\nn\nT\n0\n2 (cid:90) T 1\n= f(t) [einωt e − inωt] dt ( 12 . 20 )\nT 2i −\n0\n1 1 (cid:90) T\n= f(t)[einωt e − inωt] dt ( 12 . 21 )\ni T −\n0\nSo that\n1 (cid:90) T",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 311,
      "chunk_index": 0
    }
  },
  {
    "text": "i T −\n0\nSo that\n1 (cid:90) T\nib n = f(t)[einωt e − inωt] dt ( 12 . 22 )\nT −\n0\n1218 1222\nSo from equations ( . ) and ( . ), we get\n1 1 (cid:90) T\n(a n ib n ) = f(t)e − inωt dt A n ( 12 . 23 )\n2 − T ≡\n0\n1 1 (cid:90) T\n(a +ib ) = f(t)einωt dt B ( 12 . 24 )\nn n n\n2 T ≡\n0\n1215\nPut these back into equation ( . ) to get\n∞ (cid:18) (cid:19) ∞\nf(t) = a + ∑ 1 (a ib )einωt+ 1 (a +ib )e inωt = a + ∑ (cid:16) A einωt+B e inωt (cid:17)\n0 n n n n − 0 n n −\n2 − 2\nn=1 n=1\n1225\n( . )\n12.3.2 Getting rid of a\n0",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 311,
      "chunk_index": 1
    }
  },
  {
    "text": "n=1 n=1\n1225\n( . )\n12.3.2 Getting rid of a\n0\nNote that if we expand the range of the first summation to start from\nn = 0, then we have a term A ei0ωt = A a . So we can then write our\n0 0 0\n≡\nexpression as\n∞ ∞\nf(t) = ∑ A n einωt+ ∑ B n e − inωt (sum of A runs from zero)\nn=0 n=1\n12.3.3 Collapsing and Simplifying\nSo now we want to collapse these two terms together. Lets note that\n2 1\n∑ xn = x1+x2 = ∑− x n = x2+x1\n−\nn=1 n= 2\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 311,
      "chunk_index": 2
    }
  },
  {
    "text": "312 data science: theories, models, algorithms, and analytics\nApplying this idea, we get\n∞ ∞\nf(t) = ∑ A n einωt+ ∑ B n e − inωt ( 12 . 26 )\nn=0 n=1\n∞ 1\n= ∑ A einωt+ ∑− B einωt ( 12 . 27 )\nn ( n)\nn=0 n= ∞ −\n−\nwhere\n1 (cid:90) T\nB = f(t)e inωt dt = A\n(\n−\nn)\nT 0\n− n\n∞\n= ∑ C einωt ( 12 . 28 )\nn\nn= ∞\n−\nwhere\n1 (cid:90) T\nC = f(t)e inωt dt\nn −\nT\n0\nwhere we just renamed A to C for clarity. The big win here is that we\nn n\nhave been able to subsume a ,a ,b all into one coefficient set C . For\n0 n n n\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 312,
      "chunk_index": 0
    }
  },
  {
    "text": "0 n n n\n{ }\ncompleteness we write\n∞ ∞\nf(t) = a + ∑ (a cosnωt+b sinnωt) = ∑ C einωt\n0 n n n\nn=1 n= ∞\n−\nThis is the complex number representation of the Fourier series.\n12.4 Fourier Transform\nThe FT is a cool technique that allows us to go from the Fourier series,\nwhich needs a period T to waves that are aperiodic. The idea is to sim-\nply let the period go to infinity. Which means the frequency gets very\nsmall. We can then sample a slice of the wave to do analysis.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 312,
      "chunk_index": 1
    }
  },
  {
    "text": "We will replace f(t) with g(t) because we now need to use f or ∆f to\ndenote frequency. Recall that\n2π\nω = = 2πf, nω = 2πf\nn\nT\nTo recap\n∞ ∞\ng(t) = ∑ C einωt = ∑ C ei2πft ( 12 . 29 )\nn n\nn= ∞ n= ∞\n− −\n1 (cid:90) T\nC n = g(t)e − inωt dt ( 12 . 30 )\nT\n0\nThis may be written alternatively in frequency terms as follows\n(cid:90) T/2\nC = ∆f g(t)e i2πfnt dt\nn −\nT/2\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 312,
      "chunk_index": 2
    }
  },
  {
    "text": "riding the wave: fourier analysis 313\nwhich we substitute into the formula for g(t) and get\n∞ (cid:20) (cid:90) T/2 (cid:21)\ng(t) = ∑ ∆f g(t)e i2πfnt dt einωt\n−\nn= ∞ T/2\n− −\nTaking limits\n∞ (cid:20)(cid:90)\nT/2\n(cid:21)\ng(t) = lim ∑ g(t)e − i2πfnt dt ei2πfnt∆f\nT → ∞ n= − ∞ − T/2\ngives a double integral\n(cid:90) ∞ (cid:20)(cid:90) ∞ (cid:21)\ng(t) = g(t)e i2πft dt ei2πft df\n−\n∞ ∞\n− (cid:124) − (cid:123)(cid:122) (cid:125)\nG(f)\nThe dt is for the time domain and the df for the frequency domain.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 313,
      "chunk_index": 0
    }
  },
  {
    "text": "Hence, the Fourier transform goes from the time domain into the fre-\nquency domain, given by\n(cid:90) ∞\nG(f) = g(t)e i2πft dt\n−\n∞\n−\nThe inverse Fourier transform goes from the frequency domain into the\ntime domain\n(cid:90) ∞\ng(t) = G(f)ei2πft df\n∞\n−\nAnd the Fourier coefficients are as before\n1 (cid:90) T 1 (cid:90) T\nC = g(t)e i2πfnt dt = g(t)e inωt dt\nn − −\nT T\n0 0\nNotice the incredible similarity between the coefficients and the trans-\nform. Note the following:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 313,
      "chunk_index": 1
    }
  },
  {
    "text": "form. Note the following:\n• The coefficients give the amplitude of each component wave.\n• The transform gives the area of component waves of frequency f. You\ncan see this because the transform does not have the divide by T in it.\n• The transform gives for any frequency f, the rate of occurrence of the\ncomponent wave with that frequency, relative to other waves.\n• In short, the Fourier transform breaks a complicated, aperiodic wave\ninto simple periodic ones.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 313,
      "chunk_index": 2
    }
  },
  {
    "text": "into simple periodic ones.\nThe spectrum of a wave is a graph showing its component frequen-\ncies, i.e. the quantity in which they occur. It is the frequency components\nof the waves. But it does not give their amplitudes.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 313,
      "chunk_index": 3
    }
  },
  {
    "text": "314 data science: theories, models, algorithms, and analytics\n12.4.1 Empirical Example\nWe can use the Fourier transform function in R to compute the main\ncomponent frequencies of the times series of interest rate data as follows:\n> rd = read.table(\"tryrates.txt\",header=TRUE)\n> r1 = as.matrix(rd[4])\n> plot(r1,type=\"l\")\n> dr1 = resid(lm(r1 ~ seq(along = r1)))\n> plot(dr1,type=\"l\")\n> y=fft(dr1)\n> plot(abs(y),type=\"l\")\nThe line with\ndr1 = resid(lm(r1 ~ seq(along = r1)))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 314,
      "chunk_index": 0
    }
  },
  {
    "text": "dr1 = resid(lm(r1 ~ seq(along = r1)))\ndetrends the series, and when we plot it we see that its done. We can\nthen subject the detrended line to fourier analysis.\nThe plot of the fit of the detrended one-year interest rates is here:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 314,
      "chunk_index": 1
    }
  },
  {
    "text": "riding the wave: fourier analysis 315\nIts easy to see that the series has short frequencies and long frequencies.\nEssentially there are two factors. If we do a factor analysis of interest\nrates, it turns out we get two factors as well.\n12.5 Application to Binomial Option Pricing\n8\nTo implement the option pricing in Cerny, Exhibit .\n> ifft = function(x) { fft(x,inverse=TRUE)/length(x) }\n> ct = c(599.64,102,0,0)\n> q = c(0.43523,0.56477,0,0)\n> R = 1.0033\n> ifft(fft(ct)*( (4*ifft(q)/R)^3) )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 315,
      "chunk_index": 0
    }
  },
  {
    "text": "> R = 1.0033\n> ifft(fft(ct)*( (4*ifft(q)/R)^3) )\n[1] 81.36464+0i 115.28447-0i 265.46949+0i 232.62076-0i",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 315,
      "chunk_index": 1
    }
  },
  {
    "text": "316 data science: theories, models, algorithms, and analytics\n12.6 Application to probability functions\n12.6.1 Characteristic functions\nA characteristic function of a variable x is given by the expectation of the\nfollowing function of x:\n(cid:90) ∞\nφ(s) = E[eisx] = eisxf(x) dx\n∞\n−\nwhere f(x) is the probability density of x. By Taylor series for eisx we\nhave\n(cid:90) ∞ (cid:90) ∞ 1\neisxf(x) dx = [1+isx+ (isx)2+...]f(x)dx\n∞ ∞ 2\n−\n∑\n∞−\n(is)j\n= m\nj\nj!\nj=0\n1 1\n= 1+(is)m + (is)2m + (is)3m +...\n1 2 3",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 316,
      "chunk_index": 0
    }
  },
  {
    "text": "j!\nj=0\n1 1\n= 1+(is)m + (is)2m + (is)3m +...\n1 2 3\n2 6\nwhere m is the j-th moment.\nj\nIt is therefore easy to see that\n(cid:20) (cid:21)\n1 dφ(s)\nm =\nj ij ds\ns=0\nwhere i = √ 1.\n−\n12.6.2 Finance application\n1993\nIn a paper in , Steve Heston developed a new approach to valuing\nstock and foreign currency options using a Fourier inversion technique.\n2001\nSee also Duffie, Pan and Singleton ( ) for extension to jumps, and\n2002\nChacko and Das ( ) for a generalization of this to interest-rate deriva-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 316,
      "chunk_index": 1
    }
  },
  {
    "text": "tives with jumps.\nLets explore a much simpler model of the same so as to get the idea\nof how we can get at probability functions if we are given a stochastic\nprocess for any security. When we are thinking of a dynamically moving\nfinancial variable (say x ), we are usually interested in knowing what the\nt\nprobability is of this variable reaching a value x at time t = τ, given\nτ\nthat right now, it has value x at time t = 0. Note that τ is the remaining\n0\ntime to maturity.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 316,
      "chunk_index": 2
    }
  },
  {
    "text": "0\ntime to maturity.\nSuppose we have the following financial variable x following a very\nt\nsimple Brownian motion, i.e.\ndx = µ dt+σ dz\nt t",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 316,
      "chunk_index": 3
    }
  },
  {
    "text": "riding the wave: fourier analysis 317\nHere, µ is known as its “drift\" and “sigma” is the volatility. The differen-\ntial equation above gives the movement of the variable x and the term dz\nis a Brownian motion, and is a random variable with normal distribution\nof mean zero, and variance dt.\nWhat we are interested in is the characteristic function of this process.\nThe characteristic function of x is defined as the Fourier transform of x,\ni.e.\n(cid:90)\nF(x) = E[eisx] = eisxf(x)ds",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 317,
      "chunk_index": 0
    }
  },
  {
    "text": "i.e.\n(cid:90)\nF(x) = E[eisx] = eisxf(x)ds\nwhere s is the Fourier variable of integration, and i = √ 1, as usual.\n−\nNotice the similarity to the Fourier transforms described earlier in the\nnote. It turns out that once we have the characteristic function, then we\ncan obtain by simple calculations the probability function for x, as well\nas all the moments of x.\n12.6.3 Solving for the characteristic function\nWe write the characteristic function as F(x,τ;s). Then, using Ito’s lemma\nwe have\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 317,
      "chunk_index": 1
    }
  },
  {
    "text": "we have\n1\ndF = F dx+ F (dx)2 F dt\nx xx τ\n2 −\nF is the first derivative of F with respect to x; F is the second deriva-\nx xx\ntive, and F is the derivative with respect to remaining maturity. Since F\nτ\nis a characteristic (probability) function, the expected change in F is zero.\n1\nE(dF) = µF dt+ σ2F dt F dt = 0\nx xx τ\n2 −\nwhich gives a PDE in (x,τ):\n1\nµF + σ2F F = 0\nx xx τ\n2 −\nWe need a boundary condition for the characteristic function which is\nF(x,τ = 0;s) = eisx",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 317,
      "chunk_index": 2
    }
  },
  {
    "text": "F(x,τ = 0;s) = eisx\nWe solve the PDE by making an educated guess, which is\nF(x,τ;s) = eisx+A(τ)\nwhich implies that when τ = 0, A(τ = 0) = 0 as well. We can see that\nF = isF\nx\nF = s2F\nxx\n−\nF = A F\nτ τ",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 317,
      "chunk_index": 3
    }
  },
  {
    "text": "318 data science: theories, models, algorithms, and analytics\nSubstituting this back in the PDE gives\n1\nµisF σ2s2F A F = 0\nτ\n− 2 −\n1\nµis σ2s2 A = 0\nτ\n− 2 −\ndA 1\n= µis σ2s2\ndτ − 2\n1\ngives: A(τ) = µisτ σ2s2τ, because A(0) = 0\n− 2\nThus we finally have the characteristic function which is\n1\nF(x,τ;s) = exp[isx+µisτ σ2s2τ]\n− 2\n12.6.4 Computing the moments\nIn general, the moments are derived by differentiating the characteristic\nfunction y s and setting s = 0. The k-th moment will be\n(cid:34) (cid:35)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 318,
      "chunk_index": 0
    }
  },
  {
    "text": "(cid:34) (cid:35)\n1 ∂kF\nE[xk] =\nik ∂sk\ns=0\nLets test it by computing the mean (k = 1):\n(cid:20) (cid:21)\n1 ∂F\nE(x) = = x+µτ\ni ∂s\ns=0\nwhere x is the current value x . How about the second moment?\n0\n1 (cid:20) ∂2F (cid:21)\nE(x2) = = σ2τ+(x+µτ)2 = σ2τ+E(x)2\ni2 ∂s2\ns=0\nHence, the variance will be\nVar(x) = E(x2) E(x)2 = σ2τ+E(x)2 E(x2) = σ2τ\n− −\n12.6.5 Probability density function\nIt turns out that we can “invert” the characteristic function to get the pdf",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 318,
      "chunk_index": 1
    }
  },
  {
    "text": "(boy, this characteristic function sure is useful!). Again we use Fourier\ninversion, which result is stated as follows:\n1 (cid:90) ∞\nf(x\nτ\nx\n0\n) = Re[e\n−\nisxτ]F(x\n0\n,τ;s) ds\n| π\n0\nHere is an implementation",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 318,
      "chunk_index": 2
    }
  },
  {
    "text": "riding the wave: fourier analysis 319\n#Model for fourier inversion for arithmetic brownian motion\nx0 = 10\nmu = 10\nsig = 5\ntau = 0.25\ns = (0:10000)/100\nds = s[2]-s[1]\nphi = exp(1i*s*x0+mu*1i*s*tau-0.5*s^2*sig^2*tau)\nx = (0:250)/10\nfx=NULL\nfor ( k in 1:length(x) ) {\ng = sum(as.real(exp(-1i*s*x[k]) * phi * ds))/pi\nprint(c(x[k],g))\nfx = c(fx,g)\n}\nplot(x,fx,type=\"l\")",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 319,
      "chunk_index": 0
    }
  },
  {
    "text": "13\nMaking Connections: Network Theory\n13.1 Overview\nThe science of networks is making deep inroads into business. The term\n“network effect” is being used widely in conceptual terms to define the\ngains from piggybacking on connections in the business world. Using\nthe network to advantage coins the verb “networking” - a new, improved\nuse of the word “schmoozing”. The science of viral marketing and word-\nof-mouth transmission of information is all about exploiting the power",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 321,
      "chunk_index": 0
    }
  },
  {
    "text": "of networks. We are just seeing the beginning - as the cost of the network\nand its analysis drops rapidly, businesses will exploit them more and\nmore.\nNetworks are also useful in understanding how information flows\nin markets. Network theory is also being used by firms to find “com-\nmunities” of consumers so as to partition and focus their marketing\nefforts. There are many wonderful videos by Cornell professor Jon Klein-\nberg on YouTube and elsewhere on the importance of new tools in com-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 321,
      "chunk_index": 1
    }
  },
  {
    "text": "puter science for understanding social networks. He talks of the big\ndifference today in that networks grow organically, not in structured\nfashion as in the past with road, electricity and telecommunication net-\nworks. Modern networks are large, realistic and well-mapped. Think\nabout dating networks and sites like Linked In. A free copy of Klein-\nberg’s book on networks with David Easley may be downloaded at\nhttp://www.cs.cornell.edu/home/kleinber/networks-book/. It is writ-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 321,
      "chunk_index": 2
    }
  },
  {
    "text": "ten for an undergraduate audience and is immensely accessible. There is\nalso material on game theory and auctions in this book.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 321,
      "chunk_index": 3
    }
  },
  {
    "text": "322 data science: theories, models, algorithms, and analytics\n13.2 Graph Theory\nAny good understanding of networks must perforce begin with a digres-\nsion in graph theory. I say digression because its not clear to me yet how\na formal understanding of graph theory should be taught to business\nstudents, but yet, an informal set of ideas is hugely useful in provid-\ning a technical/conceptual framework within which to see how useful",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 322,
      "chunk_index": 0
    }
  },
  {
    "text": "network analysis will be in the coming future of a changing business\nlandscape. Also, it is useful to have a light introduction to the notation\nand terminology in graph theory so that the basic ideas are accessible\nwhen reading further.\nWhat is a graph? It is a picture of a network, a diagram consisting of\nrelationships between entities. We call the entities as vertices or nodes\n(set V) and the relationships are called the edges of a graph (set E).\nHence a graph G is defined as\nG = (V,E)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 322,
      "chunk_index": 1
    }
  },
  {
    "text": "Hence a graph G is defined as\nG = (V,E)\nIf the edges e E of a graph are not tipped with arrows implying some\n∈\ndirection or causality, we call the graph an “undirected” graph. If there\nare arrows of direction then the graph is a “directed” graph.\nIf the connections (edges) between vertices v V have weights on\n∈\nthem, then we call the graph a “weighted graph” else it’s “unweighted”.\nIn an unweighted graph, for any pair of vertices (u,v), we have\n(cid:40)\nw(u,v) = 1, if (u,v) E\nw(u,v) = ∈",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 322,
      "chunk_index": 2
    }
  },
  {
    "text": "(cid:40)\nw(u,v) = 1, if (u,v) E\nw(u,v) = ∈\nw(u,v) = 0, if (u,v) E\n(cid:51)\nIn a weighted graph the value of w(u,v) is unrestricted, and can also be\nnegative.\nDirected graphs can be cyclic or acyclic. In a cyclic graph there is a\npath from a source node that leads back to the node itself. Not so in\nan acyclic graph. The term “dag” is used to connote a “directed acyclic\ngraph”. The binomial option pricing model in finance that you have\nlearnt is an example of a dag.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 322,
      "chunk_index": 3
    }
  },
  {
    "text": "learnt is an example of a dag.\nA graph may be represented by its adjacency matrix. This is simply\nthe matrix A = w(u,v) , u,v. You can take the transpose of this matrix\n{ } ∀\nas well, which in the case of a directed graph will simply reverse the\ndirection of all edges.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 322,
      "chunk_index": 4
    }
  },
  {
    "text": "making connections: network theory 323\n13.3 Features of Graphs\nGraphs have many attributes, such as the number of nodes, and the\ndistribution of links across nodes. The structure of nodes and edges\n(links) determines how connected the nodes are, how flows take place on\nthe network, and the relative importance of each node.\nOne simple bifurcation of graphs suggests two types: (a) random\ngraphs and (b) scale-free graphs. In a beautiful article in the Scientific\n2003",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 323,
      "chunk_index": 0
    }
  },
  {
    "text": "2003\nAmerican, Barabasi and Bonabeau ( ) presented a simple schematic\n131\nto depict these two categories of graphs. See Figure . .\nFigure13.1: Comparisonofran-\ndomandscale-freegraphs. From\nBarabasi,Albert-Laszlo.,andEric\nBonabeau(2003). “Scale-FreeNet-\nworks,”ScientificAmericanMay,\n50–59.\nA random graph may be created by putting in place a set of n nodes\nand then randomly connecting pairs of nodes with some probability\np. The higher this probability the more edges the graph will have. The",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 323,
      "chunk_index": 1
    }
  },
  {
    "text": "distribution of the number of edges each node has will be more or less\nGaussian as there is a mean number of edges (n p), with some range\n·\n131\naround the mean. In Figure . , the graph on the left is a depiction of\nthis, and the distribution of links is shown to be bell-shaped. The left\ngraph is exemplified by the US highway network, as shown in simplified",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 323,
      "chunk_index": 2
    }
  },
  {
    "text": "324 data science: theories, models, algorithms, and analytics\nform. If the number of links of a node are given by a number d, the dis-\ntribution of nodes in a random graph would be f(d) N(µ,σ2), where\n∼\nµ is the mean number of nodes with variance σ2.\nA scale-free graph has a hub and spoke structure. There are a few cen-\ntral nodes that have a large number of links, and most nodes have very\n131\nfew. The distribution of links is shown on the right side of Figure . ,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 324,
      "chunk_index": 0
    }
  },
  {
    "text": "and an exemplar is the US airport network. This distribution is not bell-\nshaped at all, and appears to be exponential. There is of course a mean\nfor this distribution, but the mean is not really representative of the hub\nnodes or the non-hub nodes. Because the mean, i.e., the parameter of\nscale is unrepresentative of the population, the distribution is scale-free,\nand the networks of this type are also known as scale-free networks. The",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 324,
      "chunk_index": 1
    }
  },
  {
    "text": "distribution of nodes in a scale-free graph tends to be approximated by\na power-law distribution, i.e., f(d) d α, where usually, nature seems\n−\n∼\nto have stipulated that 2 α 3, by some curious twist of fate. The\n≤ ≤\nlog-log plot of this distribution is linear, as shown in the right side graph\n131\nin Figure . .\nThe vast majority of networks in the world tend to be scale-free. Why?\n1999\nBarabasi and Albert ( ) developed the Theory of Preferential Attach-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 324,
      "chunk_index": 2
    }
  },
  {
    "text": "ment to explain this phenomenon. The theory is intuitive, and simply\nstates that as a network grows and new nodes are added, the new nodes\ntend to attach to existing nodes that have the most links. Thus influen-\ntial nodes become even more connected, and this evolves into a hub and\nspoke structure.\nThe structure of these graphs determines other properties. For in-\nstance, scale-free graphs are much better at transmission of information,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 324,
      "chunk_index": 3
    }
  },
  {
    "text": "for example. Or for moving air traffic passengers, which is why our air-\nports are arranged thus. But a scale-free network is also susceptible to\ngreater transmission of disease, as is the case with networks of people\nwith HIV. Or, economic contagion. Later in this chapter we will examine\nfinancial network risk by studying the structure of banking networks.\nScale-free graphs are also more robust to random attacks. If a terrorist",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 324,
      "chunk_index": 4
    }
  },
  {
    "text": "group randomly attacks an airport, then unless it hits a hub, very little\ndamage is done. But the network is much more risky when targeted at-\ntacks take place. Which is why our airports and the electricity grid are at\nso much risk.\nThere are many interesting graphs, where the study of basic proper-\nties leads to many quick insights, as we will see in the rest of this chap-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 324,
      "chunk_index": 5
    }
  },
  {
    "text": "making connections: network theory 325\nter. Our of interest, if you are an academic, take a look at Microsoft’s aca-\ndemic research network. See http://academic.research.microsoft.com/\nUsing this I have plotted my own citation and co-author network in Fig-\n132\nure . .\n13.4 Searching Graphs\nThere are two types of search algorithms that are run on graphs - depth-\nfirst-search (DFS) and breadth-first search (BFS). Why do we care about",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 325,
      "chunk_index": 0
    }
  },
  {
    "text": "this? As we will see, DFS is useful in finding communities in social net-\nworks. And BFS is useful in finding the shortest connections in net-\nworks. Ask yourself, what use is that? It should not be hard to come\nup with many answers.\n13.4.1 Depth First Search\nDFS begins by taking a vertex and creating a tree of connected vertices\nfrom the source vertex, recursing downwards until it is no longer possi-\n133\nble to do so. See Figure . for an example of a DFS.\nThe algorithm for DFS is as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 325,
      "chunk_index": 1
    }
  },
  {
    "text": "The algorithm for DFS is as follows:\nfunction DFS(u):\nfor all v in SUCC(u):\nif notvisited (v):\nDFS(v)\nMARK(u)\nThis recursive algorithm results in two subtrees, which are:\nf\n(cid:37)\na b c g\n→ → →\nd\n(cid:38)\ne h i\n→ →\nThe numbers on the nodes show the sequence in which the nodes\nare accessed by the program. The typical output of a DFS algorithm is\nusually slightly less detailed, and gives a simple sequence in which the\nnodes are first visited. An example is provided in the graph package:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 325,
      "chunk_index": 2
    }
  },
  {
    "text": "> library(graph)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 325,
      "chunk_index": 3
    }
  },
  {
    "text": "326 data science: theories, models, algorithms, and analytics\nFigure13.2: Microsoft\nacademicsearchtoolfor\nco-authorshipnetworks. See:\nhttp://academic.research.microsoft.com/.\nThetopchartshowsco-authors,\nthemiddleoneshowscitations,\nandthelastoneshowsmyErdos\nnumber,i.e.,thenumberofhops\nneededtobeconnectedtoPaul\nErdosviamyco-authors. MyErdos\nnumberis3. Interestingly,Iama\nFinanceacademic,butmyshortest\npathtoErdosisthroughComputer\nScienceco-authors,anotherfieldin\nwhichIdabble.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 326,
      "chunk_index": 0
    }
  },
  {
    "text": "making connections: network theory 327\na b c d Figure13.3: Depth-first-search.\n1/12 2/11 3/10 8/9\ne f g\n13/18 5/6 4/7\nh i\n14/17 15/16\n> RNGkind(\"Mersenne Twister\")\n−\n> set .seed( 123 )\n> g 1 < randomGraph( letters [ 1 : 10 ] , 1 : 4 , p=. 3 )\n−\n1\n> g\nA graphNEL graph with undirected edges\n10\nNumber of Nodes =\n21\nNumber of Edges =\n1\n> edgeNames(g )\n[ 1 ] \"a~g\" \"a~i\" \"a~b\" \"a~d\" \"a~e\" \"a~f\" \"a~h\" \"b~f\" \"b~j \"\n[ 10 ] \"b~d\" \"b~e\" \"b~h\" \"c~h\" \"d~e\" \"d~f\" \"d~h\" \"e~f\" \"e~h\"\n[ 19 ] \"f~j \" \"f~h\" \"g~i\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 327,
      "chunk_index": 0
    }
  },
  {
    "text": "[ 19 ] \"f~j \" \"f~h\" \"g~i\"\n> RNGkind()\n1\n[ ] \"Mersenne Twister\" \"Inversion\"\n−\n1\n> DFS(g , \"a\")\na b c d e f g h i j\n0 1 6 2 3 4 8 5 9 7\nNote that the result of a DFS on a graph is a set of trees. A tree is a\nspecial kind of graph, and is inherently acyclic if the graph is acyclic. A\ncyclic graph will have a DFS tree with back edges.\nWe can think of this as partitioning the vertices into subsets of con-\nnected groups. The obvious business application comes from first under-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 327,
      "chunk_index": 1
    }
  },
  {
    "text": "standing why they are different, and secondly from being able to target\nthese groups separately by tailoring business responses to their charac-\nteristics, or deciding to stop focusing on one of them.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 327,
      "chunk_index": 2
    }
  },
  {
    "text": "328 data science: theories, models, algorithms, and analytics\nFirms that maintain data about these networks use algorithms like\nthis to find out “communities”. Within a community, the nearness of\nconnections is then determined using breadth-first-search.\nA DFS also tells you something about the connectedness of the nodes.\nIt shows that every entity in the network is not that far from the others,\nand the analysis often suggests the “small-world’s” phenomenon, or",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 328,
      "chunk_index": 0
    }
  },
  {
    "text": "what is colloquially called “six degrees of separation.” Social networks\nare extremely rich in short paths.\nNow we examine how DFS is implemented in the package igraph,\nwhich we will use throughout the rest of this chapter. Here is the sam-\nple code, which also shows how a graph may be created from a paired-\nvertex list.\n#CREATE A SIMPLE GRAPH\ndf = matrix(c(\"a\" ,\"b\" ,\"b\" ,\"c\" ,\"c\" ,\"g\" ,\n\"f\" ,\"b\" ,\"g\" ,\"d\" ,\"g\" ,\"f\" ,\n\"f\" ,\"e\" ,\"e\" ,\"h\" ,\"h\" ,\"i\") ,ncol= 2 ,byrow=TRUE)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 328,
      "chunk_index": 1
    }
  },
  {
    "text": "g = graph.data.frame(df , directed=FALSE)\nplot(g)\n#DO DEPTH FIRST SEARCH\n−\ndfs(g, \"a\")\n$root\n1 0\n[ ]\n$neimode\n1\n[ ] \"out\"\n$order\n+\n9/9\nvertices , named:\n[ 1 ] a b c g f e h i d\n$order.out\nNULL\n$father\nNULL",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 328,
      "chunk_index": 2
    }
  },
  {
    "text": "making connections: network theory 329\n$dist\nNULL\nWe also plot the graph to see what it appears like and to verify the\n134\nresults. See Figure . .\nFigure13.4: Depth-firstsearchon\nasimplegraphgeneratedfroma\npairednodelist.\n13.4.2 Breadth-first-search\nBFS explores the edges of E to discover (from a source vertex s) all reach-\nable vertices on the graph. It does this in a manner that proceeds to find\na frontier of vertices k distant from s. Only when it has located all such",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 329,
      "chunk_index": 0
    }
  },
  {
    "text": "vertices will the search then move on to locating vertices k+1 away from\nthe source. This is what distinguishes it from DFS which goes all the\nway down, without covering all vertices at a given level first.\nBFS is implemented by just labeling each node with its distance from\n135\nthe source. For an example, see Figure . . It is easy to see that this\nhelps in determining nearest neighbors. When you have a positive re-\nsponse from someone in the population it helps to be able to target the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 329,
      "chunk_index": 1
    }
  },
  {
    "text": "nearest neighbors first in a cost-effective manner. The art lies in defining\nthe edges (connections). For example, a company like Schwab might be\nable to establish a network of investors where the connections are based\non some threshold level of portfolio similarity. Then, if a certain account",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 329,
      "chunk_index": 2
    }
  },
  {
    "text": "330 data science: theories, models, algorithms, and analytics\ndisplays enhanced investment, and we know the cause (e.g. falling in-\nterest rates) then it may be useful to market funds aggressively to all\nconnected portfolios with a BFS range.\na s b c Figure13.5: Breadth-first-search.\n1 0 1 3\n1 2\nd e\nThe algorithm for BFS is as follows:\nfunction BFS(s)\nMARK(s)\nQ = {s}\nT = {s}\nWhile Q ne { }:\nChoose u in Q\nVisit u\nfor each v=SUCC(u):\nMARK(v)\nQ = Q + v\nT = T + (u,v)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 330,
      "chunk_index": 0
    }
  },
  {
    "text": "MARK(v)\nQ = Q + v\nT = T + (u,v)\nBFS also results in a tree which in this case is as follows. The level of\neach tree signifies the distance from the source vertex.\nb\n(cid:37) (cid:38)\ns d e c\n→ → →\na\n(cid:38)\nThe code is as follows:\ndf = matrix(c(\"s\" ,\"a\" ,\"s\" ,\"b\" ,\"s\" ,\"d\" ,\"b\" ,\"e\" ,\"d\" ,\"e\" ,\"e\" ,\"c\") ,\nncol= 2 ,byrow=TRUE)\ng = graph.data.frame(df , directed=FALSE)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 330,
      "chunk_index": 1
    }
  },
  {
    "text": "making connections: network theory 331\nbfs(g, \"a\")\n$root\n1 1\n[ ]\n$neimode\n1\n[ ] \"out\"\n$order\n+\n6/6\nvertices , named:\n[ 1 ] s b d a e c\n$rank\nNULL\n$father\nNULL\n$pred\nNULL\n$succ\nNULL\n$dist\nNULL\nThere is a classic book on graph theory which is a must for anyone\n1983\ninterested in reading more about this: Tarjan ( ) – Its only a little over\n100\npages and is a great example of a lot if material presented very well.\nAnother bible for reference is “Introduction to Algorithms” by Cor-\n2009",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 331,
      "chunk_index": 0
    }
  },
  {
    "text": "2009\nmen, Liserson, and Rivest ( ). You might remember that Ron Rivest is\nthe “R” in the famous RSA algorithm used for encryption.\n13.5 Strongly Connected Components\nDirected graphs are wonderful places in which to cluster members of a\nnetwork. We do this by finding strongly connected components (SCCs)\non such a graph. A SCC is a subgroup of vertices U V in a graph with\n⊂",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 331,
      "chunk_index": 1
    }
  },
  {
    "text": "332 data science: theories, models, algorithms, and analytics\nFigure13.6: Stronglyconnected\na b c d\ncomponents. Theuppergraph\nshowstheoriginalnetworkandthe\nloweroneshowsthecompressed\ne f g networkcomprisingonlytheSCCs.\nThealgorithmtodetermineSCCs\nreliesontwoDFSs. Canyouseea\nfurtherSCCinthesecondgraph?\nThereshouldnotbeone.\nh i j\nabe cd\nfg\nhi\nthe property that for all pairs of its vertices (u,v) U, both vertices are\n∈\nreachable from each other.\n136",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 332,
      "chunk_index": 0
    }
  },
  {
    "text": "∈\nreachable from each other.\n136\nFigure . shows an example of a graph broken down into its strongly\nconnected components.\nThe SCCs are extremely useful in partitioning a graph into tight units.\nIt presents local feedback effects. What it means is that targeting any one\nmember of a SCC will effectively target the whole, as well as move the\nstimulus across SCCs.\nThe most popular package for graph analysis has turned out to be\nigraph. It has versions in R, C, and Python. You can generate and plot",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 332,
      "chunk_index": 1
    }
  },
  {
    "text": "random graphs in R using this package. Here is an example.\n> library(igraph)\n> g < erdos. renyi .game( 20 , 1/20 )\n−\n> g\n20\nVertices :\n8\nEdges:\nDirected : FALSE\nEdges:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 332,
      "chunk_index": 2
    }
  },
  {
    "text": "making connections: network theory 333\n0 6 7\n[ ]\n−−\n1 0 10\n[ ]\n−−\n2 0 11\n[ ]\n−−\n3 10 14\n[ ]\n−−\n4 6 16\n[ ]\n−−\n5 11 17\n[ ]\n−−\n6 9 18\n[ ]\n−−\n7 16 19\n[ ]\n−−\n> clusters (g)\n$membership\n1 0 1 2 3 4 5 6 6 7 8 0 0 9 10 0 11 6 0 8\n[ ]\n20 6\n[ ]\n$csize\n1 5 1 1 1 1 1 4 1 2 1 1 1\n[ ]\n$no\n1 12\n[ ]\n> plot .igraph(g)\n137\nIt results in the plot in Figure . .\n13.6 Dijkstra’s Shortest Path Algorithm\nThis is one of the most well-known algorithms in theoretical computer",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 333,
      "chunk_index": 0
    }
  },
  {
    "text": "science. Given a source vertex on a weighted, directed graph, it finds\nthe shortest path to all other nodes from source s. The weight between\ntwo vertices is denoted w(u,v) as before. Dijkstra’s algorithm works for\ngraphs where w(u,v) 0. For negative weights, there is the Bellman-\n≥\nFord algorithm. The algorithm is as follows.\nfunction DIJKSTRA(G,w,s)\nS = { }\n%S = Set of vertices whose shortest paths from\n%source s have been found\nQ = V(G)\nwhile Q notequal { } :\nu = getMin(Q)\nS = S + u",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 333,
      "chunk_index": 1
    }
  },
  {
    "text": "334 data science: theories, models, algorithms, and analytics\nFigure13.7: Findingconnected\ncomponentsonagraph.\n3\n15\n13\n4\n8\n17\n10\n19\n18\n11 12\n9\n14 1\n6\n7\n0\n5\n2\n16\nQ = Q u\n−\nfor each vertex v in SUCC(u):\nif d[v] > d[u]+w(u,v) then:\nd[v] = d[u]+w(u,v)\nPRED(v) = u\nAn example of a graph on which Dijkstra’s algorithm has been ap-\n138\nplied is shown in Figure . .\nThe usefulness of this has been long exploited in operations for air-\nlines, designing transportation plans, optimal location of health-care",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 334,
      "chunk_index": 0
    }
  },
  {
    "text": "centers, and in the every day use of map-quest.\nYou can use igraph to determine shortest paths in a network. Here is\nan example using the package. First we see how to enter a graph, then\nprocess it for shortest paths.\n> el = matrix(nc=3, byrow=TRUE, c(0,1,8, 0,3,4, 1,3,3, 3,1,1, 1,2,1,\n1,4,7, 3,4,4, 2,4,1))\n> el\n[,1] [,2] [,3]\n[1,] 0 1 8\n[2,] 0 3 4\n[3,] 1 3 3\n[4,] 3 1 1\n[5,] 1 2 1\n[6,] 1 4 7\n[7,] 3 4 4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 334,
      "chunk_index": 1
    }
  },
  {
    "text": "making connections: network theory 335\na b Figure13.8: Dijkstra’salgorithm.\n1\n8/5 6\n8\ns 3 1 7 1\n0\n4\nc d\n4\n4 8/7\n[8,] 2 4 1\n> g = add.edges(graph.empty(5), t(el[,1:2]), weight=el[,3])\n> shortest.paths(g)\n[,1] [,2] [,3] [,4] [,5]\n[1,] 0 5 6 4 7\n[2,] 5 0 1 1 2\n[3,] 6 1 0 2 1\n[4,] 4 1 2 0 3\n[5,] 7 2 1 3 0\n> get.shortest.paths(g,0)\n[[1]]\n[1] 0\n[[2]]\n[1] 0 3 1\n[[3]]\n[1] 0 3 1 2\n[[4]]\n[1] 0 3\n[[5]]\n[1] 0 3 1 2 4\nHere is another example.\n> el < matrix(nc=3, byrow=TRUE,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 335,
      "chunk_index": 0
    }
  },
  {
    "text": "> el < matrix(nc=3, byrow=TRUE,\n− c(0,1,0, 0,2,2, 0,3,1, 1,2,0, 1,4,5, 1,5,2, 2,1,1, 2,3,1,\n2,6,1, 3,2,0, 3,6,2, 4,5,2, 4,7,8, 5,2,2, 5,6,1, 5,8,1,\n5,9,3, 7,5,1, 7,8,1, 8,9,4) )\n> el\n[,1] [,2] [,3]\n[1,] 0 1 0\n[2,] 0 2 2\n[3,] 0 3 1\n[4,] 1 2 0\n[5,] 1 4 5\n[6,] 1 5 2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 335,
      "chunk_index": 1
    }
  },
  {
    "text": "336 data science: theories, models, algorithms, and analytics\nFigure13.9: Networkforcomputa-\n9 tionofshortestpathalgorithm\n6\n3\n7\n1\n5\n0\n4\n8\n2\n[7,] 2 1 1\n[8,] 2 3 1\n[9,] 2 6 1\n[10,] 3 2 0\n[11,] 3 6 2\n[12,] 4 5 2\n[13,] 4 7 8\n[14,] 5 2 2\n[15,] 5 6 1\n[16,] 5 8 1\n[17,] 5 9 3\n[18,] 7 5 1\n[19,] 7 8 1\n[20,] 8 9 4\n> g = add.edges(graph.empty(10), t(el[,1:2]), weight=el[,3])\n> plot.igraph(g)\n> shortest.paths(g)\n[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,] 0 0 0 0 4 2 1 3 3 5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 336,
      "chunk_index": 0
    }
  },
  {
    "text": "[1,] 0 0 0 0 4 2 1 3 3 5\n[2,] 0 0 0 0 4 2 1 3 3 5\n[3,] 0 0 0 0 4 2 1 3 3 5\n[4,] 0 0 0 0 4 2 1 3 3 5\n[5,] 4 4 4 4 0 2 3 3 3 5\n[6,] 2 2 2 2 2 0 1 1 1 3\n[7,] 1 1 1 1 3 1 0 2 2 4\n[8,] 3 3 3 3 3 1 2 0 1 4\n[9,] 3 3 3 3 3 1 2 1 0 4\n[10,] 5 5 5 5 5 3 4 4 4 0\n> get.shortest.paths(g,4)\n[[1]]\n[1] 4 5 1 0",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 336,
      "chunk_index": 1
    }
  },
  {
    "text": "making connections: network theory 337\n[[2]]\n[1] 4 5 1\n[[3]]\n[1] 4 5 2\n[[4]]\n[1] 4 5 2 3\n[[5]]\n[1] 4\n[[6]]\n[1] 4 5\n[[7]]\n[1] 4 5 6\n[[8]]\n[1] 4 5 7\n[[9]]\n[1] 4 5 8\n[[10]]\n[1] 4 5 9\n> average.path.length(g)\n[1] 2.051724\n13.6.1 Plotting the network\nOne can also use different layout standards as follows: Here is the exam-\nple:\n> library(igraph)\n> el < matrix(nc=3, byrow=TRUE,\n+ − c(0,1,0, 0,2,2, 0,3,1, 1,2,0, 1,4,5, 1,5,2, 2,1,1, 2,3,1,\n+ 2,6,1, 3,2,0, 3,6,2, 4,5,2, 4,7,8, 5,2,2, 5,6,1, 5,8,1,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 337,
      "chunk_index": 0
    }
  },
  {
    "text": "+ 5,9,3, 7,5,1, 7,8,1, 8,9,4) )\n> g = add.edges(graph.empty(10), t(el[,1:2]), weight=el[,3])\n#GRAPHING MAIN NETWORK\ng = simplify(g)\nV(g)$name = seq(vcount(g))\nl = layout.fruchterman.reingold(g)\n#l = layout.kamada.kawai(g)\n# = layout. circle(g)\nl = layout.norm(l , 1,1, 1,1)\n− −\n#pdf(file=\"network_plot.pdf\")\nplot(g, layout=l , vertex.size=2, vertex.label=NA, vertex.color=\"#ff000033\",\nedge.color=\"grey\", edge.arrow.size=0.3, rescale=FALSE,\nxlim=range(l[,1]), ylim=range(l[ ,2]))\n1310",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 337,
      "chunk_index": 1
    }
  },
  {
    "text": "xlim=range(l[,1]), ylim=range(l[ ,2]))\n1310\nThe plots are shown in Figures . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 337,
      "chunk_index": 2
    }
  },
  {
    "text": "338 data science: theories, models, algorithms, and analytics\nFigure13.10: Plotusingthe\nFruchterman-RheingoldandCircle\nlayouts\n13.7 Degree Distribution\nThe degree of a node in the network is the number of links it has to\nother nodes. The probability distribution of the nodes is known as the\ndegree distribution. In an undirected network, this is based on the num-\nber of edges a node has, but in a directed network, we have a distribu-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 338,
      "chunk_index": 0
    }
  },
  {
    "text": "tion for in-degree and another for out-degree. Note that the weights on\nthe edges are not relevant for computing the degree distribution, though\nthere may be situations in which one might choose to avail of that infor-\nmation as well.\n#GENERATE RANDOM GRAPH\n30 0 1\ng = erdos. renyi .game( , . )\nplot .igraph(g)\nprint(g)\n30 41\nIGRAPH U Erdos renyi (gnp) graph\n−−− −−\n+ attr : name (g/c) , type (g/c) , loops (g/ l ) , p (g/n)\n+ edges:\n1 1 9 2 9 7 10 7 12 8 12 5 13 6 14 11 14\n[ ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 338,
      "chunk_index": 1
    }
  },
  {
    "text": "1 1 9 2 9 7 10 7 12 8 12 5 13 6 14 11 14\n[ ]\n−− −− −− −− −− −− −− −−\n9 5 15 12 15 13 16 15 16 1 17 18 19 18 20 2 21\n[ ]\n−− −− −− −− −− −− −− −−\n17 10 21 18 21 14 22 4 23 6 23 9 23 11 23 9 24\n[ ]\n−− −− −− −− −− −− −− −−\n25 20 24 17 25 13 26 15 26 3 27 5 27 6 27 16 27\n[ ]\n−− −− −− −− −− −− −− −−\n33 18 27 19 27 25 27 11 28 13 28 22 28 24 28 5 29\n[ ]\n−− −− −− −− −− −− −− −−\n41 7 29\n[ ]\n−−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 338,
      "chunk_index": 2
    }
  },
  {
    "text": "making connections: network theory 339\n> clusters (g)\n$membership\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n[ ]\n$csize\n1 29 1\n[ ]\n$no\n1 2\n[ ]\n1311\nThe plot is shown in Figure . .\nFigure13.11: PlotoftheErdos-\nRenyirandomgraph\nWe may compute the degree distribution with some minimal code.\n#COMPUTE DEGREE DISTRIBUTION\ndd = degree. distribution (g)\ndd = as.matrix(dd)\nd = as.matrix(seq( 0 ,max(degree(g))))\nplot(d,dd,type=\"l\" ,lwd= 3 ,col=\"blue\" ,ylab=\"Probability\" ,xlab=\"Degree\")",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 339,
      "chunk_index": 0
    }
  },
  {
    "text": "> sum(dd)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 339,
      "chunk_index": 1
    }
  },
  {
    "text": "340 data science: theories, models, algorithms, and analytics\n1 1\n[ ]\nThe resulting plot of the probability distribution is shown in Figure\n1312\n. .\nFigure13.12: Plotofthedegree\ndistributionoftheErdos-Renyi\nrandomgraph\n13.8 Diameter\nThe diameter of a graph is the longest shortest distance between any two\nnodes, across all nodes. This is easily computed as follows for the graph\nwe examined in the previous section.\n> print(diameter(g))\n1 7\n[ ]\nWe may cross-check this as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 340,
      "chunk_index": 0
    }
  },
  {
    "text": "1 7\n[ ]\nWe may cross-check this as follows:\n> res = shortest .paths(g)\n> res[which(res==Inf)]= 99\n−\n> max(res)\n1 7\n[ ]\n> length(which(res== 7 ))\n1 18\n[ ]\nWe see that the number of paths that are of length 7 are a total of 18,\nbut of course, this is duplicated as we run these paths in both directions.\nHence, there are 9 pairs of nodes that have longest shortest paths be-\n1311\ntween them. You may try to locate these on Figure . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 340,
      "chunk_index": 1
    }
  },
  {
    "text": "making connections: network theory 341\n13.9 Fragility\nFragility is an attribute of a network that is based on its degree distri-\nbution. In comparing two networks of the same average degree, how do\nassess on which network contagion is more likely? Intuitively, a scale-\nfree network is more likely to facilitate the spread of the variable of in-\nterest, be it flu, financial malaise, or information. In scale-free networks\nthe greater preponderance of central hubs results in a greater probability",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 341,
      "chunk_index": 0
    }
  },
  {
    "text": "of contagion. This is because there is a concentration of degree in a few\nnodes. The greater the concentration, the more scale-free the graph, and\nthe higher the fragility.\nWe need a measure of concentration, and economists have used the\nHerfindahl-Hirschman index for many years.\n(See https://en.wikipedia.org/wiki/Herfindahl_index.)\nThe index is trivial to compute, as it is the average degree squared for\nn nodes, i.e.,\nH = E(d2) = 1 ∑ n d2 ( 13 . 1 )\nn j\nj=1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 341,
      "chunk_index": 1
    }
  },
  {
    "text": "H = E(d2) = 1 ∑ n d2 ( 13 . 1 )\nn j\nj=1\nThis metric H increases as degree gets concentrated in a few nodes,\nkeeping the total degree of the network constant. For example, if there\nis a graph of three nodes with degrees 1,1,4 versus another graph\n{ }\nof three nodes with degrees 2,2,2 , the former will result in a higher\n{ }\nvalue of H = 18 than the latter with H = 12. If we normalize H by the\naverage degree, then we have a definition for fragility, i.e.,\nE(d2)\nFragility = ( 13 . 2 )\nE(d)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 341,
      "chunk_index": 2
    }
  },
  {
    "text": "E(d2)\nFragility = ( 13 . 2 )\nE(d)\nIn the three node graphs example, fragility is 3 and 2, respectively. We\nmay also choose other normalization factors, for example, E(d)2 in the\ndenominator. Computing this is trivial and requires a single line of code,\ngiven a vector of node degrees (d), accompanied by the degree distribu-\n137\ntion (dd), computed earlier in Section . .\n#FRAGILITY\nprint (( t(d^ 2 ) %*% dd) / (t(d) %*% dd))\n13.10 Centrality",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 341,
      "chunk_index": 3
    }
  },
  {
    "text": "13.10 Centrality\nCentrality is a property of vertices in the network. Given the adjacency\nmatrix A = w(u,v) , we can obtain a measure of the “influence” of\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 341,
      "chunk_index": 4
    }
  },
  {
    "text": "342 data science: theories, models, algorithms, and analytics\nall vertices in the network. Let x be the influence of vertex i. Then the\ni\ncolumn vector x contains the influence of each vertex. What is influence?\nThink of a web page. It has more influence the more links it has both,\nto the page, and from the page to other pages. Or think of a alumni\nnetwork. People with more connections have more influence, they are\nmore “central”.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 342,
      "chunk_index": 0
    }
  },
  {
    "text": "more “central”.\nIt is possible that you might have no connections yourself, but are\nconnected to people with great connections. In this case, you do have in-\nfluence. Hence, your influence depends on your own influence and that\nwhich you derive through others. Hence, the entire system of influence\nis interdependent, and can be written as the following matrix equation\nx = A x\nNow, we can just add a scalar here to this to get\nξ x = Ax",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 342,
      "chunk_index": 1
    }
  },
  {
    "text": "ξ x = Ax\nan eigensystem. Decompose this to get the principle eigenvector, and\nits values give you the influence of each member. In this way you can\nfind the most influential people in any network. There are several appli-\ncations of this idea to real data. This is eigenvector centrality is exactly\nwhat Google trademarked as PageRank, even though they did not invent\neigenvector centrality.\nNetwork methods have also been exploited in understanding Ven-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 342,
      "chunk_index": 2
    }
  },
  {
    "text": "ture Capitalist networks, and have been shown to be key in the success\nof VCs and companies. See the recent paper titled “Whom You Know\nMatters: Venture Capital Networks and Investment Performance” by\n2007\nHochberg, Ljungqvist and Lu ( ).\nNetworks are also key in the Federal Funds Market. See the paper\nby Adam Ashcraft and Darrell Duffie, titled “Systemic Illiquidity in the\nFederal Funds Market,” in the American Economic Review, Papers and\n2007\nProceedings. See Ashcraft and Duffie ( ).\n2005",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 342,
      "chunk_index": 3
    }
  },
  {
    "text": "Proceedings. See Ashcraft and Duffie ( ).\n2005\nSee the paper titled “Financial Communities” (Das and Sisk ( ))\nwhich also exploits eigenvector methods to uncover properties of graphs.\nThe key concept here is that of eigenvector centrality.\nLet’s do some examples to get a better idea. We will create some small\nnetworks and examine the centrality scores.\n> A = matrix(nc= 3 , byrow=TRUE, c( 0 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 ))\n> A",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 342,
      "chunk_index": 4
    }
  },
  {
    "text": "making connections: network theory 343\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 1 1\n[ ,]\n2 1 0 1\n[ ,]\n3 1 1 0\n[ ,]\n> g = graph. adjacency(A,mode=\"undirected\" ,weighted=TRUE,diag=FALSE)\n> res = evcent(g)\n> res$vector\n1 1 1 1\n[ ]\n> res = evcent(g, scale=FALSE)\n> res$vector\n1 0 5773503 0 5773503 0 5773503\n[ ] . . .\nHere is another example:\n> A = matrix(nc= 3 , byrow=TRUE, c( 0 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 0 ))\n> A\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 1 1\n[ ,]\n2 1 0 0\n[ ,]\n3 1 0 0\n[ ,]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 343,
      "chunk_index": 0
    }
  },
  {
    "text": "1 0 1 1\n[ ,]\n2 1 0 0\n[ ,]\n3 1 0 0\n[ ,]\n> g = graph. adjacency(A,mode=\"undirected\" ,weighted=TRUE,diag=FALSE)\n> res = evcent(g)\n> res$vector\n1 1 0000000 0 7071068 0 7071068\n[ ] . . .\nAnd another...\n> A = matrix(nc= 3 , byrow=TRUE, c( 0 , 2 , 1 , 2 , 0 , 0 , 1 , 0 , 0 ))\n> A\n1 2 3\n[ , ] [ , ] [ , ]\n1 0 2 1\n[ ,]\n2 2 0 0\n[ ,]\n3 1 0 0\n[ ,]\n> g = graph. adjacency(A,mode=\"undirected\" ,weighted=TRUE,diag=FALSE)\n> res = evcent(g)\n> res$vector\n1 1 0000000 0 8944272 0 4472136\n[ ] . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 343,
      "chunk_index": 1
    }
  },
  {
    "text": "344 data science: theories, models, algorithms, and analytics\nTable13.1: Summarystatistics\nYear #Colending #Coloans Colending R=E(d2)/E(d) Diam. andthetop25banksorderedon\nbanks pairs eigenvaluecentralityfor2005. The\n2005 241 75 10997 137.91 5\nR-metricisameasureofwhether\n2006 171 95 4420 172.45 5\n2007 85 49 1793 73.62 4 failurecanspreadquickly,andthis\n2008 69 84 681 68.14 4 issowhenR 2. Thediameter\n≥\n2009 69 42 598 35.35 4 ofthenetworkisthelengthofthe\nlongestgeodesic. Alsopresented",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 344,
      "chunk_index": 0
    }
  },
  {
    "text": "longestgeodesic. Alsopresented\n(Year=2005) inthesecondpanelofthetable\nNode# FinancialInstitution Normalized\narethecentralityscoresfor2005\nCentrality correspondingtoFigure13.13.\n143 JPMorganChase&Co. 1.000\n29 BankofAmericaCorp. 0.926\n47 CitigroupInc. 0.639\n85 DeutscheBankAgNewYorkBranch 0.636\n225 WachoviaBankNA 0.617\n235 TheBankofNewYork 0.573\n134 HsbcBankUSA 0.530\n39 BarclaysBankPlc 0.530\n152 Keycorp 0.524\n241 TheRoyalBankofScotlandPlc 0.523\n6 AbnAmroBankN.V. 0.448\n173 MerrillLynchBankUSA 0.374",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 344,
      "chunk_index": 1
    }
  },
  {
    "text": "173 MerrillLynchBankUSA 0.374\n198 PNCFinancialServicesGroupInc 0.372\n180 MorganStanley 0.362\n42 BnpParibas 0.337\n205 RoyalBankofCanada 0.289\n236 TheBankofNovaScotia 0.289\n218 U.S.BankNA 0.284\n50 CalyonNewYorkBranch 0.273\n158 LehmanBrothersBankFsb 0.270\n213 SumitomoMitsuiBanking 0.236\n214 SuntrustBanksInc 0.232\n221 UBSLoanFinanceLlc 0.221\n211 StateStreetCorp 0.210\n228 WellsFargoBankNA 0.198\nIn a recent paper I constructed the network graph of interbank lending,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 344,
      "chunk_index": 2
    }
  },
  {
    "text": "and this allows detection of the banks that have high centrality, and are\nmore systemically risky. The plots of the banking network are shown in\n1313\nFigure . . See the paper titled “Extracting, Linking and Integrating\nData from Public Sources: A Financial Case Study,” by Burdick et al\n2011\n( ). In this paper the centrality scores for the banks are given in Table\n131\n. .\nAnother concept of centrality is known as “betweenness”. This is the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 344,
      "chunk_index": 3
    }
  },
  {
    "text": "proportion of shortest paths that go through a node relative to all paths",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 344,
      "chunk_index": 4
    }
  },
  {
    "text": "making connections: network theory 345\nFigure13.13: Interbanklending\nnetworksbyyear. Thetoppanel\n2005\nshows2005,andthebottompanel\nisfortheyears2006-2009.\nCitigroup Inc.\nJ.P. Morgan Chase\nBank of America Corp.\n2006 2007\n2008 2009",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 345,
      "chunk_index": 0
    }
  },
  {
    "text": "346 data science: theories, models, algorithms, and analytics\nthat go through the same node. This may be expressed as\nB(v) =\n∑ n a,b (v)\nn\na=v=b a,b\n(cid:54) (cid:54)\nwhere n is the number of shortest paths from node a to node b, and\na,b\nn (v) are the number of those paths that traverse through vertex v.\na,b\nHere is an example from an earlier directed graph.\n> el < matrix(nc=3, byrow=TRUE,\n+ − c(0,1,0, 0,2,2, 0,3,1, 1,2,0, 1,4,5, 1,5,2, 2,1,1, 2,3,1,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 346,
      "chunk_index": 0
    }
  },
  {
    "text": "+ 2,6,1, 3,2,0, 3,6,2, 4,5,2, 4,7,8, 5,2,2, 5,6,1, 5,8,1,\n+ 5,9,3, 7,5,1, 7,8,1, 8,9,4) )\n> g = add.edges(graph.empty(10), t(el[,1:2]), weight=el[,3])\n> res = betweenness(g)\n> res\n[1] 0.0 18.0 17.0 0.5 5.0 19.5 0.0 0.5 0.5 0.0\n13.11 Communities\nCommunities are spatial agglomerates of vertexes who are more likely to\nconnect with each other than with others. Identifying these agglomerates\nis a cluster detection problem, a computationally difficult (NP-hard) one.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 346,
      "chunk_index": 1
    }
  },
  {
    "text": "The computational complexity arises because we do not fix the num-\nber of clusters, allow each cluster to have a different size, and permit\nporous boundaries so members can communicate both within and out-\nside their preferred clusters. Several partitions satisfy such a flexible def-\ninition. Communities are constructed by optimizing modularity, which\nis a metric of the difference between the number of within-community\nconnections and the expected number of connections, given the total",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 346,
      "chunk_index": 2
    }
  },
  {
    "text": "connectivity on the graph. Identifying communities is difficult because\nof the enormous computational complexity involved in sifting through\nall possible partitions. One fast way is to exploit the walk trap approach\n2006\nrecently developed in the physical sciences (Pons and Latapy ( ), see\n2010\nFortunato ( ) for a review) to identify communities.\nThe essential idea underlying community formation dates back at\n1962\nleast to Simon ( ). In his view, complex systems comprising several",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 346,
      "chunk_index": 3
    }
  },
  {
    "text": "entities often have coherent subsystems, or communities, that serve spe-\ncific functional purposes. Identifying communities embedded in larger\nentities can help understand the functional forces underlying larger en-\ntities. To make these ideas more concrete, we discuss applications from\nthe physical and social sciences before providing more formal defini-\ntions.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 346,
      "chunk_index": 4
    }
  },
  {
    "text": "making connections: network theory 347\nIn the life sciences, community structures help understand pathways\n2002\nin the metabolic networks of cellular organisms (Ravasz et al. ( );\n2005\nGuimera et al. ( )). Community structures also help understand the\n2011\nfunctioning of the human brain. For instance, Wu, Taki, and Sato ( )\nfind that there are community structures in the human brain with pre-\ndictable changes in their interlinkages related to aging. Community",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 347,
      "chunk_index": 0
    }
  },
  {
    "text": "structures are used to understand how food chains are compartmental-\nized, which can predict the robustness of ecosystems to shocks that en-\n2002 2003\ndanger particular species, Girvan and Newman ( ). Lusseau ( )\nfinds that communities are evolutionary hedges that avoid isolation\nwhen a member is attacked by predators. In political science, commu-\nnity structures discerned from voting patterns can detect political pref-\nerences that transcend traditional party lines, Porter, Mucha, Newman,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 347,
      "chunk_index": 1
    }
  },
  {
    "text": "2007 1\nand Friend ( ). 1Othertopicsstudiedincludesocial\n2010 interactionsandcommunityformation\nFortunato ( ) presents a relatively recent and thorough survey of\n(Zachary(1977));wordadjacencyin\nthe research in community detection. Fortunato points out that while linguisticsandcognitivesciences,New-\nman(2006);collaborationsbetweensci-\nthe computational issues are challenging, there is sufficient progress to entists(Newman(2001));andindustry",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 347,
      "chunk_index": 2
    }
  },
  {
    "text": "the point where many methods yield similar results in practice. How- structuresfromproductdescriptions,\nHobergandPhillips(2010).Forsome\never, there are fewer insights on the functional roles of communities or communitydetectiondatasets,see\nMarkNewman’swebsitehttp://www-\ntheir quantitative effect on outcomes of interest. Fortunato suggests that\npersonal.umich.edu/mejn/netdata/.\nthis is a key challenge in the literature. As he concludes “... What shall",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 347,
      "chunk_index": 3
    }
  },
  {
    "text": "we do with communities? What can they tell us about a system? This\nis the main question beneath the whole endeavor.” Community detec-\ntion methods provide useful insights into the economics of networks.\nSee this great video on a talk by Mark Newman, who is just an excel-\nlent speaker and huge contributor to the science of network analysis:\nhttp://www.youtube.com/watch?v=lETt7IcDWLI, the talk is titled “What\nNetworks Can Tell us About the World”.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 347,
      "chunk_index": 4
    }
  },
  {
    "text": "Networks Can Tell us About the World”.\nWe represent the network as the square adjacency matrix A. The rows\nand columns represent entities. Element A(i,j) equals the number of\ntimes node i and j are partners, so more frequent partnerships lead to\ngreater weights. The diagonal element A(i,i) is zero. While this repre-\nsentation is standard in the networks literature, it has economic content.\nThe matrix is undirected and symmetric, effectively assuming that the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 347,
      "chunk_index": 5
    }
  },
  {
    "text": "benefits of interactions flow to all members in a symmetric way.\nCommunity detection methods partition nodes into clusters that tend\nto interact together. It is useful to point out the considerable flexibil-\nity and realism built into the definition of our community clusters. We",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 347,
      "chunk_index": 6
    }
  },
  {
    "text": "348 data science: theories, models, algorithms, and analytics\ndo not require all nodes to belong to communities. Nor do we fix the\nnumber of communities that may exist at a time, and we also allow each\ncommunity to have different size. With this flexibility, the key compu-\ntational challenge is to find the “best” partition because the number of\npossible partitions of the nodes is extremely large. Community detection\nmethods attempt to determine a set of clusters that are internally tight-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 348,
      "chunk_index": 0
    }
  },
  {
    "text": "knit. Mathematically, this is equivalent to finding a partition of clusters\nto maximize the observed number of connections between cluster mem-\nbers minus what is expected conditional on the connections within the\ncluster, aggregated across all clusters. More formally (see, e.g., Newman\n( 2006 )), we choose partitions with high modularity Q, where\n(cid:20) (cid:21)\nQ = 1 ∑ A d i × d j δ(i,j) ( 13 . 3 )\nij\n2m − 2m ·\ni,j\nIn equation ( 13 . 3 ), A is the (i,j)-th entry in the adjacency matrix,\nij",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 348,
      "chunk_index": 1
    }
  },
  {
    "text": "ij\ni.e., the number of connections in which i and j jointly participated,\nd = ∑ A is the total number of transactions that node i participated\ni j ij\nin (or, the degree of i) and m = 1 ∑ A is the sum of all edge weights in\n2 ij ij\nmatrix A. The function δ(i,j) is an indicator equal to 1 . 0 if nodes i and j\nare from the same community, and zero otherwise. Q is bounded in [- 1 ,\n+ 1 ]. If Q > 0, intra-community connections exceed the expected number\ngiven deal flow.\n13.11.1 Modularity",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 348,
      "chunk_index": 2
    }
  },
  {
    "text": "given deal flow.\n13.11.1 Modularity\nIn order to offer the reader a better sense of how modularity is com-\nputed in different settings, we provide a simple example here, and dis-\ncuss the different interpretations of modularity that are possible. The cal-\n2006\nculations here are based on the measure developed in Newman ( ).\nSince we used the igraph package in R, we will present the code that\nmay be used with the package to compute modularity.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 348,
      "chunk_index": 3
    }
  },
  {
    "text": "Consider a network of five nodes A,B,C,D,E , where the edge\n{ }\nweights are as follows: A : B = 6, A : C = 5, B : C = 2, C : D = 2,\nand D : E = 10. Assume that a community detection algorithm as-\nsigns A,B,C to one community and D,E to another, i.e., only two\n{ } { }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 348,
      "chunk_index": 4
    }
  },
  {
    "text": "making connections: network theory 349\ncommunities. The adjacency matrix for this graph is\n \n0 6 5 0 0\n \n6 0 2 0 0\n \n \nA =  5 2 0 2 0 \nij\n{ }  \n 0 0 2 0 10 \n \n0 0 0 10 0\nLet’s first detect the communities.\n> library(igraph)\n> A = matrix(c( 0 , 6 , 5 , 0 , 0 , 6 , 0 , 2 , 0 , 0 , 5 , 2 , 0 , 2 , 0 , 0 , 0 , 2 , 0 , 10 , 0 , 0 , 0 , 10 , 0 ) , 5 , 5 )\n> g = graph. adjacency(A,mode=\"undirected\" ,diag=FALSE)\n> wtc = walktrap.community(g)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 349,
      "chunk_index": 0
    }
  },
  {
    "text": "> wtc = walktrap.community(g)\n> res=community. to .membership(g,wtc$merges, steps= 3 )\n> print(res)\n$membership\n1 1 1 1 0 0\n[ ]\n$csize\n1 2 3\n[ ]\nWe can do the same thing with a different algorithm called the “fast-\ngreedy” approach.\n> g = graph. adjacency(A,mode=\"undirected\" ,weighted=TRUE,diag=FALSE)\n> fgc = fastgreedy .community(g,merges=TRUE,modularity=TRUE,\nweights=E(g)$weight)\n> res = community. to .membership(g, fgc$merges, steps= 3 )\n> res\n$membership\n1 0 0 0 1 1\n[ ]\n$csize\n1 3 2\n[ ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 349,
      "chunk_index": 1
    }
  },
  {
    "text": "$membership\n1 0 0 0 1 1\n[ ]\n$csize\n1 3 2\n[ ]\nThe Kronecker delta matrix that delineates the communities will be\n \n1 1 1 0 0\n \n1 1 1 0 0\n \n \nδ =  1 1 1 0 0 \nij\n{ }  \n 0 0 0 1 1 \n \n0 0 0 1 1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 349,
      "chunk_index": 2
    }
  },
  {
    "text": "350 data science: theories, models, algorithms, and analytics\nThe modularity score is\n(cid:20) (cid:21)\nQ = 1 ∑ A d i × d j δ ( 13 . 4 )\nij ij\n2m − 2m ·\ni,j\nwhere m = 1 ∑ A = 1 ∑ d is the sum of edge weights in the graph,\n2 ij ij 2 i i\nA is the (i,j)-th entry in the adjacency matrix, i.e., the weight of the\nij\nedge between nodes i and j, and d = ∑ A is the degree of node i.\ni j ij\nThe function δ is Kronecker’s delta and takes value 1 when the nodes\nij",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 350,
      "chunk_index": 0
    }
  },
  {
    "text": "ij\ni and j are from the same community, else takes value zero. The core\n(cid:104) (cid:105)\nd d\nof the formula comprises the modularity matrix A i× j which\nij − 2m\ngives a score that increases when the number of connections within a\ncommunity exceeds the expected proportion of connections if they are\nassigned at random depending on the degree of each node. The score\ntakes a value ranging from 1 to +1 as it is normalized by dividing\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 350,
      "chunk_index": 1
    }
  },
  {
    "text": "−\nby 2m. When Q > 0 it means that the number of connections within\ncommunities exceeds that between communities. The program code that\ntakes in the adjacency matrix and delta matrix is as follows:\n#MODULARITY\nAmodularity = function(A, delta) {\nn = length(A[ 1 ,])\nd = matrix( 0 ,n, 1 )\nfor ( j in 1 :n) { d[ j ] = sum(A[j ,]) }\nm = 0 . 5 *sum(d)\nQ = 0\nfor ( i in 1 :n) {\nfor ( j in 1 :n) {\nQ = Q + (A[i , j ] d[ i ]*d[ j ] / ( 2 *m))*delta[i , j ]\n−\n}\n}\nQ = Q/ ( 2 *m)\n}",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 350,
      "chunk_index": 2
    }
  },
  {
    "text": "−\n}\n}\nQ = Q/ ( 2 *m)\n}\nWe use the R programming language to compute modularity using\na canned function, and we will show that we get the same result as the\nformula provided in the function above. First, we enter the two matrices\nand then call the function shown above:\n> A = matrix(c( 0 , 6 , 5 , 0 , 0 , 6 , 0 , 2 , 0 , 0 , 5 , 2 , 0 , 2 , 0 , 0 , 0 , 2 , 0 , 10 , 0 , 0 , 0 , 10 , 0 ) , 5 , 5 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 350,
      "chunk_index": 3
    }
  },
  {
    "text": "> delta = matrix(c( 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 1 , 1 ) , 5 , 5 )\n> print(Amodularity(A, delta ))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 350,
      "chunk_index": 4
    }
  },
  {
    "text": "making connections: network theory 351\n1 0 4128\n[ ] .\nWe now repeat the same analysis using the R package. Our exposi-\ntion here will also show how the walktrap algorithm is used to detect\ncommunities, and then using these communities, how modularity is\ncomputed. Our first step is to convert the adjacency matrix into a graph\nfor use by the community detection algorithm.\n> g = graph. adjacency(A,mode=\"undirected\" ,weighted=TRUE,diag=FALSE)\nWe then pass this graph to the walktrap algorithm:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 351,
      "chunk_index": 0
    }
  },
  {
    "text": "> wtc=walktrap.community(g,modularity=TRUE,weights=E(g)$weight)\n> res=community. to .membership(g,wtc$merges, steps= 3 )\n> print(res)\n$membership\n1 0 0 0 1 1\n[ ]\n$csize\n1 3 2\n[ ]\nWe see that the algorithm has assigned the first three nodes to one\ncommunity and the next two to another (look at the membership vari-\nable above). The sizes of the communities are shown in the size variable\nabove. We now proceed to compute the modularity\n> print(modularity(g, res$membership,weights=E(g)$weight))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 351,
      "chunk_index": 1
    }
  },
  {
    "text": "1 0 4128\n[ ] .\nThis confirms the value we obtained from the calculation using our\nimplementation of the formula.\nModularity can also be computed using a graph where edge weights\nare unweighted. In this case, we have the following adjacency matrix\n> A\n1 2 3 4 5\n[ , ] [ , ] [ , ] [ , ] [ , ]\n1 0 1 1 0 0\n[ ,]\n2 1 0 1 0 0\n[ ,]\n3 1 1 0 1 0\n[ ,]\n4 0 0 1 0 1\n[ ,]\n5 0 0 0 1 0\n[ ,]\nUsing our function, we get\n> print(Amodularity(A, delta ))\n1 0 22\n[ ] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 351,
      "chunk_index": 2
    }
  },
  {
    "text": "352 data science: theories, models, algorithms, and analytics\nWe can generate the same result using R:\n> g = graph. adjacency(A,mode=\"undirected\" ,diag=FALSE)\n> wtc = walktrap.community(g)\n> res=community. to .membership(g,wtc$merges, steps= 3 )\n> print(res)\n$membership\n1 1 1 1 0 0\n[ ]\n$csize\n1 2 3\n[ ]\n> print(modularity(g, res$membership))\n1 0 22\n[ ] .\nCommunity detection is an NP-hard problem for which there are no\n2009\nknown exact solutions beyond tiny systems (Fortunato, ). For larger",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 352,
      "chunk_index": 0
    }
  },
  {
    "text": "datasets, one approach is to impose numerical constraints. For example,\ngraph partitioning imposes a uniform community size, while partitional\nclustering presets the number of communities. This is too restrictive.\nThe less restrictive methods for community detection are called hier-\narchical partitioning methods, which are “divisive,” or “agglomerative.”\nThe former is a top-down approach that assumes that the entire graph is\none community and breaks it down into smaller units. It often produces",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 352,
      "chunk_index": 1
    }
  },
  {
    "text": "communities that are too large especially when there is not an extremely\nstrong community structure. Agglomerative algorithms, like the “walk-\ntrap” technique we use, begin by assuming all nodes are separate com-\nmunities and collect nodes into communities. The fast techniques are\ndynamic methods based on random walks, whose intuition is that if a\nrandom walk enters a strong community, it is likely to spend a long time",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 352,
      "chunk_index": 2
    }
  },
  {
    "text": "inside before finding a way out (Pons and Latapy ( 2006 )). 2 2SeeGirvanandNewman(2002),\nLeskovec,KangandMahoney(2010),\nCommunity detection forms part of the literature on social network\norFortunato(2010)andthereferences\nanalysis. The starting point for this work is a set of pairwise connections thereinforadiscussion.\nbetween individuals or firms, which has received much attention in the\n2008\nrecent finance literature. Cohen, Frazzini and Malloy ( a); Cohen,\n2008",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 352,
      "chunk_index": 3
    }
  },
  {
    "text": "2008\nFrazzini and Malloy ( b) analyze educational connections between\n2009\nsell-side analysts and managers. Hwang and Kim ( ) and ChidKed-\n2010\nPrabh ( ) analyze educational, employment, and other links between\nCEOs and directors. Pairwise inter-firm relations are analyzed by Ishii\n2009 2012\nand Xuan ( ) and Cai and Sevilir ( ), while VC firm connections",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 352,
      "chunk_index": 4
    }
  },
  {
    "text": "making connections: network theory 353\nwith founders and top executives are studied by Bengtsson and Hsu\n2010 2011\n( ) and Hegde and Tumlinson ( ).\nThere is more finance work on the aggregate connectedness derived\nfrom pairwise connections. These metrics are introduced to the finance\n2007\nliterature by Hochberg, Ljungqvist and Lu ( ), who study the aggre-\ngate connections of venture capitalists derived through syndications.\nThey show that firms financed by well-connected VCs are more likely to",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 353,
      "chunk_index": 0
    }
  },
  {
    "text": "2000\nexit successfully. Engelberg, Gao and Parsons ( ) show that highly\nconnected CEOs are more highly compensated.\nThe simplest measure of aggregate connectedness, degree central-\nity, simply aggregates the number of partners that a person or node\nhas worked with. A more subtle measure, eigenvector centrality, aggre-\ngates connections but puts more weight on the connections of nodes to\nmore connected nodes. Other related constructs are betweenness, which",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 353,
      "chunk_index": 1
    }
  },
  {
    "text": "reflects how many times a node is on the shortest path between two\nother nodes, and closeness, which measures a nodes distance to all other\nnodes. The important point is that each of these measures represents an\nattempt to capture a node’s stature or influence as reflected in the num-\nber of its own connections or from being connected to well-connected\nnodes.\nCommunity membership, on the other hand, is a group attribute that",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 353,
      "chunk_index": 2
    }
  },
  {
    "text": "reflects whether a node belongs to a spatial cluster of nodes that tend to\ncommunicate a lot together. Community membership is a variable in-\nherited by all members of a spatial agglomerate. However, centrality is\nan individual-centered variable that captures a node’s influence. Com-\nmunity membership does not measure the reach or influence of a node.\nRather, it is a measure focused on interactions between nodes, reflect-\ning whether a node deals with familiar partners. Neither community",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 353,
      "chunk_index": 3
    }
  },
  {
    "text": "membership nor centrality is a proper subset of the other.\nThe differences between community and centrality are visually de-\n1313 2011\npicted in Figure . , which is reproduced from Burdick et al ( ).\nThe figure shows the high centrality of Citigroup, J. P. Morgan, and Bank\nof America, well connected banks in co-lending networks. However,\nnone of these banks belong to communities, which are represented by\nbanks in the left and right nodes of the figure. In sum, community is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 353,
      "chunk_index": 4
    }
  },
  {
    "text": "a group attribute that measures whether a node belongs to a tight knit\n3\ngroup. Centrality reflects the size and heft of a node’s connections. For 3Newman(2010)bringsoutthedistinc-\ntionsfurther.SeehisSections7.1/7.2on\nanother schematic that shows the same idea, i.e., the difference between\ncentralityandSection11.6oncommu-\nnitydetection.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 353,
      "chunk_index": 5
    }
  },
  {
    "text": "354 data science: theories, models, algorithms, and analytics\n1314\ncentrality and communities is in Figure . .\nFigure13.14: Communityversus\ncentrality\nCommunity v. Centrality\nCentrality\nCommunities\n• Hub focused concept\n• Group-focused concept\n• Resources and skill of\n• Members learn-by-doing\ncentral players. 41\nthrough social interactions.\nSee my paper titled “Venture Capital Communities” where I examine\nhow VCs form communities, and whether community-funded startups",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 354,
      "chunk_index": 0
    }
  },
  {
    "text": "do better than the others (we do find so). We also find evidence of some\naspects of homophily within VC communities, though there are also\naspects of heterogeneity in characteristics.\n13.12 Word of Mouth\nWOM has become an increasingly important avenue for viral marketing.\nHere is a article on the growth of this medium. See ?. See also the really\n2009\ninteresting paper by Godes and Mayzlin ( ) titled “Firm-Created\nWord-of-Mouth Communication: Evidence from a Field Test”. This is an",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 354,
      "chunk_index": 1
    }
  },
  {
    "text": "excellent example of how firms should go about creating buzz. See also\n2004\nGodes and Mayzlin ( ): “Using Online Conversations to Study Word\nof Mouth Communication” which looks at TV ratings and WOM.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 354,
      "chunk_index": 2
    }
  },
  {
    "text": "making connections: network theory 355\n13.13 Network Models of Systemic Risk\nIn an earlier section, we saw pictures of banking networks (see Figure\n1313\n. ), i.e., the interbank loan network. In these graphs, the linkages be-\ntween banks were considered, but two things were missing. First, we\nassumed that all banks were similar in quality or financial health, and\nnodes were therefore identical. Second, we did not develop a network",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 355,
      "chunk_index": 0
    }
  },
  {
    "text": "measure of overall system risk, though we did compute fragility and\ndiameter for the banking network. What we also computed was the rela-\ntive position of each bank in the network, i.e., it’s eigenvalue centrality.\nIn the section, we augment network information of the graph with\nadditional information on the credit quality of each node in the network.\nWe then use this to compute a system-wide score of the overall risk of\n4\nthe system, denoting this as systemic risk. This section is based on . 4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 355,
      "chunk_index": 1
    }
  },
  {
    "text": "We make the following assumptions and define notation:\n• Assume n nodes, i.e., firms, or “assets.”\n• Let E Rn n be a well-defined adjacency matrix. This quantifies the\n×\n∈\ninfluence of each node on another.\n• E may be portrayed as a directed graph, i.e., E = E .\nij ji\n(cid:54)\nE = 1; E 0,1 .\njj ij\n∈ { }\n• C is a (n 1) risk vector that defines the risk score for each asset.\n×\n• We define the “systemic risk score” as\n(cid:112)\nS = C E C\n(cid:62)\n• S(C,E) is linear homogenous in C.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 355,
      "chunk_index": 2
    }
  },
  {
    "text": "(cid:62)\n• S(C,E) is linear homogenous in C.\nWe note that this score captures two important features of systemic risk:\n(a) The interconnectedness of the banks in the system, through adjacency\n(or edge) matrix E, and (b) the financial quality of each bank in the sys-\ntem, denoted by the vector C, a proxy for credit score, i.e., credit rating,\nz-score, probability of default, etc.\n13.13.1 Systemic Score, Fragility, Centrality, Diameter\nWe code up the systemic risk function as follows.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 355,
      "chunk_index": 3
    }
  },
  {
    "text": "We code up the systemic risk function as follows.\nlibrary(igraph)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 355,
      "chunk_index": 4
    }
  },
  {
    "text": "356 data science: theories, models, algorithms, and analytics\n#FUNCTION FOR RISK INCREMENT AND DECOMP\nNetRisk = function(Ri ,X) {\nS = sqrt(t(Ri) %*% X %*% Ri)\nRiskIncr = 0 . 5 * (X %*% Ri + t(X) %*% Ri) /S[ 1 , 1 ]\nRiskDecomp = RiskIncr * Ri\nresult = list (S, RiskIncr ,RiskDecomp)\n}\n15\nTo illustrate application, we generate a network of banks by creat-\ning a random graph.\n#CREATE ADJ MATRIX\ne = floor(runif( 15 * 15 )* 2 )\nX = matrix(e, 15 , 15 )\ndiag(X) = 1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 356,
      "chunk_index": 0
    }
  },
  {
    "text": "X = matrix(e, 15 , 15 )\ndiag(X) = 1\nThis creates the network adjacency matrix and network plot shown in\n1315 1\nFigure . . Note that the diagonal elements are , as this is needed for\nthe risk score.\nThe code for the plot is as follows:\n#GRAPH NETWORK: plot of the assets and the links with directed arrow\nna = length(diag(X))\nY = X; diag(Y)= 0\ng = graph. adjacency(Y)\nplot .igraph(g, layout=layout .fruchterman. reingold ,\n0 5 15\nedge.arrow. size= . ,vertex . size= ,\nvertex . label=seq( 1 ,na))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 356,
      "chunk_index": 1
    }
  },
  {
    "text": "vertex . label=seq( 1 ,na))\nWe now randomly create credit scores for these banks. Let’s assume\nwe have four levels of credit, 0,1,2,3 , where lower scores represent\n{ }\nhigher credit quality.\n#CREATE CREDIT SCORES\nRi = matrix(floor(runif(na)* 4 ) ,na, 1 )\n> Ri\n1\n[ , ]\n1 1\n[ ,]\n2 3\n[ ,]\n3 0\n[ ,]\n4 3\n[ ,]\n5 0\n[ ,]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 356,
      "chunk_index": 2
    }
  },
  {
    "text": "making connections: network theory 357\nFigure13.15: Bankingnetwork\nadjacencymatrixandplot",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 357,
      "chunk_index": 0
    }
  },
  {
    "text": "358 data science: theories, models, algorithms, and analytics\n6 0\n[ ,]\n7 2\n[ ,]\n8 0\n[ ,]\n9 0\n[ ,]\n10 2\n[ ,]\n11 0\n[ ,]\n12 2\n[ ,]\n13 2\n[ ,]\n14 1\n[ ,]\n15 3\n[ ,]\nWe may now use this generated data to compute the overall risk score\nand risk increments, discussed later.\n#COMPUTE OVERALL RISK SCORE AND RISK INCREMENT\nres = NetRisk(Ri ,X)\nS = res [[ 1 ]]; print(c(\"Risk Score\" ,S))\n2\nRiskIncr = res [[ ]]\n1 14 6287388383278\n[ ] \"Risk Score\" \" . \"\nWe compute the fragility of this network.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 358,
      "chunk_index": 0
    }
  },
  {
    "text": "We compute the fragility of this network.\n#NETWORK FRAGILITY\n1\ndeg = rowSums(X)\n−\nfrag = mean(deg^ 2 )/mean(deg)\nprint(c(\" Fragility score = \" ,frag ))\n1 8 1551724137931\n[ ] \" Fragility score = \" \" . \"\nThe centrality of the network is computed and plotted with the fol-\n1316\nlowing code. See Figure . .\n#NODE EIGEN VALUE CENTRALITY\ncent = evcent(g)$vector\nprint(\"Normalized Centrality Scores\")\nprint(cent)\nsorted_cent = sort(cent , decreasing=TRUE,index. return=TRUE)\nScent = sorted_cent$x",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 358,
      "chunk_index": 1
    }
  },
  {
    "text": "Scent = sorted_cent$x\nidxScent = sorted_cent$ix\nbarplot(t(Scent) ,col=\"dark red\" ,xlab=\"Node Number\" ,\nnames.arg=idxScent ,cex.names= 0 . 75 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 358,
      "chunk_index": 2
    }
  },
  {
    "text": "making connections: network theory 359\n> print(cent)\n1 0 7648332 1 0000000 0 7134844 0 6848305 0 7871945 0 8721071\n[ ] . . . . . .\n7 0 7389360 0 7788079 0 5647471 0 7336387 0 9142595 0 8857590\n[ ] . . . . . .\n13 0 7183145 0 7907269 0 8365532\n[ ] . . .\nFigure13.16: Centralityforthe15\nbanks.\nAnd finally, we compute diameter.\nprint(diameter(g))\n1 2\n[ ]\n13.13.2 Risk Decomposition\nBecause the function S(C,E) is homogenous of degree 1 in C, we may",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 359,
      "chunk_index": 0
    }
  },
  {
    "text": "use this property to decompose the overall systemic score into the contri-\nbution from each bank. Applying Euler’s theorem, we write this decom-\nposition as:\n∂S ∂S ∂S\nS = C + C +...+ C ( 13 . 5 )\n1 2 n\n∂C ∂C ∂C\n1 2 n\nThe risk contribution of bank j is ∂S C .\n∂C\nj\nj\nThe code and output are shown here.\n#COMPUTE RISK DECOMPOSITION\nRiskDecomp = RiskIncr * Ri\nsorted_RiskDecomp = sort(RiskDecomp, decreasing=TRUE,\nindex. return=TRUE)\nRD = sorted_RiskDecomp$x",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 359,
      "chunk_index": 1
    }
  },
  {
    "text": "360 data science: theories, models, algorithms, and analytics\nidxRD = sorted_RiskDecomp$ix\nprint(\"Risk Contribution\" );\nprint(RiskDecomp);\nprint(sum(RiskDecomp))\nbarplot(t(RD) ,col=\"dark green\" ,xlab=\"Node Number\" ,\nnames.arg=idxRD,cex.names= 0 . 75 )\nThe output is as follows:\n> print(RiskDecomp);\n1\n[ , ]\n1 0 7861238\n[ ,] .\n2 2 3583714\n[ ,] .\n3 0 0000000\n[ ,] .\n4 1 7431441\n[ ,] .\n5 0 0000000\n[ ,] .\n6 0 0000000\n[ ,] .\n7 1 7089648\n[ ,] .\n8 0 0000000\n[ ,] .\n9 0 0000000\n[ ,] .\n10 1 3671719\n[ ,] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 360,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ,] .\n9 0 0000000\n[ ,] .\n10 1 3671719\n[ ,] .\n11 0 0000000\n[ ,] .\n12 1 7089648\n[ ,] .\n13 1 8456820\n[ ,] .\n14 0 8544824\n[ ,] .\n15 2 2558336\n[ ,] .\n> print(sum(RiskDecomp))\n1 14 62874\n[ ] .\nWe see that the total of the individual bank risk contributions does in-\ndeed add up to the aggregate systemic risk score of 14.63, computed\nearlier.\nThe resulting sorted risk contributions of each node (bank) are shown\n1317\nin Figure . .\n13.13.3 Normalized Risk Score",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 360,
      "chunk_index": 1
    }
  },
  {
    "text": "1317\nin Figure . .\n13.13.3 Normalized Risk Score\nWe may also normalize the risk score to isolate the network effect by\ncomputing\n√C E C\nS¯ = (cid:62) ( 13 . 6 )\nC\n(cid:107) (cid:107)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 360,
      "chunk_index": 2
    }
  },
  {
    "text": "making connections: network theory 361\nFigure13.17: RiskDecompositions\nforthe15banks.\nwhere C = √C C is the norm of vector C. When there are no network\n(cid:62)\n(cid:107) (cid:107)\neffects, E = I, the identity matrix, and S¯ = 1, i.e., the normalized baseline\nrisk level with no network (system-wide) effects is unity. As S¯ increases\n1\nabove , it implies greater network effects.\n#Compute normalized score SBar\nSbar = S/ sqrt(t(Ri) %*% Ri)\nprint(\"Sbar (normalized risk score\" );\n> print(Sbar)\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 361,
      "chunk_index": 0
    }
  },
  {
    "text": "> print(Sbar)\n1\n[ , ]\n1 2 180724\n[ ,] .\n13.13.4 Risk Increments\nWe are also interested in the extent to which a bank may impact the\noverall risk of the system if it begins to experience deterioration in credit\nquality. Therefore, we may compute the sensitivity of S to C:\n∂S\nRisk increment = I = , j ( 13 . 7 )\nj\n∂C ∀\nj\n> RiskIncr\n1\n[ , ]\n1 0 7861238\n[ ,] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 361,
      "chunk_index": 1
    }
  },
  {
    "text": "362 data science: theories, models, algorithms, and analytics\n2 0 7861238\n[ ,] .\n3 0 6835859\n[ ,] .\n4 0 5810480\n[ ,] .\n5 0 7177652\n[ ,] .\n6 0 8544824\n[ ,] .\n7 0 8544824\n[ ,] .\n8 0 8203031\n[ ,] .\n9 0 5810480\n[ ,] .\n10 0 6835859\n[ ,] .\n11 0 9228410\n[ ,] .\n12 0 8544824\n[ ,] .\n13 0 9228410\n[ ,] .\n14 0 8544824\n[ ,] .\n15 0 7519445\n[ ,] .\nNote that risk increments were previously computed in the function for\n1318\nthe risk score. We also plot this in sorted order, as shown in Figure . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 362,
      "chunk_index": 0
    }
  },
  {
    "text": "The code for this plot is shown here.\n#PLOT RISK INCREMENTS\nsorted_RiskIncr = sort(RiskIncr , decreasing=TRUE,\nindex. return=TRUE)\nRI = sorted_RiskIncr$x\nidxRI = sorted_RiskIncr$ix\nprint(\"Risk Increment (per unit increase in any node risk\")\nprint(RiskIncr)\nbarplot(t(RI) ,col=\"dark blue\" ,xlab=\"Node Number\" ,\nnames.arg=idxRI ,cex.names= 0 . 75 )\n13.13.5 Criticality\nCriticality is compromise-weighted centrality. This new measure is de-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 362,
      "chunk_index": 1
    }
  },
  {
    "text": "fined as y = C x where x is the centrality vector for the network, and\n×\ny,C,x n. Note that this is an element-wise multiplication of vectors\n∈ R\nC and x. Critical nodes need immediate attention, either because they\nare heavily compromised or they are of high centrality, or both. It of-\nfers a way for regulators to prioritize their attention to critical financial\ninstitutions, and pre-empt systemic risk from blowing up.\n#CRITICALITY\ncrit = Ri * cent",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 362,
      "chunk_index": 2
    }
  },
  {
    "text": "making connections: network theory 363\nFigure13.18: RiskIncrementsfor\nthe15banks.\nprint(\" Criticality Vector\")\nprint( crit )\nsorted_crit = sort( crit , decreasing=TRUE,index. return=TRUE)\nScrit = sorted_crit$x\nidxScrit = sorted_crit$ix\nbarplot(t( Scrit ) ,col=\"orange\" ,xlab=\"Node Number\" ,\nnames.arg=idxScrit ,cex.names= 0 . 75 )\n> print( crit )\n1\n[ , ]\n1 0 7648332\n[ ,] .\n2 3 0000000\n[ ,] .\n3 0 0000000\n[ ,] .\n4 2 0544914\n[ ,] .\n5 0 0000000\n[ ,] .\n6 0 0000000\n[ ,] .\n7 1 4778721\n[ ,] .\n8 0 0000000",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 363,
      "chunk_index": 0
    }
  },
  {
    "text": "6 0 0000000\n[ ,] .\n7 1 4778721\n[ ,] .\n8 0 0000000\n[ ,] .\n9 0 0000000\n[ ,] .\n10 1 4672773\n[ ,] .\n11 0 0000000\n[ ,] .\n12 1 7715180\n[ ,] .\n13 1 4366291\n[ ,] .\n14 0 7907269\n[ ,] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 363,
      "chunk_index": 1
    }
  },
  {
    "text": "364 data science: theories, models, algorithms, and analytics\n15 2 5096595\n[ ,] .\n1319\nThe plot of criticality is shown in Figure . .\nFigure13.19: Criticalityforthe15\nbanks.\n13.13.6 Cross Risk\nSince the systemic risk score S is a composite of network effects and\ncredit quality, the risk contributions of all banks are impacted when\nany single bank suffers credit deterioration. A bank has the power to\nimpose externalities on other banks, and we may assess how each bank’s",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 364,
      "chunk_index": 0
    }
  },
  {
    "text": "risk contribution is impacted by one bank’s C increasing. We do this by\nsimulating changes in a bank’s credit quality and assessing the increase\nin risk contribution for the bank itself and other banks.\n#CROSS IMPACT MATRIX\n#CHECK FOR SPILLOVER EFFECTS FROM ONE NODE TO ALL OTHERS\nd_RiskDecomp = NULL\nn = length(Ri)\nfor ( j in 1 :n) {\n2\nRi = Ri\n2 1\nRi [ j ] = Ri[ j]+\n2\nres = NetRisk(Ri ,X)\nd_Risk = as.matrix(res [[ 3 ]]) RiskDecomp\n−\nd_RiskDecomp = cbind(d_RiskDecomp,d_Risk)\n}",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 364,
      "chunk_index": 1
    }
  },
  {
    "text": "making connections: network theory 365\n#3D plots\nlibrary(\"RColorBrewer\" );\nlibrary(\" lattice \" );\nlibrary(\" latticeExtra \")\ncloud(d_RiskDecomp,\npanel. 3 d.cloud = panel. 3 dbars ,\n0 25 0 25\nxbase = . , ybase = . ,\nzlim = c(min(d_RiskDecomp) , max(d_RiskDecomp)) ,\nscales = list (arrows = FALSE, just = \"right\") ,\nxlab = \"On\" , ylab = \"From\" , zlab = NULL,\nmain=\"Change in Risk Contribution\" ,\ncol . facet = level . colors(d_RiskDecomp,\nat = do. breaks(range(d_RiskDecomp) , 20 ),",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 365,
      "chunk_index": 0
    }
  },
  {
    "text": "at = do. breaks(range(d_RiskDecomp) , 20 ),\ncol . regions = cm. colors , colors = TRUE) ,\ncolorkey = list (col = cm. colors ,\nat = do. breaks(range(d_RiskDecomp) , 20 )) ,\n#screen = list (z = 40, x = 30)\n−\n)\nbrewer.div < colorRampPalette(brewer. pal( 11 , \"Spectral\") ,\n−\ninterpolate = \"spline\")\nlevelplot (d_RiskDecomp, aspect = \"iso\" ,\ncol . regions = brewer.div( 20 ) ,\nylab=\"Impact from\" , xlab=\"Impact on\" ,\nmain=\"Change in Risk Contribution\")\n1320",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 365,
      "chunk_index": 1
    }
  },
  {
    "text": "main=\"Change in Risk Contribution\")\n1320\nThe plots are shown in Figure . . We have used some advanced\nplotting functions, so as to demonstrate the facile way in which R gener-\nates beautiful plots.\nHere we see the effect of a single bank’s C value increasing by 1 , and\nplot the change in risk contribution of each bank as a consequence. We\nnotice that the effect on its own risk contribution is much higher than on\nthat of other banks.\n13.13.7 Risk Scaling",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 365,
      "chunk_index": 2
    }
  },
  {
    "text": "that of other banks.\n13.13.7 Risk Scaling\nThis is the increase in normalized risk score S¯ as the number of con-\nnections per node increases. We compute this to examine how fast the\nscore increases as the network becomes more connected. Is this growth\nexponential, linear, or logarithmic? We randomly generate graphs with\nincreasing connectivity, and recompute the risk scores. The resulting",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 365,
      "chunk_index": 3
    }
  },
  {
    "text": "366 data science: theories, models, algorithms, and analytics\nFigure13.20: Spillovereffects.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 366,
      "chunk_index": 0
    }
  },
  {
    "text": "making connections: network theory 367\n1321\nplots are shown in Figure . . We see that the risk increases at a less\nthan linear rate. This is good news, as systemic risk does not blow up as\nbanks become more connected.\n#RISK SCALING\n#SIMULATION OF EFFECT OF INCREASED CONNECTIVITY\n#RANDOM GRAPHS\nn= 50 ; k= 100 ; pvec=seq( 0 . 05 , 0 . 50 , 0 . 05 );\nsvec=NULL; sbarvec=NULL\nfor (p in pvec) {\ns_temp = NULL\nsbar_temp = NULL\nfor ( j in 1 :k) {\ng = erdos. renyi .game(n,p, directed=TRUE);",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 367,
      "chunk_index": 0
    }
  },
  {
    "text": "g = erdos. renyi .game(n,p, directed=TRUE);\nA = get . adjacency(g)\ndiag(A) = 1\nc = as.matrix(round(runif(n, 0 , 2 ) , 0 ))\nsyscore = as.numeric(sqrt(t(c) %*% A %*% c))\nsbarscore = syscore/n\ns_temp = c(s_temp, syscore)\nsbar_temp = c(sbar_temp, sbarscore)\n}\nsvec = c(svec ,mean(s_temp))\nsbarvec = c(sbarvec ,mean(sbar_temp))\n}\nplot(pvec ,svec ,type=\"l\" ,\nxlab=\"Prob of connecting to a node\" ,\nylab=\"S\" ,lwd= 3 ,col=\"red\")\nplot(pvec ,sbarvec ,type=\"l\" ,\nxlab=\"Prob of connecting to a node\" ,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 367,
      "chunk_index": 1
    }
  },
  {
    "text": "xlab=\"Prob of connecting to a node\" ,\nylab=\"S_Avg\" ,lwd= 3 ,col=\"red\")\n13.13.8 Too Big To Fail?\nAn often suggested remedy for systemic risk is to break up large banks,\ni.e., directly mitigate the too-big-to-fail phenomenon. We calculate the\nchange in risk score S, and normalized risk score S¯ as the number of\nnodes increases, while keeping the average number of connections be-\n5000\ntween nodes constant. This is repeated times for each fixed number",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 367,
      "chunk_index": 2
    }
  },
  {
    "text": "368 data science: theories, models, algorithms, and analytics\nFigure13.21: Howriskincreases\nwithconnectivityofthenetwork.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 368,
      "chunk_index": 0
    }
  },
  {
    "text": "making connections: network theory 369\n5000\nof nodes and the mean risk score across simulations is plotted on\nthe y-axis against the number of nodes on the x-axis. We see that sys-\ntemic risk increases when banks are broken up, but the normalized risk\nscore decreases. Despite the network effect S¯ declining, overall risk S in\n1322\nfact increases. See Figure . .\n#TOO BIG TO FAIL\n#SIMULATION OF EFFECT OF INCREASED NODES AND REDUCED CONNECTIVITY",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 369,
      "chunk_index": 0
    }
  },
  {
    "text": "nvec=seq( 10 , 100 , 10 ); k= 5000 ; svec=NULL; sbarvec=NULL\nfor (n in nvec) {\ns_temp = NULL\nsbar_temp = NULL\np =\n5/n\nfor ( j in 1 :k) {\ng = erdos. renyi .game(n,p, directed=TRUE);\nA = get . adjacency(g)\ndiag(A) = 1\nc = as.matrix(round(runif(n, 0 , 2 ) , 0 ))\nsyscore = as.numeric(sqrt(t(c) %*% A %*% c))\nsbarscore = syscore/n\ns_temp = c(s_temp, syscore)\nsbar_temp = c(sbar_temp, sbarscore)\n}\nsvec = c(svec ,mean(s_temp))\nsbarvec = c(sbarvec ,mean(sbar_temp))\n}\nplot(nvec ,svec ,type=\"l\" ,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 369,
      "chunk_index": 1
    }
  },
  {
    "text": "}\nplot(nvec ,svec ,type=\"l\" ,\nxlab=\"Number of nodes\" ,ylab=\"S\" ,\nylim=c( 0 ,max(svec )) ,lwd= 3 ,col=\"red\")\nplot(nvec ,sbarvec ,type=\"l\" ,\nxlab=\"Number of nodes\" ,ylab=\"S_Avg\" ,\nylim=c( 0 ,max(sbarvec )) ,lwd= 3 ,col=\"red\")\n13.13.9 Application of the model to the banking network in India\nThe program code for systemic risk networks was applied to real-world\ndata in India to produce daily maps of the Indian banking network,\nas well as the corresponding risk scores. The credit risk vector C was",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 369,
      "chunk_index": 2
    }
  },
  {
    "text": "based on credit ratings for Indian financial institutions (FIs). The net-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 369,
      "chunk_index": 3
    }
  },
  {
    "text": "370 data science: theories, models, algorithms, and analytics\nFigure13.22: Howriskincreases\nwithconnectivityofthenetwork.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 370,
      "chunk_index": 0
    }
  },
  {
    "text": "making connections: network theory 371\nwork adjacency matrix was constructred using the ideas in a paper by\n2012\nBillio, Getmansky, Lo, and Pelizzon ( ) who create a network using\nGranger causality. This directed network comprises an adjacency matrix\nof values (0,1) where node i connects to node j if the returns of bank i\nGranger cause those of bank j, i.e., edge E = 1. This was applied to\ni,j\nU.S. financial institution stock return data, and in a follow-up paper, to",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 371,
      "chunk_index": 0
    }
  },
  {
    "text": "CDS spread data from U.S., Europe, and Japan (see Billio, Getmansky,\n2014\nGray, Lo, Merton, and Pelizzon ( )), where the global financial system\nis also found to be highly interconnected. In the application of the Das\n2014\n( ) methodology to India, the network matrix is created using this\nGranger causality method to Indian FI stock returns.\nThe system is available in real time and may be accessed directly\nthrough a browser. To begin, different selections may be made of a sub-\n1323",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 371,
      "chunk_index": 1
    }
  },
  {
    "text": "1323\nset of FIs for analysis. See Figure . for the screenshots of this step.\nOnce these selections are made and the “Submit” button is hit, the\nsystem generates the network and the various risk metrics, shown in\n1324 1325\nFigures . and . , respectively.\n13.14 Map of Science\nIt is appropriate to end this chapter by showcasing network science with\na wonderful image of the connection network between various scientific\n1326\ndisciplines. See Figure . . Note that the social sciences are most con-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 371,
      "chunk_index": 2
    }
  },
  {
    "text": "nected to medicine and engineering. But there is homophily here, i.e.,\nlikes tend to be in groups with likes.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 371,
      "chunk_index": 3
    }
  },
  {
    "text": "372 data science: theories, models, algorithms, and analytics\nFigure13.23: Screensforselecting\ntherelevantsetofIndianFIsto\nconstructthebankingnetwork.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 372,
      "chunk_index": 0
    }
  },
  {
    "text": "making connections: network theory 373\nFigure13.24: ScreensfortheIndian\nFIsbankingnetwork. Theupper\nplotshowstheentirenetwork.\nThelowerplotshowsthenetwork\nwhenwemouseoverthebankin\nthemiddleoftheplot. Redlines\nshowthatthebankisimpacted\nbytheotherbanks,andbluelines\ndepictthatthebankimpactsthe\nothers,inaGrangercausalmanner.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 373,
      "chunk_index": 0
    }
  },
  {
    "text": "374 data science: theories, models, algorithms, and analytics\nFigure13.25: Screensforsystemic\nriskmetricsoftheIndianFIsbank-\ningnetwork. Thetopplotshows\nthecurrentriskmetrics,andthe\nbottomplotshowsthehistoryfrom\n2008.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 374,
      "chunk_index": 0
    }
  },
  {
    "text": "making connections: network theory 375\nFigure13.26: TheMapofScience.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 375,
      "chunk_index": 0
    }
  },
  {
    "text": "14\nStatistical Brains: Neural Networks\n14.1 Overview\nNeural Networks (NNs) are one form of nonlinear regression. You are\nusually familiar with linear regressions, but nonlinear regressions are\njust as easy to understand. In a linear regression, we have\nY = X β+e\n(cid:48)\nwhere X Rt n and the regression solution is (as is known from before),\n×\n∈\nsimply equal to β = (X X) 1(X Y).\n(cid:48) − (cid:48)\nTo get this result we minimize the sum of squared errors.\nmine\n(cid:48)\ne = (Y X\n(cid:48)\nβ)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 377,
      "chunk_index": 0
    }
  },
  {
    "text": "mine\n(cid:48)\ne = (Y X\n(cid:48)\nβ)\n(cid:48)\n(Y X\n(cid:48)\nβ)\nβ − −\n= (Y X β) Y (Y X β) (X β)\n(cid:48) (cid:48) (cid:48) (cid:48) (cid:48)\n− − −\n= Y Y (X β) Y Y (X β)+β2(X X)\n(cid:48) (cid:48) (cid:48) (cid:48) (cid:48) (cid:48)\n− −\n= Y\n(cid:48)\nY 2(X\n(cid:48)\nβ)\n(cid:48)\nY+β2(X\n(cid:48)\nX)\n−\nDifferentiating w.r.t. β gives the following f.o.c:\n2β(X\n(cid:48)\nX) 2(X\n(cid:48)\nY) = 0\n−\n=\n⇒\nβ = (X X) 1(X Y)\n(cid:48) − (cid:48)\nWe can examine this by using the markowitzdata.txt data set.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 377,
      "chunk_index": 1
    }
  },
  {
    "text": "> data = read.table(\"markowitzdata.txt\",header=TRUE)\n> dim(data)\n[1] 1507 10\n> names(data)\n[1] \"X.DATE\" \"SUNW\" \"MSFT\" \"IBM\" \"CSCO\" \"AMZN\" \"mktrf\"\n[8] \"smb\" \"hml\" \"rf\"\n> amzn = as.matrix(data[,6])\n> f3 = as.matrix(data[ ,7:9])",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 377,
      "chunk_index": 2
    }
  },
  {
    "text": "378 data science: theories, models, algorithms, and analytics\n> res = lm(amzn ~ f3)\n> summary(res)\nCall:\nlm(formula = amzn ~ f3)\nResiduals:\nMin 1Q Median 3Q Max\n0.225716 0.014029 0.001142 0.013335 0.329627\n− − −\nCoefficients:\nEstimate Std. Error t value Pr(>|t|)\n(Intercept) 0.0015168 0.0009284 1.634 0.10249\nf3mktrf 1.4190809 0.1014850 13.983 < 2e 16 ***\nf3smb 0.5228436 0.1738084 3.008 0.002−67\n**\nf3hml 1.1502401 0.2081942 5.525 3.88e 08\n***\n− − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 378,
      "chunk_index": 0
    }
  },
  {
    "text": "***\n− − −\n− S − i − gnif. codes: 0 ï£¡***ï£¡ 0.001 ï£¡**ï£¡ 0.01 ï£¡*ï£¡ 0.05 ï£¡.ï£¡ 0.1 ï£¡ ï£¡ 1\nResidual standard error: 0.03581 on 1503 degrees of freedom\nMultiple R squared: 0.2233, Adjusted R squared: 0.2218\nF statisti − c : 144.1 on 3 and 1503 DF, p va − lue: < 2.2e 16\n− − −\n> wuns = matrix(1,length(amzn),1)\n> x = cbind(wuns,f3)\n> b = solve(t(x) %*% x) %*% (t(x) %*% amzn)\n> b\n[,1]\n0.001516848\nmktrf 1.419080894\nsmb 0.522843591\nhml 1.150240145\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 378,
      "chunk_index": 1
    }
  },
  {
    "text": "smb 0.522843591\nhml 1.150240145\n−\nWe see at the end of the program listing that our formula for the co-\nefficients of the minimized least squares problem β = (X X) 1(X Y)\n(cid:48) − (cid:48)\nexactly matches that from the regression command lm.\n14.2 Nonlinear Regression\nA nonlinear regression is of the form\nY = f(X;β)+e\nwhere f( ) is a nonlinear function. Note that, for example, Y = β +\n0\n·\nβ X + β X2 +e is not a nonlinear regression, even though it contains\n1 2\nnonlinear terms like X2.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 378,
      "chunk_index": 2
    }
  },
  {
    "text": "1 2\nnonlinear terms like X2.\nComputing the coefficients in a nonlinear regression again follows in\nthe same way as for a linear regression.\nmine\n(cid:48)\ne = (Y f(X;β))\n(cid:48)\n(Y f(X;β))\nβ − −\n= Y\n(cid:48)\nY 2f(X;β)\n(cid:48)\nY+ f(X;β)\n(cid:48)\nf(X;β)\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 378,
      "chunk_index": 3
    }
  },
  {
    "text": "statistical brains: neural networks 379\nDifferentiating w.r.t. β gives the following f.o.c:\n(cid:18) (cid:19) (cid:18) (cid:19)\ndf(X;β) (cid:48) df(X;β) (cid:48)\n2 Y+2 f(X;β) = 0\n− dβ dβ\n(cid:18) (cid:19) (cid:18) (cid:19)\ndf(X;β) (cid:48) df(X;β) (cid:48)\nY = f(X;β)\ndβ dβ\nwhich is then solved numerically for β Rn. The approach taken usually\n∈\ninvolves the Newton-Raphson method, see for example:\nhttp://en.wikipedia.org/wiki/Newton’s method.\n14.3 Perceptrons",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 379,
      "chunk_index": 0
    }
  },
  {
    "text": "14.3 Perceptrons\nNeural networks are special forms of nonlinear regressions where the\ndecision system for which the NN is built mimics the way the brain is\nsupposed to work (whether it works like a NN is up for grabs of course).\nThe basic building block of a neural network is a perceptron. A per-\nceptron is like a neuron in a human brain. It takes inputs (e.g. sensory in\na real brain) and then produces an output signal. An entire network of\nperceptrons is called a neural net.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 379,
      "chunk_index": 1
    }
  },
  {
    "text": "perceptrons is called a neural net.\nFor example, if you make a credit card application, then the inputs\ncomprise a whole set of personal data such as age, sex, income, credit\nscore, employment status, etc, which are then passed to a series of per-\nceptrons in parallel. This is the first “layer” of assessment. Each of the\nperceptrons then emits an output signal which may then be passed to\nanother layer of perceptrons, who again produce another signal. This",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 379,
      "chunk_index": 2
    }
  },
  {
    "text": "second layer is often known as the “hidden” perceptron layer. Finally,\nafter many hidden layers, the signals are all passed to a single percep-\ntron which emits the decision signal to issue you a credit card or to deny\nyour application.\nPerceptrons may emit continuous signals or binary (0,1) signals. In\nthe case of the credit card application, the final perceptron is a binary\none. Such perceptrons are implemented by means of “squashing” func-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 379,
      "chunk_index": 3
    }
  },
  {
    "text": "tions. For example, a really simple squashing function is one that issues\n1 0\na if the function value is positive and a if it is negative. More gener-\nally,\n(cid:40)\n1 if g(x) > T\nS(x) =\n0 if g(x) T\n≤\nwhere g(x) is any function taking positive and negative values, for in-\nstance, g(x) ( ∞ ,+∞). T is a threshold level.\n∈ −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 379,
      "chunk_index": 4
    }
  },
  {
    "text": "380 data science: theories, models, algorithms, and analytics\nA neural network with many layers is also known as a “multi-layered”\nperceptron, i.e., all those perceptrons together may be thought of as one\n141\nsingle, big perceptron. See Figure . for an example of such a network\nf(x) Figure14.1: Afeed-forwardmulti-\nlayerneuralnetwork.\nx1 x2 x3 x4\ny1 y2 y3\nz1\nNeural net models are related to Deep Learning, where the number of\nhidden layers is vastly greater than was possible in the past when com-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 380,
      "chunk_index": 0
    }
  },
  {
    "text": "putational power was limited. Now, deep learning nets cascade through\n20 30\n- layers, resulting in a surprising ability of neural nets in mimicking\nhuman learning processes. see: http://en.wikipedia.org/wiki/Deep_\nlearning. And also see: http://deeplearning.net/.\nBinary NNs are also thought of as a category of classifier systems.\nThey are widely used to divide members of a population into classes.\nBut NNs with continuous output are also popular. As we will see later,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 380,
      "chunk_index": 1
    }
  },
  {
    "text": "researchers have used NNs to learn the Black-Scholes option pricing\nmodel.\nAreas of application: credit cards, risk management, forecasting cor-\nporate defaults, forecasting economic regimes, measuring the gains from\nmass mailings by mapping demographics to success rates.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 380,
      "chunk_index": 2
    }
  },
  {
    "text": "statistical brains: neural networks 381\n14.4 Squashing Functions\nSquashing functions may be more general than just binary. They usually\nsquash the output signal into a narrow range, usually (0,1). A common\nchoice is the logistic function (also known as the sigmoid function).\n1\nf(x) =\n1+e wx\n−\nThink of w as the adjustable weight. Another common choice is the pro-\nbit function\nf(x) = Φ(w x)\nwhere Φ( ) is the cumulative normal distribution function.\n·\n14.5 How does the NN work?",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 381,
      "chunk_index": 0
    }
  },
  {
    "text": "·\n14.5 How does the NN work?\nThe easiest way to see how a NN works is to think of the simplest NN,\ni.e. one with a single perceptron generating a binary output. The per-\nceptron has n inputs, with values x ,i = 1...n and current weights\ni\nw ,i = 1...n. It generates an output y.\ni\nThe “net input” is defined as\nn\n∑\nw x\ni i\ni=1\nIf the net input is greater than a threshold T, then the output signal is\ny = 1, and if it is less than T, the output is y = 0. The actual output",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 381,
      "chunk_index": 1
    }
  },
  {
    "text": "is called the “desired” output and is denoted d = 0,1 . Hence, the\n{ }\n“training” data provided to the NN comprises both the inputs x and the\ni\ndesired output d.\nThe output of our single perceptron model will be the sigmoid func-\ntion of the net input, i.e.\n1\ny =\n1+exp( ∑n w x )\n− i=1 i i\nFor a given input set, the error in the NN is\nE = 1 ∑ m (y d )2\nj j\n2 −\nj=1\nwhere m is the size of the training data set. The optimal NN for given",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 381,
      "chunk_index": 2
    }
  },
  {
    "text": "data is obtained by finding the weights w that minimize this error func-\ni\ntion E. Once we have the optimal weights, we have a calibrated “feed-\nforward” neural net.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 381,
      "chunk_index": 3
    }
  },
  {
    "text": "382 data science: theories, models, algorithms, and analytics\nFor a given squashing function f, and input x = [x ,x ,...,x ] , the\n1 2 n (cid:48)\nmulti-layer perceptron will given an output at the hidden layer of\n(cid:32) (cid:33)\nn\n∑\ny(x) = f w + w x\n0 j j\nj=1\nand then at the final output level the node is\n(cid:32) (cid:32) (cid:33)(cid:33)\nN n\n∑ ∑\nz(x) = f w + w f w + w x\n0 i 0i ji j\n·\ni=1 j=1\nwhere the nested structure of the neural net is quite apparent.\n14.5.1 Logit/Probit Model",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 382,
      "chunk_index": 0
    }
  },
  {
    "text": "14.5.1 Logit/Probit Model\nThe special model above with a single perceptron is actually nothing\nelse than the logit regression model. If the squashing function is taken to\nthe cumulative normal distribution, then the model becomes the probit\nregression model. In both cases though, the model is fitted by minimiz-\ning squared errors, not by maximum likelihood, which is how standard\nlogit/probit models are parameterized.\n14.5.2 Connection to hyperplanes",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 382,
      "chunk_index": 1
    }
  },
  {
    "text": "14.5.2 Connection to hyperplanes\nNote that in binary squashing functions, the net input is passed through\na sigmoid function and then compared to the threshold level T. This\nsigmoid function is a monotone one. Hence, this means that there must\nbe a level T at which the net input ∑n w x must be for the result to be\n(cid:48) i=1 i i\non the cusp. The following is the equation for a hyperplane\nn\n∑\nw x = T\ni i (cid:48)\ni=1\nwhich also implies that observations in n-dimensional space of the in-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 382,
      "chunk_index": 2
    }
  },
  {
    "text": "puts x , must lie on one side or the other of this hyperplane. If above the\ni\nhyperplane, then y = 1, else y = 0. Hence, single perceptrons in neural\nnets have a simple geometrical intuition.\n14.6 Feedback/Backpropagation\nWhat distinguishes neural nets from ordinary nonlinear regressions is\nfeedback. Neural nets learn from feedback as they are used. Feedback is\nimplemented using a technique called backpropagation.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 382,
      "chunk_index": 3
    }
  },
  {
    "text": "statistical brains: neural networks 383\nSuppose you have a calibrated NN. Now you obtain another observa-\ntion of data and run it through the NN. Comparing the output value y\nwith the desired observation d gives you the error for this observation. If\nthe error is large, then it makes sense to update the weights in the NN,\nso as to self-correct. This process of self-correction is known as “back-\npropagation”.\nThe benefit of backpropagation is that a full re-fitting exercise is not",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 383,
      "chunk_index": 0
    }
  },
  {
    "text": "required. Using simple rules the correction to the weights can be applied\ngradually in a learning manner.\nLets look at backpropagation with a simple example using a single\nperceptron. Consider the j-th perceptron. The sigmoid of this is\n1\ny j = 1+exp (cid:0) ∑n w x (cid:1)\n− i=1 i ij\nwhere y is the output of the j-th perceptron, and x is the i-th input to\nj ij\nthe j-th perceptron. The error from this observation is (y d ). Recalling\nj j\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 383,
      "chunk_index": 1
    }
  },
  {
    "text": "j j\n−\nthat E = 1 ∑m (y d )2, we may compute the change in error with\n2 j=1 j − j\nrespect to the j-th output, i.e.\n∂E\n= y d\nj j\n∂y −\nj\nNote also that\ndy\nj\n= y (1 y )w\nj j i\ndx −\nij\nand\ndy\nj\n= y (1 y )x\nj j ij\ndw −\ni\nNext, we examine how the error changes with input values:\n∂E ∂E dy j\n= = (y d )y (1 y )w\nj j j j i\n∂x ∂y × dx − −\nij j ij\nWe can now get to the value of interest, which is the change in error\nvalue with respect to the weights\n∂E ∂E dy j\n= = (y d )y (1 y )x , i\nj j j j ij",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 383,
      "chunk_index": 2
    }
  },
  {
    "text": "∂E ∂E dy j\n= = (y d )y (1 y )x , i\nj j j j ij\n∂w ∂y × dw − − ∀\ni j i\nWe thus have one equation for each weight w and each observation j.\ni\n(Note that the w apply across perceptrons. A more general case might\ni\nbe where we have weights for each perceptron, i.e., w .) Instead of up-\nij\ndating on just one observation, we might want to do this for many obser-\nvations in which case the error derivative would be\n∂E ∑\n= (y d )y (1 y )x , i\nj j j j ij\n∂w − − ∀\ni j",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 383,
      "chunk_index": 3
    }
  },
  {
    "text": "384 data science: theories, models, algorithms, and analytics\nTherefore, if ∂E > 0, then we would need to reduce w to bring down\n∂w\ni\ni\nE. By how much? Here is where some art and judgment is imposed.\nThere is a tuning parameter 0 < γ < 1 which we apply to w to shrink it\ni\nwhen the weight needs to be reduced. Likewise, if the derivative ∂E < 0,\n∂w\ni\nthen we would increase w by dividing it by γ.\ni\n14.6.1 Extension to many perceptrons",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 384,
      "chunk_index": 0
    }
  },
  {
    "text": "i\n14.6.1 Extension to many perceptrons\nOur notation now becomes extended to weights w which stand for the\nik\nweight on the i-th input to the k-th perceptron. The derivative for the\nerror becomes\n∂E ∑\n= (y d )y (1 y )x , i,k\nj j j j ikj\n∂w − − ∀\nik j\nHence all nodes in the network have their weights updated. In many\ncases of course, we can just take the derivatives numerically. Change the\nweight w and see what happens to the error.\nik\n14.7 Research Applications\n14.7.1 Discovering Black-Scholes",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 384,
      "chunk_index": 1
    }
  },
  {
    "text": "14.7.1 Discovering Black-Scholes\n1994\nSee the paper by Hutchinson, Lo, and Poggio ( )), A Nonparametric\nApproach to Pricing and Hedging Securities Via Learning Networks, The\nJournal of Finance, Vol XLIX.\n14.7.2 Forecasting\n2005\nSee the paper by Ghiassi, Saidane, and Zimbra ( ). “A dynamic arti-\nficial neural network model for forecasting time series events,” Interna-\ntional Journal of Forecasting 21 , 341 – 362 .\n14.8 Package neuralnet in R",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 384,
      "chunk_index": 2
    }
  },
  {
    "text": "14.8 Package neuralnet in R\nThe package focuses on multi-layer perceptrons (MLP), see Bishop\n1995\n( ), which are well applicable when modeling functional relation-\nships. The underlying structure of an MLP is a directed graph, i.e. it\nconsists of vertices and directed edges, in this context called neurons and\n1995\nsynapses. [See Bishop ( ), Neural networks for pattern recognition.\nOxford University Press, New York.]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 384,
      "chunk_index": 3
    }
  },
  {
    "text": "statistical brains: neural networks 385\nThe data set used by this package as an example is the infert data set\nthat comes bundled with R.\n> library(neuralnet)\nLoading required package: grid\nLoading required package: MASS\n> names( infert )\n1\n[ ] \"education\" \"age\" \"parity\" \"induced\"\n5\n[ ] \"case\" \"spontaneous\" \"stratum\" \"pooled.stratum\"\n> summary( infert )\neducation age parity induced\n0 5 12 21 00 1 000 0 0000\nyrs : Min. : . Min. : . Min. : .\n−\n6 11 120 1 28 00 1 1 000 1 0 0000",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 385,
      "chunk_index": 0
    }
  },
  {
    "text": "−\n6 11 120 1 28 00 1 1 000 1 0 0000\nyrs: st Qu.: . st Qu.: . st Qu.: .\n−\n12 116 31 00 2 000 0 0000\n+ yrs: Median : . Median : . Median : .\n31 50 2 093 0 5726\nMean : . Mean : . Mean : .\n3 35 25 3 3 000 3 1 0000\nrd Qu.: . rd Qu.: . rd Qu.: .\n44 00 6 000 2 0000\nMax. : . Max. : . Max. : .\ncase spontaneous stratum pooled.stratum\n0 0000 0 0000 1 00 1 00\nMin. : . Min. : . Min. : . Min. : .\n1 0 0000 1 0 0000 1 21 00 1 19 00\nst Qu.: . st Qu.: . st Qu.: . st Qu.: .\n0 0000 0 0000 42 00 36 00",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 385,
      "chunk_index": 1
    }
  },
  {
    "text": "0 0000 0 0000 42 00 36 00\nMedian : . Median : . Median : . Median : .\n0 3347 0 5766 41 87 33 58\nMean : . Mean : . Mean : . Mean : .\n3 1 0000 3 1 0000 3 62 25 3 48 25\nrd Qu.: . rd Qu.: . rd Qu.: . rd Qu.: .\n1 0000 2 0000 83 00 63 00\nMax. : . Max. : . Max. : . Max. : .\nThis data set examines infertility after induced and spontaneous abor-\ntion. The variables induced and spontaneous take values in 0,1,2 in-\n{ }\ndicating the number of previous abortions. The variable parity denotes\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 385,
      "chunk_index": 2
    }
  },
  {
    "text": "1\nthe number of births. The variable case equals if the woman is infertile\n0\nand otherwise. The idea is to model infertility.\nAs a first step, let’s fit a logit model to the data.\n> res = glm(case ~ age+parity+induced+spontaneous,\nfamily=binomial(link=\"logit\"), data=infert)\n> summary(res)\nCall:\nglm(formula = case ~ age + parity + induced + spontaneous,\nfamily = binomial(link = \"logit\"),\ndata = infert)\nDeviance Residuals:\nMin 1Q Median 3Q Max\n1.6281 0.8055 0.5298 0.8668 2.6141\n− − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 385,
      "chunk_index": 3
    }
  },
  {
    "text": "1.6281 0.8055 0.5298 0.8668 2.6141\n− − −\nCoefficients:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 385,
      "chunk_index": 4
    }
  },
  {
    "text": "386 data science: theories, models, algorithms, and analytics\nEstimate Std. Error z value Pr(>|z|)\n(Intercept) 2.85239 1.00428 2.840 0.00451 **\nage −0.05318 0.03014 −1.764 0.07767 .\nparity 0.70883 0.18091 3.918 8.92e 05 ***\ninduced −1.18966 0.28987 −4.104 4.06e −05 ***\nspontaneous 1.92534 0.29863 6.447 1.14e −10 ***\n−\n− S − i − gnif. codes: 0 ï£¡***ï£¡ 0.001 ï£¡**ï£¡ 0.01 ï£¡*ï£¡ 0.05 ï£¡.ï£¡ 0.1 ï£¡ ï£¡ 1\n(Dispersion parameter for binomial family taken to be 1)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 386,
      "chunk_index": 0
    }
  },
  {
    "text": "Null deviance: 316.17 on 247 degrees of freedom\nResidual deviance: 260.94 on 243 degrees of freedom\nAIC: 270.94\nNumber of Fisher Scoring iterations: 4\nAll explanatory variables are statistically significant. We now run this\ndata through a neural net, as follows.\n> nn = neuralnet(case~age+parity+induced+spontaneous,hidden=2,data=infert)\n> nn\nCall: neuralnet(formula = case ~ age + parity + induced + spontaneous, data = infert , hidden = 2)\n1 repetition was calculated.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 386,
      "chunk_index": 1
    }
  },
  {
    "text": "1 repetition was calculated.\nError Reached Threshold Steps\n1 19.36463007 0.008949536618 20111\n> nn$result.matrix\n1\nerror 19.364630070610\nreached.threshold 0.008949536618\nsteps 20111.000000000000\nIntercept.to.1layhid1 9.422192588834\nage.to.1layhid1 1.293381222338\nparity.to.1layhid1 −19.489105822032\ninduced.to.1layhid1 −37.616977251411\nspontaneous.to.1layhid1 32.647955233030\nIntercept.to.1layhid2 5.142357912661\nage.to.1layhid2 0.077293384832\nparity.to.1layhid2 −2.875918354167",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 386,
      "chunk_index": 2
    }
  },
  {
    "text": "parity.to.1layhid2 −2.875918354167\ninduced.to.1layhid2 4.552792010965\nspontaneous.to.1layhid2 −5.558639450018\nIntercept.to.case\n−1.155876751703\n1layhid.1.to.case 0.545821730892\n1layhid.2.to.case −1.022853550121\n−\n142\nNow we can go ahead and visualize the neural net. See Figure . .\nWe see the weights on the initial input variables that go into two hid-\nden perceptrons, and then these are fed into the output perceptron, that\ngenerates the result. We can look at the data and output as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 386,
      "chunk_index": 3
    }
  },
  {
    "text": "> head(cbind(nn$covariate ,nn$net . result [[ 1 ]]))\n1 2 3 4 5\n[ , ] [ , ] [ , ] [ , ] [ , ]\n1 26 6 1 2 0 1420779618\n.\n2 42 1 1 0 0 5886305435\n.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 386,
      "chunk_index": 4
    }
  },
  {
    "text": "statistical brains: neural networks 387\nFigure14.2: Theneuralnetforthe\n1 1\ninfertdatasetwithtwopercep-\ntronsinasinglehiddenlayer.\nage\n-0\n.0\n-1.29338 9\n.4\n2 2\n7 1\n7 9\n2\n9\nparity\n-19.48911 -0.54582\n1 .1\n5\n2 5\n.8\n7\n8\n8\n5\n9\n2 case\n8\n9\n6\n1\n3 7. 6 5 .1\ninduced\n-4.55279\n4 2\n3\n6\n-1.02285\n6\n9\n7\n3\n2.\n6 4 -5.55864\nspontaneous\nError: 19.36463 Steps: 20111",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 387,
      "chunk_index": 0
    }
  },
  {
    "text": "388 data science: theories, models, algorithms, and analytics\n3 39 6 2 0 0 1330583729\n.\n4 34 4 2 0 0 1404906398\n.\n5 35 3 1 1 0 4175799845\n.\n6 36 4 2 1 0 8385294748\n.\nWe can compare the output to that from the logit model, by looking at\nthe correlation of the fitted values from both models.\n> cor(cbind(nn$net . result [[ 1 ]] , res$fitted . values ))\n1 2\n[ , ] [ , ]\n1 1 0000000000 0 8814759106\n[ ,] . .\n2 0 8814759106 1 0000000000\n[ ,] . .\n88",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 388,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ,] . .\n2 0 8814759106 1 0000000000\n[ ,] . .\n88\nAs we see, the models match up with % correlation. The output is a\nprobability of infertility.\nWe can add in an option for back propagation, and see how the results\nchange.\n> nn 2 = neuralnet(case~age+parity+induced+spontaneous ,\nhidden= 2 , algorithm=\"rprop+\" , data=infert )\n> cor(cbind(nn 2$net . result [[ 1 ]] , res$fitted . values ))\n1 2\n[ , ] [ , ]\n1 1 00000000 0 88816742\n[ ,] . .\n2 0 88816742 1 00000000\n[ ,] . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 388,
      "chunk_index": 1
    }
  },
  {
    "text": "[ ,] . .\n2 0 88816742 1 00000000\n[ ,] . .\n> cor(cbind(nn 2$net . result [[ 1 ]] ,nn$fitted . result [[ 1 ]]))\nThere does not appear to be any major improvement.\nGiven a calibrated neural net, how do we use it to compute values for\na new observation? Here is an example.\n> compute(nn, covariate=matrix(c( 30 , 1 , 0 , 1 ) , 1 , 4 ))\n$neurons\n$neurons[[ 1 ]]\n1 2 3 4 5\n[ , ] [ , ] [ , ] [ , ] [ , ]\n1 1 30 1 0 1\n[ ,]\n$neurons[[ 2 ]]\n1 2 3\n[ , ] [ , ] [ , ]\n1 1 0 00000009027594872 0 5351507372",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 388,
      "chunk_index": 2
    }
  },
  {
    "text": "1 1 0 00000009027594872 0 5351507372\n[ ,] . .\n$net . result\n1\n[ , ]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 388,
      "chunk_index": 3
    }
  },
  {
    "text": "statistical brains: neural networks 389\n1 0 6084958711\n[ ,] .\nWe can assess statistical significance of the model as follows:\n> confidence.interval(nn,alpha=0.10)\n$lower.ci\n$lower.ci[[1]]\n$lower.ci[[1]][[1]]\n[,1] [,2]\n[1,] 1.942871917 1.0100502322\n[2,] 2.178214123 0.1677202246\n[3,] −32.411347153 −0.6941528859\n[4,] −12.311139796 −9.8846504753\n[5,] 10.339781603 −12.1349900614\n−\n$lower.ci[[1]][[2]]\n[,1]\n[1,] 0.7352919387\n[2,] 0.7457112438\n[3,] −1.4851089618\n−\n$upper.ci\n$upper.ci[[1]]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 389,
      "chunk_index": 0
    }
  },
  {
    "text": "[3,] −1.4851089618\n−\n$upper.ci\n$upper.ci[[1]]\n$upper.ci[[1]][[1]]\n[,1] [,2]\n[1,] 16.9015132608 9.27466559308\n[2,] 0.4085483215 0.01313345496\n[3,] −6.5668644910 6.44598959422\n[4,] 6−2.9228147066 0.77906645334\n[5,] 54.9561288631 1.01771116133\n$upper.ci[[1]][[2]]\n[,1]\n[1,] 1.5764615647\n[2,] 0.3459322180\n[3,] −0.5605981384\n−\n$nic\n[1] 21.19262393\nThe confidence level is (1 α). This is at the 90 % level, and at the 5 %\n−\nlevel we get:\n> confidence.interval(nn,alpha=0.95)\n$lower.ci\n$lower.ci[[1]]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 389,
      "chunk_index": 1
    }
  },
  {
    "text": "$lower.ci\n$lower.ci[[1]]\n$lower.ci[[1]][[1]]\n[,1] [,2]\n[1,] 9.137058342 4.98482188887\n[2,] 1.327113719 0.08074072852\n[3,] −19.981740610 −2.73981647809\n[4,] −36.652242454 4.75605852615\n[5,] 31.797500416 −5.80934975682\n−\n$lower.ci[[1]][[2]]\n[,1]\n[1,] 1.1398427910\n[2,] 0.5534421216\n[3,] −1.0404761197\n−\n$upper.ci\n$upper.ci[[1]]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 389,
      "chunk_index": 2
    }
  },
  {
    "text": "390 data science: theories, models, algorithms, and analytics\n$upper.ci[[1]][[1]]\n[,1] [,2]\n[1,] 9.707326836 5.29989393645\n[2,] 1.259648725 0.07384604115\n[3,] −18.996471034 −3.01202023024\n[4,] −38.581712048 4.34952549578\n[5,] 33.498410050 −5.30792914321\n−\n$upper.ci[[1]][[2]]\n[,1]\n[1,] 1.1719107124\n[2,] 0.5382013402\n[3,] −1.0052309806\n−\n$nic\n[1] 21.19262393\n14.9 Package nnet in R\nWe repeat these calculations using this alternate package.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 390,
      "chunk_index": 0
    }
  },
  {
    "text": "> nn 3 = nnet(case~age+parity+induced+spontaneous ,data=infert , size= 2 )\n# weights : 13\n58 675032\ninitial value .\n10 47 924314\niter value .\n20 41 032965\niter value .\n30 40 169634\niter value .\n40 39 548014\niter value .\n50 39 025079\niter value .\n60 38 657788\niter value .\n70 38 464035\niter value .\n80 38 273805\niter value .\n90 38 189795\niter value .\n100 38 116595\niter value .\n38 116595\nfinal value .\n100\nstopped after iterations\n3\n> nn\na 4 2 1 network with 13 weights\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 390,
      "chunk_index": 1
    }
  },
  {
    "text": "3\n> nn\na 4 2 1 network with 13 weights\n− −\ninputs : age parity induced spontaneous\noutput(s ): case\noptions were\n−\n> nn 3 .out = predict(nn 3 )\n> dim(nn 3 .out)\n1 248 1\n[ ]\n> cor(cbind(nn$fitted . result [[ 1 ]] ,nn 3 .out))",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 390,
      "chunk_index": 2
    }
  },
  {
    "text": "statistical brains: neural networks 391\n1\n[ , ]\n1 1\n[ ,]\nWe see that package nnet gives the same result as that from package\nneuralnet.\nAs another example of classification, rather than probability, we revisit\nthe IRIS data set we have used in the realm of Bayesian classifiers.\n> data(iris)\n> # use half the iris data\n> ir = rbind(iris3 [ , ,1] , iris3 [ , ,2] , iris3 [ , ,3])\n> targets = class.ind( c(rep(\"s\" , 50), rep(\"c\" , 50), rep(\"v\" , 50)) )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 391,
      "chunk_index": 0
    }
  },
  {
    "text": "> samp = c(sample(1:50,25), sample(51:100,25), sample(101:150,25))\n> ir1 = nnet(ir[samp,] , targets[samp,] ,size = 2, rang = 0.1,\ndecay = 5e 4, maxit = 200)\n−\n# weights: 19\ninitial value 57.017869\niter 10 value 43.401134\niter 20 value 30.331122\niter 30 value 27.100909\niter 40 value 26.459441\niter 50 value 18.899712\niter 60 value 18.082379\niter 70 value 17.716302\niter 80 value 17.574713\niter 90 value 17.555689\niter 100 value 17.528989\niter 110 value 17.523788\niter 120 value 17.521761",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 391,
      "chunk_index": 1
    }
  },
  {
    "text": "iter 110 value 17.523788\niter 120 value 17.521761\niter 130 value 17.521578\niter 140 value 17.520840\niter 150 value 17.520649\niter 150 value 17.520649\nfinal value 17.520649\nconverged\n> orig = max.col(targets[ samp,])\n−\n> orig\n[1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1\n[36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[71] 3 3 3 3 3\n> pred = max.col(predict(ir1 , ir[ samp,]))\n−\n> pred",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 391,
      "chunk_index": 2
    }
  },
  {
    "text": "−\n> pred\n[1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 3 3 1 1 1 1 1 1 1\n[36] 3 3 1 1 1 1 1 1 1 1 3 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[71] 3 3 3 3 3\n> table(orig ,pred)\npred\norig 1 2 3\n1 20 0 5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 391,
      "chunk_index": 3
    }
  },
  {
    "text": "392 data science: theories, models, algorithms, and analytics\n2 0 25 0\n3 0 0 25",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 392,
      "chunk_index": 0
    }
  },
  {
    "text": "15\nZero or One: Optimal Digital Portfolios\nDigital assets are investments with returns that are binary in nature, i.e.,\nthey either have a very large or very small payoff. We explore the fea-\ntures of optimal portfolios of digital assets such as venture investments,\ncredit assets and lotteries. These portfolios comprise correlated assets\nwith joint Bernoulli distributions. Using a simple, standard, fast recur-\nsion technique to generate the return distribution of the portfolio, we",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 393,
      "chunk_index": 0
    }
  },
  {
    "text": "derive guidelines on how investors in digital assets may think about con-\nstructing their portfolios. We find that digital portfolios are better when\nthey are homogeneous in the size of the assets, but heterogeneous in the\nsuccess probabilities of the asset components.\nThe return distributions of digital portfolios are highly skewed and\nfat-tailed. A good example of such a portfolio is a venture fund. A sim-\nple representation of the payoff to a digital investment is Bernoulli with",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 393,
      "chunk_index": 1
    }
  },
  {
    "text": "a large payoff for a successful outcome and a very small (almost zero)\npayoff for a failed one. The probability of success of digital investments\n5 25\nis typically small, in the region of – % for new ventures (see Das, Ja-\n2003\ngannathan and Sarin ( )). Optimizing portfolios of such investments\nis therefore not amenable to standard techniques used for mean-variance\noptimization.\nIt is also not apparent that the intuitions obtained from the mean-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 393,
      "chunk_index": 2
    }
  },
  {
    "text": "variance setting carry over to portfolios of Bernoulli assets. For instance,\nit is interesting to ask, ceteris paribus, whether diversification by in-\ncreasing the number of assets in the digital portfolio is always a good\nthing. Since Bernoulli portfolios involve higher moments, how diversi-\nfication is achieved is by no means obvious. We may also ask whether\nit is preferable to include assets with as little correlation as possible",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 393,
      "chunk_index": 3
    }
  },
  {
    "text": "or is there a sweet spot for the optimal correlation levels of the assets?\nShould all the investments be of even size, or is it preferable to take a",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 393,
      "chunk_index": 4
    }
  },
  {
    "text": "394 data science: theories, models, algorithms, and analytics\nfew large bets and several small ones? And finally, is a mixed portfolio\nof safe and risky assets preferred to one where the probability of success\nis more uniform across assets? These are all questions that are of interest\nto investors in digital type portfolios, such as CDO investors, venture\ncapitalists and investors in venture funds.\nWe will use a method that is based on standard recursion for model-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 394,
      "chunk_index": 0
    }
  },
  {
    "text": "ing of the exact return distribution of a Bernoulli portfolio. This method\non which we build was first developed by Andersen, Sidenius and Basu\n2003\n( ) for generating loss distributions of credit portfolios. We then ex-\namine the properties of these portfolios in a stochastic dominance frame-\nwork framework to provide guidelines to digital investors. These guide-\nlines are found to be consistent with prescriptions from expected utility\noptimization. The prescriptions are as follows:\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 394,
      "chunk_index": 1
    }
  },
  {
    "text": "optimization. The prescriptions are as follows:\n1\n. Holding all else the same, more digital investments are preferred,\nmeaning for example, that a venture portfolio should seek to maxi-\nmize market share.\n2\n. As with mean-variance portfolios, lower asset correlation is better, un-\nless the digital investor’s payoff depends on the upper tail of returns.\n3\n. A strategy of a few large bets and many small ones is inferior to one\nwith bets being roughly the same size.\n4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 394,
      "chunk_index": 2
    }
  },
  {
    "text": "with bets being roughly the same size.\n4\n. And finally, a mixed portfolio of low-success and high-success assets\nis better than one with all assets of the same average success probabil-\nity level.\n151 154\nSection . explains the methodology used. Section . presents the\n155\nresults. Conclusions and further discussion are in Section . .\n15.1 Modeling Digital Portfolios\nAssume that the investor has a choice of n investments in digital assets",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 394,
      "chunk_index": 3
    }
  },
  {
    "text": "(e.g., start-up firms). The investments are indexed i = 1,2,...,n. Each in-\nvestment has a probability of success that is denoted q , and if successful,\ni\nthe payoff returned is S dollars. With probability (1 q ), the invest-\ni i\n−\nment will not work out, the start-up will fail, and the money will be lost\nin totality. Therefore, the payoff (cashflow) is\n(cid:40)\nS with prob q\nPayoff = C = i i ( 15 . 1 )\ni\n0 with prob (1 q )\ni\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 394,
      "chunk_index": 4
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 395\nThe specification of the investment as a Bernoulli trial is a simple rep-\nresentation of reality in the case of digital portfolios. This mimics well\nfor example, the case of the venture capital business. Two generaliza-\ntions might be envisaged. First, we might extend the model to allowing\nS to be random, i.e., drawn from a range of values. This will complicate\ni\nthe mathematics, but not add much in terms of enriching the model’s",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 395,
      "chunk_index": 0
    }
  },
  {
    "text": "results. Second, the failure payoff might be non-zero, say an amount a .\ni\nThen we have a pair of Bernoulli payoffs S ,a . Note that we can de-\ni i\n{ }\ncompose these investment payoffs into a project with constant payoff a\ni\nplus another project with payoffs S a ,0 , the latter being exactly the\ni i\n{ − }\noriginal setting where the failure payoff is zero. Hence, the version of the\nmodel we solve in this note, with zero failure payoffs, is without loss of\ngenerality.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 395,
      "chunk_index": 1
    }
  },
  {
    "text": "generality.\nUnlike stock portfolios where the choice set of assets is assumed to\nbe multivariate normal, digital asset investments have a joint Bernoulli\ndistribution. Portfolio returns of these investments are unlikely to be\nGaussian, and hence higher-order moments are likely to matter more.\nIn order to generate the return distribution for the portfolio of digital\nassets, we need to account for the correlations across digital investments.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 395,
      "chunk_index": 2
    }
  },
  {
    "text": "We adopt the following simple model of correlation. Define y to be the\ni\nperformance proxy for the i-th asset. This proxy variable will be simu-\nlated for comparison with a threshold level of performance to determine\nwhether the asset yielded a success or failure. It is defined by the follow-\ning function, widely used in the correlated default modeling literature,\n2003\nsee for example Andersen, Sidenius and Basu ( ):\n(cid:113)\ny = ρ X+ 1 ρ2 Z, i = 1...n ( 15 . 2 )\ni i − i i",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 395,
      "chunk_index": 3
    }
  },
  {
    "text": "y = ρ X+ 1 ρ2 Z, i = 1...n ( 15 . 2 )\ni i − i i\nwhere ρ [0,1] is a coefficient that correlates threshold y with a nor-\ni i\n∈\nmalized common factor X N(0,1). The common factor drives the\n∼\ncorrelations amongst the digital assets in the portfolio. We assume that\nZ N(0,1) and Corr(X,Z ) = 0, i. Hence, the correlation between\ni i\n∼ ∀\nassets i and j is given by ρ ρ . Note that the mean and variance of y\ni j i\n×\nare: E(y ) = 0,Var(y ) = 1, i. Conditional on X, the values of y are all\ni i i\n∀",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 395,
      "chunk_index": 4
    }
  },
  {
    "text": "i i i\n∀\nindependent, as Corr(Z,Z ) = 0.\ni j\nWe now formalize the probability model governing the success or\nfailure of the digital investment. We define a variable x , with distribu-\ni\ntion function F( ), such that F(x ) = q , the probability of success of the\ni i\n·\ndigital investment. Conditional on a fixed value of X, the probability of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 395,
      "chunk_index": 5
    }
  },
  {
    "text": "396 data science: theories, models, algorithms, and analytics\nsuccess of the i-th investment is defined as\npX Pr[y < x X] ( 15 . 3 )\ni ≡ i i |\nAssuming F to be the normal distribution function, we have\n(cid:20) (cid:113) (cid:21)\npX = Pr ρ X+ 1 ρ2 Z < x X\ni i − i i i |\n \nx ρ X\n= PrZ\ni\n<\n(cid:113)\ni − i X\n1\nρ2|\n− i\n(cid:34) (cid:35)\nF 1(q ) ρ X\n= Φ − i − i ( 15 . 4 )\n(cid:112)\n1 ρ\ni\n−\nwhere\nΦ(.)\nis the cumulative normal density function. Therefore, given",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 396,
      "chunk_index": 0
    }
  },
  {
    "text": "the level of the common factor X, asset correlation ρ, and the uncondi-\ntional success probabilities q , we obtain the conditional success prob-\ni\nability for each asset pX. As X varies, so does pX. For the numerical\ni i\nexamples here we choose the function F(x ) to the cumulative normal\ni\nprobability function.\nWe use a fast technique for building up distributions for sums of\nBernoulli random variables. In finance, this recursion technique was in-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 396,
      "chunk_index": 1
    }
  },
  {
    "text": "troduced in the credit portfolio modeling literature by Andersen, Side-\n2003\nnius and Basu ( ).\nWe deem an investment in a digital asset as successful if it achieves\nits high payoff S . The cashflow from the portfolio is a random variable\ni\nC = ∑n C. The maximum cashflow that may be generated by the\ni=1 i\nportfolio will be the sum of all digital asset cashflows, because each and\nevery outcome was a success, i.e.,\nn\nC = ∑ S ( 15 . 5 )\nmax i\ni=1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 396,
      "chunk_index": 2
    }
  },
  {
    "text": "n\nC = ∑ S ( 15 . 5 )\nmax i\ni=1\nTo keep matters simple, we assume that each S is an integer, and that\ni\nwe round off the amounts to the nearest significant digit. So, if the\nsmallest unit we care about is a million dollars, then each S will be in\ni\nunits of integer millions.\nRecall that, conditional on a value of X, the probability of success of\ndigital asset i is given as pX. The recursion technique will allow us to\ni\ngenerate the portfolio cashflow probability distribution for each level of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 396,
      "chunk_index": 3
    }
  },
  {
    "text": "X. We will then simply compose these conditional (on X) distributions\nusing the marginal distribution for X, denoted g(X), into the uncon-\nditional distribution for the entire portfolio. Therefore, we define the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 396,
      "chunk_index": 4
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 397\nprobability of total cashflow from the portfolio, conditional on X, to be\nf(C X). Then, the unconditional cashflow distribution of the portfolio\n|\nbecomes\n(cid:90)\nf(C) = f(C X) g(X) dX ( 15 . 6 )\nX | ·\nThe distribution f(C X) is easily computed numerically as follows.\n|\nWe index the assets with i = 1...n. The cashflow from all assets\ntaken together will range from zero to C . Suppose this range is bro-\nmax",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 397,
      "chunk_index": 0
    }
  },
  {
    "text": "max\nken into integer buckets, resulting in N buckets in total, each one con-\nB\ntaining an increasing level of total cashflow. We index these buckets by\nj = 1...N , with the cashflow in each bucket equal to B . B represents\nB j j\nthe total cashflow from all assets (some pay off and some do not), and\nthe buckets comprise the discrete support for the entire distribution of\n10\ntotal cashflow from the portfolio. For example, suppose we had as-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 397,
      "chunk_index": 1
    }
  },
  {
    "text": "sets, each with a payoff of C = 3. Then C = 30. A plausible set of\ni max\nbuckets comprising the support of the cashflow distribution would be:\n0,3,6,9,12,15,18,21,24,27,C .\nmax\n{ }\nDefine P(k,B ) as the probability of bucket j’s cashflow level B if we\nj j\naccount for the first k assets. For example, if we had just 3 assets, with\n132 7\npayoffs of value , , respectively, then we would have buckets, i.e.\nB = 0,1,2,3,4,5,6 . After accounting for the first asset, the only\nj\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 397,
      "chunk_index": 2
    }
  },
  {
    "text": "j\n{ }\npossible buckets with positive probability would be B = 0,1, and af-\nj\nter the first two assets, the buckets with positive probability would be\nB = 0,1,3,4. We begin with the first asset, then the second and so on,\nj\nand compute the probability of seeing the returns in each bucket. Each\nprobability is given by the following recursion:\nP(k+1,B ) = P(k,B ) [1 pX ]+P(k,B S ) pX , k = 1,...,n 1.\nj j − k+1 j − k+1 k+1 −\n157\n( . )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 397,
      "chunk_index": 3
    }
  },
  {
    "text": "j j − k+1 j − k+1 k+1 −\n157\n( . )\nThus the probability of a total cashflow of B after considering the first\nj\n(k +1) firms is equal to the sum of two probability terms. First, the\nprobability of the same cashflow B from the first k firms, given that\nj\nfirm (k+1) did not succeed. Second, the probability of a cashflow of\nB S from the first k firms and the (k+1)-st firm does succeed.\nj k+1\n−\nWe start off this recursion from the first asset, after which the N\nB",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 397,
      "chunk_index": 4
    }
  },
  {
    "text": "B\nbuckets are all of probability zero, except for the bucket with zero cash-\nflow (the first bucket) and the one with S cashflow, i.e.,\n1\nP(1,0) = 1 pX ( 15 . 8 )\n− 1\nP(1,S ) = pX ( 15 . 9 )\n1 1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 397,
      "chunk_index": 5
    }
  },
  {
    "text": "398 data science: theories, models, algorithms, and analytics\nAll the other buckets will have probability zero, i.e., P(1,B = 0,S ) =\nj 1\n(cid:54) { }\n0. With these starting values, we can run the system up from the first\nasset to the n-th one by repeated application of equation ( 15 . 7 ). Finally,\nwe will have the entire distribution P(n,B ), conditional on a given value\nj\nof X. We then compose all these distributions that are conditional on X\n156",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 398,
      "chunk_index": 0
    }
  },
  {
    "text": "156\ninto one single cashflow distribution using equation ( . ). This is done\nby numerically integrating over all values of X.\n15.2 Implementation in R\n15.2.1 Basic recursion\nGiven a set of outcomes and conditional (on state X) probabilities. we\ndevelop the recursion logic above in the following R function:\nasbrec = function(w,p) {\n#w: payoffs\n#p: probabilities\n#BASIC SET UP\nN = length(w)\nmaxloss = sum(w)\nbucket = c( 0 ,seq(maxloss))\nLP = matrix( 0 ,N,maxloss+ 1 ) #probability grid over losses",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 398,
      "chunk_index": 1
    }
  },
  {
    "text": "#DO FIRST FIRM\n1 1 1 1\nLP[ , ] = p[ ];\n−\n1 1 1 1\nLP[ ,w[ ]+ ] = p[ ];\n#LOOP OVER REMAINING FIRMS\nfor ( i in seq( 2 ,N)) {\nfor ( j in seq(maxloss+ 1 )) {\n1 1\nLP[i , j ] = LP[i ,j ]*( p[ i ])\n− −\nif (bucket[ j] w[ i ] >= 0 ) {\n−\n1\nLP[i , j ] = LP[i , j ] + LP[i ,j w[ i ]]*p[ i ]\n− −\n}\n}\n}\n#FINISH UP\nlossprobs = LP[N,]",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 398,
      "chunk_index": 2
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 399\nprint(t(LP))\nresult = matrix(c(bucket , lossprobs ) ,(maxloss+ 1 ), 2 )\n}\nWe use this function in the following example.\nw = c( 5 , 8 , 4 , 2 , 1 )\np = array( 1 /length(w) ,length(w))\nres = asbrec(w,p)\nprint(res)\nprint(sum(res [ , 2 ]))\nbarplot(res [ , 2 ] ,names.arg=res [ , 1 ] ,\nxlab=\"portfolio value\" ,ylab=\"probability\")\nThe output of this run is as follows:\n1 2 3 4 5 6\n[ , ] [ , ] [ , ] [ , ] [ , ] [ , ]\n1 0 0 8 0 64 0 512 0 4096 0 32768",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 399,
      "chunk_index": 0
    }
  },
  {
    "text": "1 0 0 8 0 64 0 512 0 4096 0 32768\n[ ,] . . . . .\n2 1 0 0 0 00 0 000 0 0000 0 08192\n[ ,] . . . . .\n3 2 0 0 0 00 0 000 0 1024 0 08192\n[ ,] . . . . .\n4 3 0 0 0 00 0 000 0 0000 0 02048\n[ ,] . . . . .\n5 4 0 0 0 00 0 128 0 1024 0 08192\n[ ,] . . . . .\n6 5 0 2 0 16 0 128 0 1024 0 10240\n[ ,] . . . . .\n7 6 0 0 0 00 0 000 0 0256 0 04096\n[ ,] . . . . .\n8 7 0 0 0 00 0 000 0 0256 0 02560\n[ ,] . . . . .\n9 8 0 0 0 16 0 128 0 1024 0 08704\n[ ,] . . . . .\n10 9 0 0 0 00 0 032 0 0256 0 04096\n[ ,] . . . . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 399,
      "chunk_index": 1
    }
  },
  {
    "text": "10 9 0 0 0 00 0 032 0 0256 0 04096\n[ ,] . . . . .\n11 10 0 0 0 00 0 000 0 0256 0 02560\n[ ,] . . . . .\n12 11 0 0 0 00 0 000 0 0064 0 01024\n[ ,] . . . . .\n13 12 0 0 0 00 0 032 0 0256 0 02176\n[ ,] . . . . .\n14 13 0 0 0 04 0 032 0 0256 0 02560\n[ ,] . . . . .\n15 14 0 0 0 00 0 000 0 0064 0 01024\n[ ,] . . . . .\n16 15 0 0 0 00 0 000 0 0064 0 00640\n[ ,] . . . . .\n17 16 0 0 0 00 0 000 0 0000 0 00128\n[ ,] . . . . .\n18 17 0 0 0 00 0 008 0 0064 0 00512\n[ ,] . . . . .\n19 18 0 0 0 00 0 000 0 0000 0 00128",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 399,
      "chunk_index": 2
    }
  },
  {
    "text": "19 18 0 0 0 00 0 000 0 0000 0 00128\n[ ,] . . . . .\n20 19 0 0 0 00 0 000 0 0016 0 00128\n[ ,] . . . . .\n21 20 0 0 0 00 0 000 0 0000 0 00032\n[ ,] . . . . .\nHere each column represents one pass through the recursion. Since there\nare five assets, we get five passes, and the final column is the result we\nare looking for. The plot of the outcome distribution is shown in Figure",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 399,
      "chunk_index": 3
    }
  },
  {
    "text": "400 data science: theories, models, algorithms, and analytics\n151\n. .\n0 1 2 3 4 5 6 7 8 9 11 13 15 17 19\nportfolio value\nytilibaborp\n03.0\n52.0\n02.0\n51.0\n01.0\n50.0\n00.0\nFigure15.1: Plotofthefinalout-\ncomedistributionforadigital\nportfoliowithfiveassetsofout-\ncomes 5,8,4,2,1 allofequal\n{ }\nprobability.\nWe can explore these recursion calculations in some detail as follows.\nNote that in our example p = 0.2,i = 1,2,3,4,5. We are interested\ni",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 400,
      "chunk_index": 0
    }
  },
  {
    "text": "i\nin computing P(k,B), where k denotes the k-th recursion pass, and B\ndenotes the return bucket. Recall that we have five assets with return\nlevels of 5,8,4,2,1 , respecitvely. After i = 1, we have\n{ }\nP(1,0) = (1 p ) = 0.8\n1\n−\nP(1,5) = p = 0.2\n1\nP(1,j) = 0,j = 0,5\n(cid:54) { }\nThe completes the first recursion pass and the values can be verified\n2 1\nfrom the R output above by examining column (column contains the\nvalues of the return buckets). We now move on the calculations needed",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 400,
      "chunk_index": 1
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 401\nfor the second pass in the recursion.\nP(2,0) = P(1,0)(1 p ) = 0.64\n2\n−\nP(2,5) = P(1,5)(1 p )+P(1,5 8)p = 0.2(0.8)+0(0.2) = 0.16\n2 2\n− −\nP(2,8) = P(1,8)(1 p )+P(1,8 8)p = 0(0.8)+0.8(0.2) = 0.16\n2 2\n− −\nP(2,13) = P(1,13)(1 p )+P(1,13 8)p = 0(0.8)+0.2(0.2) = 0.04\n2 2\n− −\nP(2,j) = 0,j = 0,5,8,13\n(cid:54) { }\nThe third recursion pass is as follows:\nP(3,0) = P(2,0)(1 p ) = 0.512\n3\n−\nP(3,4) = P(2,4)(1 p )+P(2,4 4) = 0(0.8)+0.64(0.2) = 0.128\n3\n− −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 401,
      "chunk_index": 0
    }
  },
  {
    "text": "3\n− −\nP(3,5) = P(2,5)(1 p )+P(2,5 4)p = 0.16(0.8)+0(0.2) = 0.128\n3 3\n− −\nP(3,8) = P(2,8)(1 p )+P(2,8 4)p = 0.16(0.8)+0(0.2) = 0.128\n3 3\n− −\nP(3,9) = P(2,9)(1 p )+P(2,9 4)p = 0(0.8)+0.16(0.2) = 0.032\n3 3\n− −\nP(3,12) = P(2,12)(1 p )+P(2,12 4)p = 0(0.8)+0.16(0.2) = 0.032\n3 3\n− −\nP(3,13) = P(2,13)(1 p )+P(2,13 4)p = 0.04(0.8)+0(0.2) = 0.032\n3 3\n− −\nP(3,17) = P(2,17)(1 p )+P(2,17 4)p = 0(0.8)+0.04(0.2) = 0.008\n3 3\n− −\nP(3,j) = 0,j = 0,4,5,8,9,12,13,17\n(cid:54) { }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 401,
      "chunk_index": 1
    }
  },
  {
    "text": "P(3,j) = 0,j = 0,4,5,8,9,12,13,17\n(cid:54) { }\nNote that the same computation work even when the outcomes are not\nof equal probability.\n15.2.2 Combining conditional distributions\nWe now demonstrate how we will integrate the conditional probability\ndistributions pX into an unconditional probability distribution of out-\n(cid:82)\ncomes, denoted p = pXg(X) dX, where g(X) is the density function\nX\nof the state variable X. We create a function to combine the conditional",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 401,
      "chunk_index": 2
    }
  },
  {
    "text": "distribution functions. This function calls the absrec function that we\nhad used earlier.\n#FUNCTION TO COMPUTE FULL RETURN DISTRIBUTION\n#INTEGRATES OVER X BY CALLING ASBREC\ndigiprob = function(L,q,rho) {\n0 1\ndx = .\nx = seq( 40 , 40 )*dx\n−\nfx = dnorm(x)*dx\nfx = fx/sum(fx)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 401,
      "chunk_index": 3
    }
  },
  {
    "text": "402 data science: theories, models, algorithms, and analytics\nmaxloss = sum(L)\nbucket = c( 0 ,seq(maxloss))\ntotp = array ( 0 ,(maxloss+ 1 ))\nfor ( i in seq(length(x))) {\np = pnorm((qnorm(q) rho*x[ i ]) / sqrt( 1 rho^ 2 ))\n− −\nldist = asbrec(L,p)\n2\ntotp = totp + ldist [ , ]*fx[ i ]\n}\nresult = matrix(c(bucket , totp ) ,(maxloss+ 1 ), 2 )\n}\nNote that now we will use the unconditional probabilities of success for\neach asset, and correlate them with a specified correlation level. We run",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 402,
      "chunk_index": 0
    }
  },
  {
    "text": "this with two correlation levels 0.5,+0.5 .\n{− }\n# INTEGRATE OVER CONDITIONAL DISTRIBUTIONS\n−−−−−− −−−−\nw = c( 5 , 8 , 4 , 2 , 1 )\nq = c( 0 . 1 , 0 . 2 , 0 . 1 , 0 . 05 , 0 . 15 )\n0 25\nrho = .\nres 1 = digiprob(w,q,rho)\n0 75\nrho = .\nres 2 = digiprob(w,q,rho)\npar(mfrow=c( 2 , 1 ))\nbarplot(res 1 [ , 2 ] ,names.arg=res 1 [ , 1 ] , xlab=\"portfolio value\" ,\n0 25\nylab=\"probability\" ,main=\"rho = . \")\nbarplot(res 2 [ , 2 ] ,names.arg=res 2 [ , 1 ] , xlab=\"portfolio value\" ,\n0 75",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 402,
      "chunk_index": 1
    }
  },
  {
    "text": "0 75\nylab=\"probability\" ,main=\"rho = . \")\nThe output plots of the unconditional outcome distribution are shown in\n152\nFigure . . We can see the data for the plots as follows.\n> cbind(res 1 , res 2 )\n1 2 3 4\n[ , ] [ , ] [ , ] [ , ]\n1 0 0 5391766174 0 0 666318464\n[ ,] . .\n2 1 0 0863707325 1 0 046624312\n[ ,] . .\n3 2 0 0246746918 2 0 007074104\n[ ,] . .\n4 3 0 0049966420 3 0 002885901\n[ ,] . .\n5 4 0 0534700675 4 0 022765422\n[ ,] . .\n6 5 0 0640540228 5 0 030785967\n[ ,] . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 402,
      "chunk_index": 2
    }
  },
  {
    "text": "[ ,] . .\n6 5 0 0640540228 5 0 030785967\n[ ,] . .\n7 6 0 0137226107 6 0 009556413\n[ ,] . .\n8 7 0 0039074039 7 0 002895774\n[ ,] . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 402,
      "chunk_index": 3
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 403\nrho = 0.25\n0 1 2 3 4 5 6 7 8 9 11 13 15 17 19\nportfolio value\nytilibaborp\n4.0\n2.0\n0.0\nrho = 0.75\n0 1 2 3 4 5 6 7 8 9 11 13 15 17 19\nportfolio value\nytilibaborp\n6.0\n4.0\n2.0\n0.0\nFigure15.2: Plotofthefinalout-\ncomedistributionforadigital\nportfoliowithfiveassetsofout-\ncomes 5,8,4,2,1 withuncon-\n{ }\nditionalprobabilityofsuccessof\n0.1,0.2,0.1,0.05,0.15 ,respecitvely.\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 403,
      "chunk_index": 0
    }
  },
  {
    "text": "404 data science: theories, models, algorithms, and analytics\n9 8 0 1247287209 8 0 081172499\n[ ,] . .\n10 9 0 0306776806 9 0 029154885\n[ ,] . .\n11 10 0 0086979993 10 0 008197488\n[ ,] . .\n12 11 0 0021989842 11 0 004841742\n[ ,] . .\n13 12 0 0152035638 12 0 014391319\n[ ,] . .\n14 13 0 0186144920 13 0 023667222\n[ ,] . .\n15 14 0 0046389439 14 0 012776165\n[ ,] . .\n16 15 0 0013978502 15 0 006233366\n[ ,] . .\n17 16 0 0003123473 16 0 004010559\n[ ,] . .\n18 17 0 0022521668 17 0 005706283\n[ ,] . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 404,
      "chunk_index": 0
    }
  },
  {
    "text": "18 17 0 0022521668 17 0 005706283\n[ ,] . .\n19 18 0 0006364672 18 0 010008267\n[ ,] . .\n20 19 0 0002001003 19 0 002144265\n[ ,] . .\n21 20 0 0000678949 20 0 008789582\n[ ,] . .\nThe left column of probabilities has correlation of ρ = 0.25 and the right\none is the case when ρ = 0.75. We see that the probabilities on the right\nare lower for low outcomes (except zero) and high for high outcomes.\nWhy? See the plot of the difference between the high correlation case\n153",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 404,
      "chunk_index": 1
    }
  },
  {
    "text": "153\nand low correlation case in Figure . .\n15.3 Stochastic Dominance (SD)\nSD is an ordering over probabilistic bundles. We may want to know if\none VC’s portfolio dominates another in a risk-adjusted sense. Differ-\nent SD concepts apply to answer this question. For example if portfo-\nlio A does better than portfolio B in every state of the world, it clearly\ndominates. This is called “state-by-state” dominance, and is hardly ever\nencountered. Hence, we briefly examine two more common types of SD.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 404,
      "chunk_index": 2
    }
  },
  {
    "text": "1\n. First-order Stochastic Dominance (FSD): For cumulative distribution\nfunction F(X) over states X, portfolio A dominates B if Prob(A k)\n≥ ≥\nProb(B k) for all states k X, and Prob(A k) > Prob(B k)\n≥ ∈ ≥ ≥\nfor some k. It is the same as Prob(A k) Prob(B k) for all states\n≤ ≤ ≤\nk X, and Prob(A k) < Prob(B k) for some k.This implies\n∈ ≤ ≤\nthat F (k) F (k). The mean outcome under A will be higher than\nA B\n≤\nunder B, and all increasing utility functions will give higher utility for",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 404,
      "chunk_index": 3
    }
  },
  {
    "text": "A. This is a weaker notion of dominance than state-wise, but also not\nas often encountered in practice.\n> x = seq( 4,4,0.1)\n> F_B = pno − rm(x,mean=0,sd=1);\n> F_A = pnorm(x,mean=0.25,sd=1);\n> F_A F_B #FSD exists\n[1] −2.098272e 05 3.147258e 05 4.673923e 05 6.872414e 05 1.000497e 04\n[6] −1.442118e −04 −2.058091e −04 −2.908086e −04 −4.068447e −04 −5.635454e −04\n− − − − − − − − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 404,
      "chunk_index": 4
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 405\n0 1 2 3 4 5 6 7 8 9 11 13 15 17 19\nborP\nni\nffiD\n01.0\n50.0\n00.0\nFigure15.3: Plotofthedifferencein\ndistributionforadigitalportfolio\nwithfiveassetswhenρ = 0.75\nminusthatwhenρ = 0.25. Weuse\noutcomes 5,8,4,2,1 withuncon-\n{ }\nditionalprobabilityofsuccessof\n0.1,0.2,0.1,0.05,0.15 ,respecitvely.\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 405,
      "chunk_index": 0
    }
  },
  {
    "text": "406 data science: theories, models, algorithms, and analytics\n[11] 7.728730e 04 1.049461e 03 1.410923e 03 1.878104e 03 2.475227e 03\n[16] −3.229902e −03 −4.172947e −03 −5.337964e −03 −6.760637e −03 −8.477715e −03\n[21] −1.052566e −02 −1.293895e −02 −1.574810e −02 −1.897740e −02 −2.264252e −02\n[26] −2.674804e −02 −3.128519e −02 −3.622973e −02 −4.154041e −02 −4.715807e −02\n[31] −5.300548e −02 −5.898819e −02 −6.499634e −02 −7.090753e −02 −7.659057e −02",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 0
    }
  },
  {
    "text": "[36] −8.191019e −02 −8.673215e −02 −9.092889e −02 −9.438507e −02 −9.700281e −02\n[41] −9.870633e −02 −9.944553e −02 −9.919852e −02 −9.797262e −02 −9.580405e −02\n[46] −9.275614e −02 −8.891623e −02 −8.439157e −02 −7.930429e −02 −7.378599e −02\n[51] −6.797210e −02 −6.199648e −02 −5.598646e −02 −5.005857e −02 −4.431528e −02\n[56] −3.884257e −02 −3.370870e −02 −2.896380e −02 −2.464044e −02 −2.075491e −02\n[61] −1.730902e −02 −1.429235e −02 −1.168461e −02 −9.458105e −03 −7.580071e −03",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 1
    }
  },
  {
    "text": "[66] −6.014807e −03 −4.725518e −03 −3.675837e −03 −2.831016e −03 −2.158775e −03\n[71] −1.629865e −03 −1.218358e −03 −9.017317e −04 −6.607827e −04 −4.794230e −04\n[76] −3.443960e −04 −2.449492e −04 −1.724935e −04 −1.202675e −04 −8.302381e −05\n[81] −5.674604e −05 − − − − − − − −\n− −\n2\n. Second-order Stochastic Dominance (SSD): Here the portfolios have\nthe same mean but the risk is less for portfolio A. Then we say that\nportfolio A has a “mean-preserving spread” over portfolio B. Techni-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 2
    }
  },
  {
    "text": "(cid:82)k (cid:82)\ncally this is the same as [F (k) F (k)] dX < 0, and XdF (X) =\n(cid:82) XdF (X). Mean-varian − c ∞ e m A odel − s in B which portfolios on X the e A ffi-\nX B\ncient frontier dominate those below are a special case of SSD. See the\nexample below, there is no FSD, but there is SSD.\n> x = seq( 4,4,0.1)\n> F_B = pno − rm(x,mean=0,sd=2);\n> F_A = pnorm(x,mean=0,sd=1);\n> F_A F_B #No FSD\n[1] −0.02271846 0.02553996 0.02864421 0.03204898 0.03577121 0.03982653",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 3
    }
  },
  {
    "text": "[7] −0.04422853 −0.04898804 −0.05411215 −0.05960315 −0.06545730 −0.07166345\n[13] −0.07820153 −0.08504102 −0.09213930 −0.09944011 −0.10687213 −0.11434783\n[19] −0.12176261 −0.12899464 −0.13590512 −0.14233957 −0.14812981 −0.15309708\n[25] −0.15705611 −0.15982015 −0.16120699 −0.16104563 −0.15918345 −0.15549363\n[31] −0.14988228 −0.14229509 −0.13272286 −0.12120570 −0.10783546 −0.09275614\n[37] −0.07616203 −0.05829373 −0.03943187 −0.01988903 −0.00000000 −0.01988903",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 4
    }
  },
  {
    "text": "[43] −0.03943187 −0.05829373 −0.07616203 −0.09275614 0.10783546 0.12120570\n[49] 0.13272286 0.14229509 0.14988228 0.15549363 0.15918345 0.16104563\n[55] 0.16120699 0.15982015 0.15705611 0.15309708 0.14812981 0.14233957\n[61] 0.13590512 0.12899464 0.12176261 0.11434783 0.10687213 0.09944011\n[67] 0.09213930 0.08504102 0.07820153 0.07166345 0.06545730 0.05960315\n[73] 0.05411215 0.04898804 0.04422853 0.03982653 0.03577121 0.03204898\n[79] 0.02864421 0.02553996 0.02271846",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 5
    }
  },
  {
    "text": "[79] 0.02864421 0.02553996 0.02271846\n> cumsum(F_A F_B) #But there is SSD\n[1] 2.2718−46e 02 4.825842e 02 7.690264e 02 1.089516e 01 1.447228e 01\n[6] −1.845493e −01 −2.287779e −01 −2.777659e −01 −3.318781e −01 −3.914812e −01\n[11] −4.569385e −01 −5.286020e −01 −6.068035e −01 −6.918445e −01 −7.839838e −01\n[16] −8.834239e −01 −9.902961e −01 −1.104644e − +00 −1.226407e − +00 −1.355401e − +00\n[21] −1.491306e − +00 −1.633646e − +00 −1.781776e+00 −1.934873e+00 −2.091929e+00",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 6
    }
  },
  {
    "text": "[26] −2.251749e+00 −2.412956e+00 −2.574002e+00 −2.733185e+00 −2.888679e+00\n[31] −3.038561e+00 −3.180856e+00 −3.313579e+00 −3.434785e+00 −3.542620e+00\n[36] −3.635376e+00 −3.711538e+00 −3.769832e+00 −3.809264e+00 −3.829153e+00\n[41] −3.829153e+00 −3.809264e+00 −3.769832e+00 −3.711538e+00 −3.635376e+00\n[46] −3.542620e+00 −3.434785e+00 −3.313579e+00 −3.180856e+00 −3.038561e+00\n[51] −2.888679e+00 −2.733185e+00 −2.574002e+00 −2.412956e+00 −2.251749e+00",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 7
    }
  },
  {
    "text": "[56] −2.091929e+00 −1.934873e+00 −1.781776e+00 −1.633646e+00 −1.491306e+00\n[61] −1.355401e+00 −1.226407e+00 −1.104644e+00 −9.902961e 01 −8.834239e 01\n[66] −7.839838e 01 −6.918445e 01 −6.068035e 01 −5.286020e −01 −4.569385e −01\n[71] −3.914812e −01 −3.318781e −01 −2.777659e −01 −2.287779e −01 −1.845493e −01\n[76] −1.447228e −01 −1.089516e −01 −7.690264e −02 −4.825842e −02 −2.271846e −02\n− − − − − − − − − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 406,
      "chunk_index": 8
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 407\n[81] 2.220446e 16\n− −\n15.4 Portfolio Characteristics\nArmed with this established machinery, there are several questions an\ninvestor (e.g. a VC) in a digital portfolio may pose. First, is there an op-\ntimal number of assets, i.e., ceteris paribus, are more assets better than\nfewer assets, assuming no span of control issues? Second, are Bernoulli\nportfolios different from mean-variances ones, in that is it always better",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 407,
      "chunk_index": 0
    }
  },
  {
    "text": "to have less asset correlation than more correlation? Third, is it better\nto have an even weighting of investment across the assets or might it\nbe better to take a few large bets amongst many smaller ones? Fourth,\nis a high dispersion of probability of success better than a low disper-\nsion? These questions are very different from the ones facing investors\nin traditional mean-variance portfolios. We shall examine each of these\nquestions in turn.\n15.4.1 How many assets?",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 407,
      "chunk_index": 1
    }
  },
  {
    "text": "questions in turn.\n15.4.1 How many assets?\nWith mean-variance portfolios, keeping the mean return of the portfolio\nfixed, more securities in the portfolio is better, because diversification re-\nduces the variance of the portfolio. Also, with mean-variance portfolios,\nhigher-order moments do not matter. But with portfolios of Bernoulli\nassets, increasing the number of assets might exacerbate higher-order\nmoments, even though it will reduce variance. Therefore it may not be",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 407,
      "chunk_index": 2
    }
  },
  {
    "text": "worthwhile to increase the number of assets (n) beyond a point.\nIn order to assess this issue we conducted the following experiment.\nWe invested in n assets each with payoff of 1/n. Hence, if all assets suc-\n1\nceed, the total (normalized) payoff is . This normalization is only to\nmake the results comparable across different n, and is without loss of\ngenerality. We also assumed that the correlation parameter is ρ = 0.25,\ni",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 407,
      "chunk_index": 3
    }
  },
  {
    "text": "i\nfor all i. To make it easy to interpret the results, we assumed each asset\nto be identical with a success probability of q = 0.05 for all i. Using\ni\nthe recursion technique, we computed the probability distribution of the\nportfolio payoff for four values of n = 25,50,75,100 . The distribution\n{ }\n154 4\nfunction is plotted in Figure . , left panel. There are plots, one for\neach n, and if we look at the bottom left of the plot, the leftmost line is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 407,
      "chunk_index": 4
    }
  },
  {
    "text": "for n = 100. The next line to the right is for n = 75, and so on.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 407,
      "chunk_index": 5
    }
  },
  {
    "text": "408 data science: theories, models, algorithms, and analytics\nOne approach to determining if greater n is better for a digital portfo-\nlio is to investigate if a portfolio of n assets stochastically dominates one\nwith less than n assets. On examination of the shapes of the distribution\nfunctions for different n, we see that it is likely that as n increases, we\nobtain portfolios that exhibit second-order stochastic dominance (SSD)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 408,
      "chunk_index": 0
    }
  },
  {
    "text": "over portfolios with smaller n. The return distribution when n = 100\n(denoted G ) would dominate that for n = 25 (denoted G ) in the SSD\n100 25\n(cid:82) (cid:82) (cid:82)u\nsense, if x dG (x) = x dG (x), and [G (x) G (x)] dx 0\nx 100 x 25 0 100 − 25 ≤\nfor all u (0,1). That is, G has a mean-preserving spread over G ,\n25 100\n∈\nor G has the same mean as G but lower variance, i.e., implies su-\n100 25\nperior mean-variance efficiency. To show this we plotted the integral\n(cid:82)u",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 408,
      "chunk_index": 1
    }
  },
  {
    "text": "(cid:82)u\n[G (x) G (x)] dx and checked the SSD condition. We found that\n0 100 − 25\n154\nthis condition is satisfied (see Figure . ). As is known, SSD implies\nmean-variance efficiency as well.\n0.0 0.2 0.4 0.6 0.8 1.0\n0.1\n9.0\n8.0\n7.0\n6.0\n5.0\n4.0\nNormalized Total Payoff\nytilibaborP\nevitalumuC\n0.0 0.2 0.4 0.6 0.8 1.0\n4.0-\n6.0-\n8.0-\n0.1-\n2.1-\nNormalized total payoff\n)52G(F\nsunim\n)001G(F\ndetargetnI\nFigure15.4: Distributionfunc-\ntionsforreturnsfromBernoulli\ninvestmentsasthenumberofin-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 408,
      "chunk_index": 2
    }
  },
  {
    "text": "investmentsasthenumberofin-\nvestments(n)increases. Usingthe\nrecursiontechniquewecomputed\ntheprobabilitydistributionofthe\nportfoliopayoffforfourvaluesof\nn = 25,50,75,100 . Thedistri-\n{ }\nbutionfunctionisplottedinthe\nleftpanel. Thereare4plots,one\nforeachn,andifwelookatthe\nbottomleftoftheplot,theleftmost\nlineisforn = 100. Thenextline\ntotherightisforn = 75,andso\non. Therightpanelplotsthevalue\nof\n(cid:82)u[G\n(x) G (x)] dxforall\n0 100 − 25\nu (0,1),andconfirmsthatitis\n∈\nalwaysnegative. Thecorrelation",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 408,
      "chunk_index": 3
    }
  },
  {
    "text": "∈\nalwaysnegative. Thecorrelation\nparameterisρ=0.25.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 408,
      "chunk_index": 4
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 409\nWe also examine if higher n portfolios are better for a power utility\ninvestor with utility function, U(C) =\n(0.1+C)1\n−\nγ\n, where C is the normal-\n1 γ\n−\nized total payoff of the Bernoulli portfolio. Expected utility is given by\n∑ U(C) f(C). We set the risk aversion coefficient to γ = 3 which is in\nC\n151\nthe standard range in the asset-pricing literature. Table . reports the\nresults. We can see that the expected utility increases monotonically with",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 409,
      "chunk_index": 0
    }
  },
  {
    "text": "n. Hence, for a power utility investor, having more assets is better than\nless, keeping the mean return of the portfolio constant. Economically,\nin the specific case of VCs, this highlights the goal of trying to capture\na larger share of the number of available ventures. The results from the\nSSD analysis are consistent with those of expected power utility.\nTable15.1: Expectedutilityfor\nn E(C) Pr[C >0.03] Pr[C >0.07] Pr[C >0.10] Pr[C >0.15] E[U(C)] Bernoulliportfoliosasthenumber",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 409,
      "chunk_index": 1
    }
  },
  {
    "text": "25 0.05 0.665 0.342 0.150 0.059 29.259 ofinvestments(n)increases. The\n50 0.05 0.633 0.259 0.084 0.024 − 26.755 tablereportstheportfoliostatistics\n− forn = 25,50,75,100 . Expected\n75 0.05 0.620 0.223 0.096 0.015 25.876 { }\n− utilityisgiveninthelastcolumn.\n100 0.05 0.612 0.202 0.073 0.011 25.433 Thecorrelationparameterisρ =\n−\n0.25. TheutilityfunctionisU(C)=\nWe have abstracted away from issues of the span of management by (0.1+C)1 − γ/(1 γ),γ=3.\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 409,
      "chunk_index": 2
    }
  },
  {
    "text": "−\ninvestors. Given that investors actively play a role in their invested assets\nin digital portfolios, increasing n beyond a point may of course become\n2003\ncostly, as modeled in Kanniainen and Keuschnigg ( ).\n15.4.2 The impact of correlation\nAs with mean-variance portfolios, we expect that increases in payoff cor-\nrelation for Bernoulli assets will adversely impact portfolios. In order to\nverify this intuition we analyzed portfolios keeping all other variables",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 409,
      "chunk_index": 3
    }
  },
  {
    "text": "the same, but changing correlation. In the previous subsection, we set\nthe parameter for correlation to be ρ = 0.25. Here, we examine four\nlevels of the correlation parameter: ρ = 0.09,0.25,0.49,0.81 . For each\n{ }\nlevel of correlation, we computed the normalized total payoff distribu-\ntion. The number of assets is kept fixed at n = 25 and the probability of\nsuccess of each digital asset is 0.05 as before.\n155\nThe results are shown in Figure . where the probability distribution",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 409,
      "chunk_index": 4
    }
  },
  {
    "text": "function of payoffs is shown for all four correlation levels. We find that\nthe SSD condition is met, i.e., that lower correlation portfolios stochasti-\ncally dominate (in the SSD sense) higher correlation portfolios. We also\nexamined changing correlation in the context of a power utility investor",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 409,
      "chunk_index": 5
    }
  },
  {
    "text": "410 data science: theories, models, algorithms, and analytics\nwith the same utility function as in the previous subsection. The results\n152\nare shown in Table . . We confirm that, as with mean-variance portfo-\nlios, Bernoulli portfolios also improve if the assets have low correlation.\nHence, digital investors should also optimally attempt to diversify their\nportfolios. Insurance companies are a good example—they diversify risk\nacross geographical and other demographic divisions.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 410,
      "chunk_index": 0
    }
  },
  {
    "text": "0.0 0.2 0.4 0.6 0.8 1.0\n0.1\n9.0\n8.0\n7.0\n6.0\n5.0\n4.0\n3.0\nNormalized Total Payoff\nytilibaborP\nevitalumuC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n1.0-\n2.0-\n3.0-\n4.0-\n5.0-\n6.0-\nNormalized total payoff\n)]18.0=ohr[G(F\nsunim\n)]90.0=ohr[G(F\ndetargetnI\nFigure15.5: Distributionfunc-\ntionsforreturnsfromBernoulli\ninvestmentsasthecorrelationpa-\nrameter(ρ2)increases. Usingthe\nrecursiontechniquewecomputed\ntheprobabilitydistributionofthe\nportfoliopayoffforfourvaluesof\nρ = 0.09,0.25,0.49,0.81 shown\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 410,
      "chunk_index": 1
    }
  },
  {
    "text": "ρ = 0.09,0.25,0.49,0.81 shown\n{ }\nbytheblack,red,greenandblue\nlinesrespectively. Thedistribution\nfunctionisplottedintheleftpanel.\nTherightpanelplotsthevalueof\n(cid:82) 0 u[G ρ=0.09 (x) − G ρ=0.81 (x)] dxfor\nallu (0,1),andconfirmsthatitis\n∈\nalwaysnegative.\ng˘a˘\n15.4.3 Uneven bets?\nDigital asset investors are often faced with the question of whether to\nbet even amounts across digital investments, or to invest with different\nweights. We explore this question by considering two types of Bernoulli",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 410,
      "chunk_index": 2
    }
  },
  {
    "text": "portfolios. Both have n = 25 assets within them, each with a success",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 410,
      "chunk_index": 3
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 411\nTable15.2: Expectedutilityfor\nρ E(C) Pr[C >0.03] Pr[C >0.07] Pr[C >0.10] Pr[C >0.15] E[U(C)] Bernoulliportfoliosasthecor-\n0.32 0.05 0.715 0.356 0.131 0.038 28.112 relation(ρ)increases. Thetable\n0.52 0.05 0.665 0.342 0.150 0.059 − 29.259 reportstheportfoliostatisticsfor\n− ρ= 0.09,0.25,0.49,0.81 . Expected\n0.72 0.05 0.531 0.294 0.170 0.100 32.668 { }\n− utilityisgiveninthelastcolumn.\n0.92 0.05 0.283 0.186 0.139 0.110 39.758 TheutilityfunctionisU(C) =\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 411,
      "chunk_index": 0
    }
  },
  {
    "text": "−\n(0.1+C)1\n−\nγ/(1 γ),γ=3.\n−\nprobability of q = 0.05. The first has equal payoffs, i.e., 1/25 each. The\ni\nsecond portfolio has payoffs that monotonically increase, i.e., the payoffs\nare equal to j/325,j = 1,2,...,25. We note that the sum of the payoffs\n1 153\nin both cases is . Table . shows the utility of the investor, where the\nutility function is the same as in the previous sections. We see that the\nutility for the balanced portfolio is higher than that for the imbalanced",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 411,
      "chunk_index": 1
    }
  },
  {
    "text": "one. Also the balanced portfolio evidences SSD over the imbalanced\nportfolio. However, the return distribution has fatter tails when the port-\nfolio investments are imbalanced. Hence, investors seeking to distin-\nguish themselves by taking on greater risk in their early careers may be\nbetter off with imbalanced portfolios.\nTable15.3: Expectedutilityfor\nE(C) ProbabilitythatC>x Bernoulliportfolioswhenthe\nWts E[U(C)] x=0.01 x=0.02 x=0.03 x=0.07 x=0.10 x=0.15 x=0.25\nportfoliocomprisesbalancedin-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 411,
      "chunk_index": 2
    }
  },
  {
    "text": "portfoliocomprisesbalancedin-\nBalanced 0.05 0.490 0.490 0.490 0.278 0.169 0.107 0.031\nvestinginassetsversusimbalanced\n33.782\n− weights. Boththebalancedand\nImbalanced 0.05 0.464 0.437 0.408 0.257 0.176 0.103 0.037 imbalancedportfoliohaven = 25\n34.494 assetswithinthem,eachwitha\n−\nsuccessprobabilityofq = 0.05.\ni\nThefirsthasequalpayoffs,i.e.\n15.4.4 Mixing safe and risky assets 1/25each. Thesecondportfolio\nhaspayoffsthatmonotonicallyin-\ncrease,i.e. thepayoffsareequalto",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 411,
      "chunk_index": 3
    }
  },
  {
    "text": "crease,i.e. thepayoffsareequalto\nIs it better to have assets with a wide variation in probability of success\nj/325,j =1,2,...,25. Wenotethat\nor with similar probabilities? To examine this, we look at two portfolios\nthesumofthepayoffsinbothcases\nof n = 26 assets. In the first portfolio, all the assets have a probability of is1. Thecorrelationparameteris\nsuccess equal to q = 0.10. In the second portfolio, half the firms have a ρ = 0.55. Theutilityfunctionis\ni U(C) = (0.1+C)1\n−\nγ/(1 γ),γ =",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 411,
      "chunk_index": 4
    }
  },
  {
    "text": "i U(C) = (0.1+C)1\n−\nγ/(1 γ),γ =\nsuccess probability of 0.05 and the other half have a probability of 0.15. −\n3.\nThe payoff of all investments is 1/26. The probability distribution of\npayoffs and the expected utility for the same power utility investor (with\nγ = 3) are given in Table 15 . 4 . We see that mixing the portfolio between\ninvestments with high and low probability of success results in higher\nexpected utility than keeping the investments similar. We also confirmed",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 411,
      "chunk_index": 5
    }
  },
  {
    "text": "that such imbalanced success probability portfolios also evidence SSD\nover portfolios with similar investments in terms of success rates. This",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 411,
      "chunk_index": 6
    }
  },
  {
    "text": "412 data science: theories, models, algorithms, and analytics\nresult does not have a natural analog in the mean-variance world with\nnon-digital assets. For empirical evidence on the efficacy of various di-\n2006\nversification approaches, see Lossen ( ).\nTable15.4: Expectedutilityfor\nE(C) ProbabilitythatC>x Bernoulliportfolioswhentheport-\nWts E[U(C)] x=0.01 x=0.02 x=0.03 x=0.07 x=0.10 x=0.15 x=0.25\nfoliocomprisesbalancedinvesting\nUniform 0.10 0.701 0.701 0.701 0.502 0.366 0.270 0.111",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 412,
      "chunk_index": 0
    }
  },
  {
    "text": "inassetswithidenticalsuccess\n24.625\n− probabilitiesversusinvestingin\nMixed 0.10 0.721 0.721 0.721 0.519 0.376 0.273 0.106 assetswithmixedsuccessprobabil-\n23.945 ities. Boththeuniformandmixed\n−\nportfolioshaven=26assetswithin\nthem. Inthefirstportfolio,allthe\nassetshaveaprobabilityofsuccess\n15.5 Conclusions\nequaltoq = 0.10. Inthesecond\ni\nportfolio,halfthefirmshavea\nDigital asset portfolios are different from mean-variance ones because successprobabilityof0.05andthe\notherhalfhaveaprobabilityof",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 412,
      "chunk_index": 1
    }
  },
  {
    "text": "otherhalfhaveaprobabilityof\nthe asset returns are Bernoulli with small success probabilities. We\n0.15. Thepayoffofallinvestments\nused a recursion technique borrowed from the credit portfolio litera-\nis1/26. Thecorrelationparameter\nture to construct the payoff distributions for Bernoulli portfolios. We find isρ = 0.55. Theutilityfunctionis\nthat many intuitions for these portfolios are similar to those of mean-\nU(C) = (0.1+C)1\n−\nγ/(1\n−\nγ),γ =\n3.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 412,
      "chunk_index": 2
    }
  },
  {
    "text": "U(C) = (0.1+C)1\n−\nγ/(1\n−\nγ),γ =\n3.\nvariance ones: diversification by adding assets is useful, low correlations\namongst investments is good. However, we also find that uniform bet\nsize is preferred to some small and some large bets. Rather than con-\nstruct portfolios with assets having uniform success probabilities, it is\npreferable to have some assets with low success rates and others with\nhigh success probabilities, a feature that is noticed in the case of venture",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 412,
      "chunk_index": 3
    }
  },
  {
    "text": "funds. These insights augment the standard understanding obtained\nfrom mean-variance portfolio optimization.\nThe approach taken here is simple to use. The only inputs needed are\nthe expected payoffs of the assets C, success probabilities q , and the av-\ni i\nerage correlation between assets, given by a parameter ρ. Broad statistics\non these inputs are available, say for venture investments, from papers\n2003\nsuch as Das, Jagannathan and Sarin ( ). Therefore, using data, it is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 412,
      "chunk_index": 4
    }
  },
  {
    "text": "easy to optimize the portfolio of a digital asset fund. The technical ap-\nproach here is also easily extended to features including cost of effort by\ninvestors as the number of projects grows (Kanniainen and Keuschnigg\n2003\n( )), syndication, etc. The number of portfolios with digital assets ap-\npears to be increasing in the marketplace, and the results of this analysis\nprovide important intuition for asset managers.\n2\nThe approach in Section is just one way in which to model joint suc-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 412,
      "chunk_index": 5
    }
  },
  {
    "text": "cess probabilities using a common factor. Undeniably, there are other",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 412,
      "chunk_index": 6
    }
  },
  {
    "text": "zero or one: optimal digital portfolios 413\nways too, such as modeling joint probabilities directly, making sure that\nthey are consistent with each other, which itself may be mathematically\ntricky. It is indeed possible to envisage that, for some different system\nof joint success probabilities, the qualitative nature of the results may\ndiffer from the ones developed here. It is also possible that the system\nwe adopt here with a single common factor X may be extended to more",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 413,
      "chunk_index": 0
    }
  },
  {
    "text": "than one common factor, an approach often taken in the default litera-\nture.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 413,
      "chunk_index": 1
    }
  },
  {
    "text": "16\nAgainst the Odds: Mathematics of Gambling\n16.1 Introduction\nMost people hate mathematics but love gambling. Which of course, is\nstrange because gambling is driven mostly by math. Think of any type\nof gambling and no doubt there will be maths involved: Horse-track\nbetting, sports betting, blackjack, poker, roulette, stocks, etc.\n16.1.1 Odds\n4\nOddly, bets are defined by their odds. If a bet on a horse is quoted at -\n1 4\nto- odds, it means that if you win, you receive times your wager plus\n1 5",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 415,
      "chunk_index": 0
    }
  },
  {
    "text": "1 5\nthe amount wagered. That is, if you bet $ , you get back $ .\nThe odds effectively define the probability of winning. Lets define this\nto be p. If the odds are fair, then the expected gain is zero, i.e.\n$4p+(1 p)( $1) = $0\n− −\nwhich implies that p = 1/5. Hence, if the odds are x : 1, then the proba-\nbility of winning is p = 1 = 0.2.\nx+1\n16.1.2 Edge\nEveryone bets because they think they have an advantage, or an edge\nover the others. It might be that they just think they have better informa-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 415,
      "chunk_index": 1
    }
  },
  {
    "text": "tion, better understanding, are using secret technology, or actually have\nprivate information (which may be illegal).\nThe edge is the expected profit that will be made from repeated trials\nrelative to the bet size. You have an edge if you can win with higher\nprobability (p ) than p = 1/(x+1). In the above example, with bet size\n∗",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 415,
      "chunk_index": 2
    }
  },
  {
    "text": "416 data science: theories, models, algorithms, and analytics\n1\n$ each time, suppose your probability of winning is not 1/5, but instead\nit is 1/4. What is your edge? The expected profit is\n( 1) (3/4)+4 (1/4) = 1/4\n− × ×\n1\nDividing this by the bet size (i.e. $ ) gives the edge equal to 1/4. No\nedge means zero or negative value betting.\n16.1.3 Bookmakers\nThese folks set the odds. Odds are dynamic of course. If the bookie\nthinks the probability of a win is 1/5, then he will set the odds to be a",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 416,
      "chunk_index": 0
    }
  },
  {
    "text": "41 351\nbit less than : , maybe something like . : . In this way his expected\n351\nintake minus payout is positive. At . : odds, if there are still a lot of\ntakers, then the bookie surely realizes that the probability of a win must\nbe higher than in his own estimation. He also infers that p > 1/(3.5+1),\n31\nand will then change the odds to say : . Therefore, he acts as a market\nmaker in the bet.\n16.2 Kelly Criterion\nSuppose you have an edge. How should you bet over repeated plays of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 416,
      "chunk_index": 1
    }
  },
  {
    "text": "the game to maximize your wealth. (Do you think this is the way that\n1956\nhedge funds operate?) The Kelly ( ) criterion says that you should\ninvest only a fraction of your wealth in the bet. By keeping some aside\nyou are guaranteed to not end up in ruin.\nWhat fraction should you bet? The answer is that you should bet\nEdge p x (1 p )\n∗ ∗\nf = = − −\nOdds x\nwhere the odds are expressed in the form x : 1. Recall that p is your\n∗\nprivately known probability of winning.\n16.2.1 Example",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 416,
      "chunk_index": 2
    }
  },
  {
    "text": "16.2.1 Example\nUsing the same numbers as we had before, i.e., x = 4, p = 1/4 = 0.25,\n∗\nwe get\n0.25(4) (1 0.25) 0.25\nf = − − = = 0.0625\n4 4\n625\nwhich means we invest . % of the current bankroll. Lets simulate this\nstrategy using R. Here is a simple program to simulate it, with optimal\nKelly betting, and over- and under-betting.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 416,
      "chunk_index": 3
    }
  },
  {
    "text": "against the odds: mathematics of gambling 417\n#Simulation of the Kelly Criterion\n#Basic data\npstar = 0.25 #private prob of winning\nodds = 4 #actual odds\np = 1/(1+odds) #house probability of winning\nedge = pstar*odds (1 pstar)\n− −\nf = edge/odds\nprint(c(\"p=\",p, \"pstar=\",pstar , \"edge=\",edge, \"f\",f))\nn = 1000\nx = runif(n)\nf_over = 1.5 *f\nf_under = 0.5 *f\nbankroll = rep(0,n); bankroll[1]=1\nbr_overbet = bankroll; br_overbet[1]=1\nbr_underbet = bankroll; br_underbet[1]=1\nfor (i in 2:n) {",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 417,
      "chunk_index": 0
    }
  },
  {
    "text": "for (i in 2:n) {\nif (x[i]<=pstar) {\nbankroll[i] = bankroll[i 1] + bankroll[i 1]*f*odds\nbr_overbet[i] = br_overb − et[i 1] + br_ove − rbet[i 1]*f_over*odds\nbr_underbet[i] = br_underbet − [i 1] + br_underbe − t[i 1]*f_under*odds\n− −\n}\nelse {\nbankroll[i] = bankroll[i 1] bankroll[i 1]*f\nbr_overbet[i] = br_overb − et[i −1] br_ove − rbet[i 1]*f_over\nbr_underbet[i] = br_underbet − [i 1− ] br_underbe − t[i 1]*f_under\n− − −\n}\n}\npar(mfrow=c(3,1))\nplot(bankroll ,type=\"l\")\nplot(br_overbet,type=\"l\")",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 417,
      "chunk_index": 1
    }
  },
  {
    "text": "plot(br_overbet,type=\"l\")\nplot(br_underbet,type=\"l\")\nprint(c(bankroll[n],br_overbet[n],br_underbet[n]))\nprint(c(bankroll[n]/br_overbet[n],bankroll[n]/br_underbet[n]))\nHere is the run time listing.\n> source(\"kelly .R\")\n1 0 2 0 25 0 25\n[ ] \"p=\" \" . \" \"pstar=\" \" . \" \"edge=\" \" . \" \"f\"\n8 0 0625 1000\n[ ] \" . \" \"n=\" \" \"\n1 542 29341 67 64294 158 83357\n[ ] . . .\n1 8 016999 3 414224\n[ ] . .\n1\nWe repeat bets a thousand times. The initial pot is $ only, but after a\n54229",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 417,
      "chunk_index": 2
    }
  },
  {
    "text": "54229\nthousand trials, the optimal strategy ends up at $ . , the over-betting\n6764 15883\none yields$ . , and the under-betting one delivers $ . . The ratio\n802 341\nof the optimal strategy to these two sub-optimal ones is . and . ,\nrespectively. This is conservative. Rerunning the model for another trial\nwith n = 1000 we get:\n> source(\"kelly .R\")\n1 0 2 0 25 0 25\n[ ] \"p=\" \" . \" \"pstar=\" \" . \" \"edge=\" \" . \" \"f\"\n8 0 0625 1000\n[ ] \" . \" \"n=\" \" \"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 417,
      "chunk_index": 3
    }
  },
  {
    "text": "418 data science: theories, models, algorithms, and analytics\n1 6 426197 15 1 734158 12 1 313690 12\n[ ] . e+ . e+ . e+\n1 3705 657 4891 714\n[ ] . .\n3705 4891\nThe ratios are huge in comparison in this case, i.e., and , re-\nspectively. And when we raise the trials to n = 5000, we have\n> source(\"kelly .R\")\n1 0 2 0 25 0 25\n[ ] \"p=\" \" . \" \"pstar=\" \" . \" \"edge=\" \" . \" \"f\"\n8 0 0625 5000\n[ ] \" . \" \"n=\" \" \"\n1 484145279169 1837741 9450314895\n[ ]\n1 263445 8383 51 2306\n[ ] . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 418,
      "chunk_index": 0
    }
  },
  {
    "text": "[ ]\n1 263445 8383 51 2306\n[ ] . .\nNote here that over-betting is usually worse then under-betting the Kelly\noptimal. Hence, many players employ what is known as the ‘Half-Kelly”\nrule, i.e., they bet f/2.\nLook at the resultant plot of the three strategies for the first example,\n161\nshown in Figure . . The top plot follows the Kelly criterion, but the\nother two deviate from it, by overbetting or underbetting the fraction\ngiven by Kelly.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 418,
      "chunk_index": 1
    }
  },
  {
    "text": "given by Kelly.\nWe can very clearly see that not betting Kelly leads to far worse out-\n1000\ncomes than sticking with the Kelly optimal plan. We ran this for\nperiods, as if we went to the casino every day and placed one bet (or\nwe placed four bets every minute for about four hours straight). Even\nwithin a few trials, the performance of the Kelly is remarkable. Note\nthough that this is only one of the simulated outcomes. The simulations",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 418,
      "chunk_index": 2
    }
  },
  {
    "text": "would result in different types of paths of the bankroll value, but gener-\nally, the outcomes are similar to what we see in the figure.\nOver-betting leads to losses faster than under-betting as one would\nnaturally expect, because it is the more risky strategy.\nIn this model, under the optimal rule, the probability of dropping to\n1/n of the bankroll is 1/n. So the probability of dropping to 90 % of the\nbankroll (n = 1.11) is 0.9. Or, there is a 90 % chance of losing 10 % of the\nbankroll.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 418,
      "chunk_index": 3
    }
  },
  {
    "text": "bankroll.\nAlternate betting rules are: (a) fixed size bets, (b) double up bets. The\nformer is too slow, the latter ruins eventually.\n16.2.2 Deriving the Kelly Criterion\nFirst we define some notation. Let B be the bankroll at time t. We index\nt\ntime as going from time t = 1,...,N.\nThe odds are denoted, as before x : 1, and the random variable denot-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 418,
      "chunk_index": 4
    }
  },
  {
    "text": "against the odds: mathematics of gambling 419\n0 200 400 600 800 1000\n0051\n005\n0\nIndex\nllorknab\n0 200 400 600 800 1000\n0021\n008\n004\n0\nIndex\ntebrevo_rb\n0 200 400 600 800 1000\n051\n05\n0\nIndex\ntebrednu_rb\nFigure16.1: Bankrollevolution\nundertheKellyrule. Thetop\nplotfollowstheKellycriterion,\nbuttheothertwodeviatefrom\nit,byoverbettingorunderbetting\nthefractiongivenbyKelly. The\nvariablesare: oddsare4to1,\nimplyingahouseprobability\nof p = 0.2,ownprobabilityof\nwinningis p ∗ =0.25.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 419,
      "chunk_index": 0
    }
  },
  {
    "text": "420 data science: theories, models, algorithms, and analytics\ning the outcome (i.e., gains) of the wager is written as\n(cid:40)\nx w/p p\nZ =\nt\n1 w/p (1 p)\n− −\nWe are said to have an edge when E(Z ) > 0. The edge will be equal to\nt\npx (1 p) > 0.\n− −\nWe invest fraction f of our bankroll, where 0 < f < 1, and since f = 1,\n(cid:54)\nthere is no chance of being wiped out. Each wager is for an amount fB\nt\nand returns fB Z . Hence, we may write\nt t\nB = B + fB Z\nt t 1 t 1 t\n− −\n= B [1+ fZ ]\nt 1 t\n−\nt\n∏",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 420,
      "chunk_index": 0
    }
  },
  {
    "text": "t t 1 t 1 t\n− −\n= B [1+ fZ ]\nt 1 t\n−\nt\n∏\n= B [1+ fZ ]\n0 t\ni=1\nIf we define the growth rate as\n(cid:18) (cid:19)\n1 B\nt\ng (f) = ln\nt\nt B\n0\n1 ∏ t\n= ln [1+ fZ ]\nt\nt\ni=1\n1 ∑ t\n= ln[1+ fZ ]\nt\nt\ni=1\nTaking the limit by applying the law of large numbers, we get\ng(f) = lim g (f) = E[ln(1+ fZ)]\nt\nt ∞\n→\nwhich is nothing but the time average of ln(1+ fZ). We need to find the\nf that maximizes g(f). We can write this more explicitly as\ng(f) = pln(1+ fx)+(1 p)ln(1 f)\n− −\nDifferentiating to get the f.o.c,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 420,
      "chunk_index": 1
    }
  },
  {
    "text": "− −\nDifferentiating to get the f.o.c,\n∂g x 1\n= p +(1 p) − = 0\n∂f 1+ fx − 1 f\n−\nSoving this first-order condition for f gives\npx (1 p)\nThe Kelly criterion: f ∗ = − −\nx\nThis is the optimal fraction of the bankroll that should be invested\nin each wager. Note that we are back to the well-known formula of\nEdge/Odds we saw before.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 420,
      "chunk_index": 2
    }
  },
  {
    "text": "against the odds: mathematics of gambling 421\n16.3 Entropy\nEntropy is defined by physicists as the extent of disorder in the universe.\nEntropy in the universe keeps on increasing. Things get more and more\ndisorderly. The arrow of time moves on inexorably, and entropy keeps\non increasing.\nIt is intuitive that as the entropy of a communication channel in-\ncreases, its informativeness decreases. The connection between entropy\nand informativeness was made by Claude Shannon, the father of infor-\n1948",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 421,
      "chunk_index": 0
    }
  },
  {
    "text": "1948\nmation theory. It was his PhD thesis at MIT. See Shannon ( ).\nWith respect to probability distributions, entropy of a discrete distri-\nbution taking values p ,p ,...,p is\n1 2 K\n{ }\nK\n∑\nH = p ln(p )\nj j\n−\nj=1\nFor the simple wager we have been considering, entropy is\nH = [plnp+(1 p)ln(1 p)]\n− − −\nThis is called Shannon entropy after his seminal work in 1948 . For p =\n1/2,1/5,1/100 entropy is\n> p= 0 . 5 ; (p*log(p)+( 1 p)*log( 1 p))\n− − −\n1 0 6931472\n[ ] .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 421,
      "chunk_index": 1
    }
  },
  {
    "text": "− − −\n1 0 6931472\n[ ] .\n> p= 0 . 2 ; (p*log(p)+( 1 p)*log( 1 p))\n− − −\n1 0 5004024\n[ ] .\n> p= 0 . 01 ; (p*log(p)+( 1 p)*log( 1 p))\n− − −\n1 0 05600153\n[ ] .\nWe see various probability distributions in decreasing order of entropy.\nAt p = 0.5 entropy is highest.\nNote that the normal distribution is the one with the highest entropy\nin its class of distributions.\n16.3.1 Linking the Kelly Criterion to Entropy\nFor the particular case of a simple random walk, we have odds x = 1. In\nthis case,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 421,
      "chunk_index": 2
    }
  },
  {
    "text": "this case,\nf ∗ = p (1 p) = 2p 1\n− − −",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 421,
      "chunk_index": 3
    }
  },
  {
    "text": "422 data science: theories, models, algorithms, and analytics\nwhere we see that p = 1/2, and the optimal average bet value is\ng\n∗\n= pln(1+ f)+(1 p)ln(1 f)\n− −\n= pln(2p)+(1 p)ln[2(1 p)]\n− −\n= ln2+ plnp+(1 p)ln(1 p)\n− −\n= ln2 H\n−\nwhere H is the entropy of the distribution of Z. For p = 0.5, we have\ng ∗ = ln2 0.5ln(0.5) 0.5ln(0.5) = 1.386294\n− −\nWe note that g is decreasing in entropy, because informativeness\n∗\ndeclines with entropy and so the portfolio earns less if we have less of an",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 422,
      "chunk_index": 0
    }
  },
  {
    "text": "edge, i.e. our winning information is less than perfect.\n16.3.2 Linking the Kelly criterion to portfolio optimization\nA small change in the mathematics above leads to an analogous concept\nfor portfolio policy. The value of a portfolio follows the dynamics below\nt\n∏\nB = B [1+(1 f)r+ fZ ] = B [1+r+ f(Z r)]\nt t 1 t 0 t\n− − −\ni=1\nHence, the growth rate of the portfolio is given by\n(cid:18) (cid:19)\n1 B\nt\ng (f) = ln\nt\nt B\n0\n(cid:32) (cid:33)\n1 ∏ t\n= ln [1+r+ f(Z r)]\nt\nt −\ni=1\n1 ∑ t",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 422,
      "chunk_index": 1
    }
  },
  {
    "text": "1 ∏ t\n= ln [1+r+ f(Z r)]\nt\nt −\ni=1\n1 ∑ t\n= ln([1+r+ f(Z r)])\nt\nt −\ni=1\nTaking the limit by applying the law of large numbers, we get\ng(f) = lim g (f) = E[ln(1+r+ f(Z r))]\nt\nt ∞ −\n→\nHence, maximizing the growth rate of the portfolio is the same as max-\nimizing expected log utility. For a much more detailed analysis, see\n1996\nBrowne and Whitt ( ).\n16.3.3 Implementing day trading\nWe may choose any suitable distribution for the asset Z. Suppose Z is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 422,
      "chunk_index": 2
    }
  },
  {
    "text": "normally distributed with mean µ and variance σ2. Then we just need to",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 422,
      "chunk_index": 3
    }
  },
  {
    "text": "against the odds: mathematics of gambling 423\nfind f such that we have\nf ∗ = argmax f E[ln(1+r+ f(Z − r))]\nThis may be done numerically. Note now that this does not guarantee\nthat 0 < f < 1, which does not preclude ruin.\nHow would a day-trader think about portfolio optimization? His\nproblem would be closer to that of a gambler’s because he is very much\nlike someone at the tables, making a series of bets, whose outcomes be-\ncome known in very short time frames. A day-trader can easily look at",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 423,
      "chunk_index": 0
    }
  },
  {
    "text": "his history of round-trip trades and see how many of them made money,\nand how many lost money. He would then obtain an estimate of p, the\nprobability of winning, which is the fraction of total round-trip trades\nthat make money.\nThe Lavinio ( 2000 ) d-ratio is known as the ‘gain-loss” ratio and is as\nfollows:\nn ∑n max(0, Z )\nd = d × j=1 − j\nn ∑n max(0,Z )\nu × j=1 j\nwhere n is the number of down (loss) trades, and n is the number of\nd u",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 423,
      "chunk_index": 1
    }
  },
  {
    "text": "d u\nup (gain) trades and n = n +n , and Z are the returns on the trades.\nd u j\nIn our original example at the beginning of this chapter, we have odds\nof 4 : 1 , implying n = 4 loss trades for each win (n = 1) trade, and a\nd u\nwinning trade nets +4, and a losing trade nets 1. Hence, we have\n−\n4 (1+1+1+1)\nd = × = 4 = x\n1 4\n×\nwhich is just equal to the odds. Once, these are computed, the day-\ntrader simply plugs them in to the formula we had before, i.e.,\npx (1 p) (1 p)\nf = − − = p −\nx − x",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 423,
      "chunk_index": 2
    }
  },
  {
    "text": "px (1 p) (1 p)\nf = − − = p −\nx − x\nOf course, here p = 0.2. A trader would also constantly re-assess the\nvalues of p and x given that the markets change over time.\n16.4 Casino Games\n162\nThe statistics of various casino games are displayed in Figure . . To\nrecap, note that the Kelly criterion maximizes the average bankroll and\nalso minimizes the risk of ruin, but is of no use if the house had an edge.\nYou need to have an edge before it works. But then it really works! It is",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 423,
      "chunk_index": 3
    }
  },
  {
    "text": "424 data science: theories, models, algorithms, and analytics\nnot a short-term formula and works over a long sequence of bets. Nat-\nurally it follows that it also minimizes the number of bets needed to\ndouble the bankroll.\nFigure16.2: See\nhttp://wizardofodds.com/gambling/house-edge/.\nTheHouseEdgeforvariousgames.\nTheedgeisthesameas f inour\n−\nnotation. Thestandarddeviationis\nthatofthebankrollof$1forone\nbet.\n1997\nIn a neat paper, Thorp ( ) presents various Kelly rules for black-\n1962",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 424,
      "chunk_index": 0
    }
  },
  {
    "text": "1962\njack, sports betting, and the stock market. Reading Thorp ( ) for\nblackjack is highly recommended. And of course there is the great story\n2003\nof the MIT Blackjack Team in Mezrich ( ). Here is an example from\n1997\nThorp ( ).\nSuppose you have an edge where you can win +1 with probability\n0.51, and lose 1 with probability 0.49 when the blackjack deck is “hot”\n−\nand when it is cold the probabilities are reversed. We will bet f on the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 424,
      "chunk_index": 1
    }
  },
  {
    "text": "hot deck and af,a < 1 on the cold deck. We have to bet on cold decks\njust to prevent the dealer from getting suspicious. Hot and cold decks",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 424,
      "chunk_index": 2
    }
  },
  {
    "text": "against the odds: mathematics of gambling 425\noccur with equal probability. Then the Kelly growth rate is\ng(f) = 0.5[0.51ln(1+ f)+0.49ln(1 f)]+0.5[0.49ln(1+af)+0.51ln(1 af)]\n− −\nIf we do not bet on cold decks, then a = 0 and f = 0.02 using the usual\n∗\nformula. As a increases from 0 to 1 , we see that f decreases. Hence, we\n∗\nbet less of our pot to make up for losses from cold decks. We compute\nthis and get the following:\na = 0 f ∗ = 0.020\n→\na = 1/4 f ∗ = 0.014\n→\na = 1/2 f ∗ = 0.008\n→",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 425,
      "chunk_index": 0
    }
  },
  {
    "text": "→\na = 1/4 f ∗ = 0.014\n→\na = 1/2 f ∗ = 0.008\n→\na = 3/4 f ∗ = 0.0032\n→",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 425,
      "chunk_index": 1
    }
  },
  {
    "text": "17\nIn the Same Boat: Cluster Analysis and Prediction Trees\n17.1 Introduction\nThere are many aspects of data analysis that call for grouping individ-\nuals, firms, projects, etc. These fall under the rubric of what may be\ntermed as “classification” analysis. Cluster analysis comprises a group of\ntechniques that uses distance metrics to bunch data into categories.\nThere are two broad approaches to cluster analysis:\n1\n. Agglomerative or Hierarchical or Bottom-up: In this case we begin",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 427,
      "chunk_index": 0
    }
  },
  {
    "text": "with all entities in the analysis being given their own cluster, so that\nwe start with n clusters. Then, entities are grouped into clusters based\non a given distance metric between each pair of entities. In this way\na hierarchy of clusters is built up and the researcher can choose which\ngrouping is preferred.\n2 . Partitioning or Top-down: In this approach, the entire set of n entities\nis assumed to be a cluster. Then it is progressively partitioned into\nsmaller and smaller clusters.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 427,
      "chunk_index": 1
    }
  },
  {
    "text": "smaller and smaller clusters.\nWe will employ both clustering approaches and examine their properties\nwith various data sets as examples.\n17.2 Clustering using k-means\nThis approach is bottom-up. If we have a sample of n observations to be\nallocated to k clusters, then we can initialize the clusters in many ways.\nOne approach is to assume that each observation is a cluster unto itself.\nWe proceed by taking each observation and allocating it to the nearest",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 427,
      "chunk_index": 2
    }
  },
  {
    "text": "cluster using a distance metric. At the outset, we would simply allocate\nan observation to its nearest neighbor.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 427,
      "chunk_index": 3
    }
  },
  {
    "text": "428 data science: theories, models, algorithms, and analytics\nHow is nearness measured? We need a distance metric, and one com-\nmon one is Euclidian distance. Suppose we have two observations x\ni\nand x . These may be represented by a vector of attributes. Suppose\nj\nour observations are people, and the attributes are {height, weight, IQ}\n= x = h ,w ,I for the i-th individual. Then the Euclidian distance\ni i i i\n{ }\nbetween two individuals i and j is\n(cid:113)\nd = (h h )2+(w w )2+(I I )2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 428,
      "chunk_index": 0
    }
  },
  {
    "text": "(cid:113)\nd = (h h )2+(w w )2+(I I )2\nij i j i j i j\n− − −\nIn contrast, the “Manhattan” distance is given by (when is this more\nappropriate?)\nd = h h + w w + I I\nij i j i j i j\n| − | | − | | − |\nWe may use other metrics such as the cosine distance, or the Maha-\nlanobis distance. A matrix of n n values of all d s is called the “dis-\nij\n×\ntance matrix.” Using this distance metric we assign nodes to clusters or\nattach them to nearest neighbors. After a few iterations, no longer are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 428,
      "chunk_index": 1
    }
  },
  {
    "text": "clusters made up of singleton observations, and the number of clusters\nreaches k, the preset number required, and then all nodes are assigned to\none of these k clusters. As we examine each observation we then assign\nit (or re-assign it) to the nearest cluster, where the distance is measured\nfrom the observation to some representative node of the cluster. Some\ncommon choices of the representative node in a cluster of are:\n1\n. Centroid of the cluster. This is the mean of the observations in the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 428,
      "chunk_index": 2
    }
  },
  {
    "text": "cluster for each attribute. The centroid of the two observations above\nis the average vector (h +h )/2,(w +w )/2,(I + I )/2 . This is\ni j i j i j\n{ }\noften called the “center” of the cluster. If there are more nodes then\nthe centroid is the average of the same coordinate for all nodes.\n2\n. Closest member of the cluster.\n3\n. Furthest member of the cluster.\nThe algorithm converges when no re-assignments of observations to\nclusters occurs. Note that k-means is a random algorithm, and may not",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 428,
      "chunk_index": 3
    }
  },
  {
    "text": "always return the same clusters every time the algorithm is run. Also,\none needs to specify the number of clusters to begin with and there may\nbe no a-priori way in which to ascertain the correct number. Hence, trial\nand error and examination of the results is called for. Also, the algorithm\naims to have balanced clusters, but this may not always be appropriate.\nIn R, we may construct the distance matrix using the dist function.\nUsing the NCAA data we are already familiar with, we have:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 428,
      "chunk_index": 4
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 429\n> ncaa = read. table(\"ncaa. txt\" ,header=TRUE)\n> names(ncaa)\n1\n[ ] \"No\" \"NAME\" \"GMS\" \"PTS\" \"REB\" \"AST\" \"TO\" \"A.T\" \"STL\" \"BLK\"\n11 3\n[ ] \"PF\" \"FG\" \"FT\" \"X P\"\n3 14\n> d = dist (ncaa[ , : ] , method=\"euclidian\")\nExamining this matrix will show that it contains n(n 1)/2 elements,\n−\ni.e., the number of pairs of nodes. Only the lower triangular matrix of d\nis populated.\nIt is important to note that since the size of the variables is very dif-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 429,
      "chunk_index": 0
    }
  },
  {
    "text": "ferent, simply applying the dist function is not advised, as the larger\nvariables swamp the distance calculation. It is best to normalize the vari-\nables first, before calculating distances. The scale function in R is simple\nto apply as follows.\n> ncaa_data = as.matrix(ncaa[,3:14])\n> summary(ncaa_data)\nGMS PTS REB AST TO\nMin. :1.000 Min. :46.00 Min. :19.00 Min. : 2.00 Min. : 5.00\n1st Qu.:1.000 1st Qu.:61.75 1st Qu.:31.75 1st Qu.:10.00 1st Qu.:11.00",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 429,
      "chunk_index": 1
    }
  },
  {
    "text": "Median :2.000 Median :67.00 Median :34.35 Median :13.00 Median :13.50\nMean :1.984 Mean :67.10 Mean :34.47 Mean :12.75 Mean :13.96\n3rd Qu.:2.250 3rd Qu.:73.12 3rd Qu.:37.20 3rd Qu.:15.57 3rd Qu.:17.00\nMax. :6.000 Max. :88.00 Max. :43.00 Max. :20.00 Max. :24.00\nA.T STL BLK PF\nMin. :0.1500 Min. : 2.000 Min. :0.000 Min. :12.00\n1st Qu.:0.7400 1st Qu.: 5.000 1st Qu.:1.225 1st Qu.:16.00\nMedian :0.9700 Median : 7.000 Median :2.750 Median :19.00\nMean :0.9778 Mean : 6.823 Mean :2.750 Mean :18.66",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 429,
      "chunk_index": 2
    }
  },
  {
    "text": "Mean :0.9778 Mean : 6.823 Mean :2.750 Mean :18.66\n3rd Qu.:1.2325 3rd Qu.: 8.425 3rd Qu.:4.000 3rd Qu.:20.00\nMax. :1.8700 Max. :12.000 Max. :6.500 Max. :29.00\nFG FT X3P\nMin. :0.2980 Min. :0.2500 Min. :0.0910\n1st Qu.:0.3855 1st Qu.:0.6452 1st Qu.:0.2820\nMedian :0.4220 Median :0.7010 Median :0.3330\nMean :0.4233 Mean :0.6915 Mean :0.3334\n3rd Qu.:0.4632 3rd Qu.:0.7705 3rd Qu.:0.3940\nMax. :0.5420 Max. :0.8890 Max. :0.5220\n> ncaa_data = scale(ncaa_data)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 429,
      "chunk_index": 3
    }
  },
  {
    "text": "> ncaa_data = scale(ncaa_data)\nThe scale function above normalizes all columns of data. If you run\nsummary again, all variables will have mean zero and unit standard devi-\nation. Here is a check.\n> round(apply(ncaa_data , 2 ,mean) , 2 )\n3\nGMS PTS REB AST TO A.T STL BLK PF FG FT X P\n0 0 0 0 0 0 0 0 0 0 0 0\n> apply(ncaa_data , 2 ,sd)\n3\nGMS PTS REB AST TO A.T STL BLK PF FG FT X P\n1 1 1 1 1 1 1 1 1 1 1 1\nClustering takes many observations with their characteristics and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 429,
      "chunk_index": 4
    }
  },
  {
    "text": "then allocates them into buckets or clusters based on their similarity.\nIn finance, we may use cluster analysis to determine groups of similar\n171\nfirms. For example, see Figure . , where I ran a cluster analysis on VC",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 429,
      "chunk_index": 5
    }
  },
  {
    "text": "430 data science: theories, models, algorithms, and analytics\nfinancing of startups to get a grouping of types of venture financing into\ndifferent styles.\nUnlike regression analysis, cluster analysis uses only the right-hand\nside variables, and there is no dependent variable required. We group\nobservations purely on their overall similarity across characteristics.\nHence, it is closely linked to the notion of “communities” that we stud-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 430,
      "chunk_index": 0
    }
  },
  {
    "text": "ied earlier, though that concept lives in the domain of networks.\nFigure17.1: VCStyleClusters.\n1: Early/Exp stage—Non US\n2: Exp stage—Computer\n3: Early stage—Computer\n4: Early/Exp/Late stage—Non High-­‐tech\n5: Early/Exp stage—Comm/Media\n6: Late stage—Comm/Media & Computer\n7: Early/Exp/Late stage—Medical\n8: Early/Exp/Late stage—Biotech\n9: Early/Exp/Late stage—Semiconductors\n10: Seed stage\n11: Buyout stage <Large Inv>\n12: Other stage\n17.2.1 Example: Randomly generated data in kmeans",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 430,
      "chunk_index": 1
    }
  },
  {
    "text": "17.2.1 Example: Randomly generated data in kmeans\nHere we use the example from the kmeans function to see how the clus-\nters appear. This function is standard issue, i.e., it comes with the stats",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 430,
      "chunk_index": 2
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 431\npackage, which is included in the base R distribution and does not need\nto be separately installed. The data is randomly generated but has two\nbunches of items with different means, so we should be easily able to see\ntwo separate clusters. You will need the graphics package which is also\nin the base installation.\n> require(graphics)\n>\n> # a 2 dimensional example\n> x < − rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 431,
      "chunk_index": 0
    }
  },
  {
    "text": "+ − matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))\n> colnames(x) < c(\"x\", \"y\")\n> (cl < kmeans − (x, 2))\nK means − clustering with 2 clusters of sizes 52, 48\n−\nCluster means:\nx y\n1 0.98813364 1.01967200\n2 0.02752225 0.02651525\n− −\nClustering vector:\n[1] 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2\n[36] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[71] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 431,
      "chunk_index": 1
    }
  },
  {
    "text": "Within cluster sum of squares by cluster:\n[1] 10.509092 6.445904\nAvailable components:\n[1] \"cluster\" \"centers\" \"withinss\" \"size\"\n> plot(x, col = cl$cluster)\n> points(cl$centers , col = 1:2, pch = 8, cex=2)\n172\nThe plotted clusters appear in Figure . .\n5\nWe can also examine the same example with clusters. The output is\n173\nshown in Figure .\n> ## random starts do help here with too many clusters\n> (cl < kmeans(x, 5, nstart = 25))\nK means − clustering with 5 clusters of sizes 25, 22, 16, 20, 17\n−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 431,
      "chunk_index": 2
    }
  },
  {
    "text": "−\nCluster means:\nx y\n1 0.1854632 0.1129291\n2 −0.1321432 0.2089422\n3 0.9217674 −0.6424407\n4 0.7404867 1.2253548\n5 1.3078410 1.1022096\nClustering vector:\n[1] 1 2 1 1 2 2 2 4 2 1 2 1 1 1 1 2 2 1 2 1 1 1 1 2 2 1 1 2 2 3 1 2 2 1 2\n[36] 2 3 2 2 1 1 2 1 1 1 1 1 2 1 2 5 5 4 4 4 4 4 4 5 4 5 4 5 5 5 5 3 4 3 3\n[71] 3 3 3 5 5 5 5 5 4 5 4 4 3 4 5 3 5 4 3 5 4 4 3 3 4 3 4 3 4 3\nWithin cluster sum of squares by cluster:\n[1] 2.263606 1.311527 1.426708 2.084694 1.329643\nAvailable components:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 431,
      "chunk_index": 3
    }
  },
  {
    "text": "Available components:\n[1] \"cluster\" \"centers\" \"withinss\" \"size\"\n> plot(x, col = cl$cluster)\n> points(cl$centers , col = 1:5, pch = 8)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 431,
      "chunk_index": 4
    }
  },
  {
    "text": "432 data science: theories, models, algorithms, and analytics\n-0.5 0.0 0.5 1.0 1.5\n0.2\n5.1\n0.1\n5.0\n0.0\n5.0-\nx\ny\nFigure17.2: Twoclusterexample.\n17.2.2 Example: Clustering of VC financing rounds\n2001\nIn this section we examine data on VC’s financing of startups from –\n2006\n, using data on individual financing rounds. The basic information\nthat we have is shown below.\n> data = read.csv(\"vc_clust .csv\" ,header=TRUE,sep=\" ,\")\n> dim(data)\n1 3697 47\n[ ]\n> names(data)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 432,
      "chunk_index": 0
    }
  },
  {
    "text": "> dim(data)\n1 3697 47\n[ ]\n> names(data)\n[ 1 ] \"fund_name\" \"fund_year\" \"fund_avg_rd_invt\"\n[ 4 ] \"fund_avg_co_invt\" \"fund_num_co\" \"fund_num_rds\"\n[ 7 ] \"fund_tot_invt\" \"stage_num 1 \" \"stage_num 2 \"\n[ 10 ] \"stage_num 3 \" \"stage_num 4 \" \"stage_num 5 \"\n[ 13 ] \"stage_num 6 \" \"stage_num 7 \" \"stage_num 8 \"\n[ 16 ] \"stage_num 9 \" \"stage_num 10 \" \"stage_num 11 \"\n[ 19 ] \"stage_num 12 \" \"stage_num 13 \" \"stage_num 14 \"\n[ 22 ] \"stage_num 15 \" \"stage_num 16 \" \"stage_num 17 \"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 432,
      "chunk_index": 1
    }
  },
  {
    "text": "[ 25 ] \"invest_type_num 1 \" \"invest_type_num 2 \" \"invest_type_num 3 \"\n[ 28 ] \"invest_type_num 4 \" \"invest_type_num 5 \" \"invest_type_num 6 \"\n[ 31 ] \"fund_nation_US\" \"fund_state_CAMA\" \"fund_type_num 1 \"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 432,
      "chunk_index": 2
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 433\n-0.5 0.0 0.5 1.0 1.5\n0.2\n5.1\n0.1\n5.0\n0.0\n5.0-\nx\ny\nFigure17.3: Fiveclusterexample.\n[ 34 ] \"fund_type_num 2 \" \"fund_type_num 3 \" \"fund_type_num 4 \"\n[ 37 ] \"fund_type_num 5 \" \"fund_type_num 6 \" \"fund_type_num 7 \"\n[ 40 ] \"fund_type_num 8 \" \"fund_type_num 9 \" \"fund_type_num 10 \"\n[ 43 ] \"fund_type_num 11 \" \"fund_type_num 12 \" \"fund_type_num 13 \"\n[ 46 ] \"fund_type_num 14 \" \"fund_type_num 15 \"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 433,
      "chunk_index": 0
    }
  },
  {
    "text": "[ 46 ] \"fund_type_num 14 \" \"fund_type_num 15 \"\nWe clean out all rows that have missing values as follows:\n> idx = which(rowSums(is.na(data))==0)\n> length(idx)\n[1] 2975\n> data = data[idx,]\n> dim(data)\n[1] 2975 47\nWerunafirst-cutk-meansanalysisusinglimiteddata.\n> idx = c(3,6,31,32)\n> cdata = data[,idx]\n> names(cdata)\n[1] \"fund_avg_rd_invt\" \"fund_num_rds\" \"fund_nation_US\"\n[4] \"fund_state_CAMA\"\n> fit = kmeans(cdata,4)\n> fit$size\n[1] 2856 2 95 22\n> fit$centers",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 433,
      "chunk_index": 1
    }
  },
  {
    "text": "> fit$size\n[1] 2856 2 95 22\n> fit$centers\nfund_avg_rd_invt fund_num_rds fund_nation_US fund_state_CAMA\n1 4714.894 8.808824 0.5560224 0.2244398\n2 1025853.650 7.500000 0.0000000 0.0000000\n3 87489.873 6.400000 0.4631579 0.1368421\n4 302948.114 5.318182 0.7272727 0.2727273",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 433,
      "chunk_index": 2
    }
  },
  {
    "text": "434 data science: theories, models, algorithms, and analytics\nWe see that the clusters are hugely imbalanced, with one cluster ac-\ncounting for most of the investment rounds. Let’s try a different cut\nnow. Using investment type = {buyout, early, expansion, late, other, seed}\n4\ntypes of financing, we get the following, assuming clusters.\n> idx = c(25,26,27,28,29,30,31,32)\n> cdata = data[,idx]\n> names(cdata)\n[1] \"invest_type_num1\" \"invest_type_num2\" \"invest_type_num3\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 434,
      "chunk_index": 0
    }
  },
  {
    "text": "[4] \"invest_type_num4\" \"invest_type_num5\" \"invest_type_num6\"\n[7] \"fund_nation_US\" \"fund_state_CAMA\"\n> fit = kmeans(cdata,4)\n> fit$size\n[1] 2199 65 380 331\n> fit$centers\ninvest_type_num1 invest_type_num2 invest_type_num3 invest_type_num4\n1 0.0000000 0.00000000 0.00000000 0.00000000\n2 0.0000000 0.00000000 0.00000000 0.00000000\n3 0.6868421 0.12631579 0.06052632 0.12631579\n4 0.4592145 0.09969789 0.39274924 0.04833837\ninvest_type_num5 invest_type_num6 fund_nation_US fund_state_CAMA",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 434,
      "chunk_index": 1
    }
  },
  {
    "text": "1 0 1 0.5366075 0.2391996\n2 1 0 0.7538462 0.1692308\n3 0 0 1.0000000 0.3236842\n4 0 0 0.1178248 0.0000000\n6\nHere we get a very different outcome. Now, assuming clusters, we\nhave:\n> idx = c(25,26,27,28,29,30,31,32)\n> cdata = data[,idx]\n> fit = kmeans(cdata,6)\n> fit$size\n[1] 34 526 176 153 1673 413\n> fit$centers\ninvest_type_num1 invest_type_num2 invest_type_num3 invest_type_num4\n1 0 0.3235294 0 0.3529412\n2 0 0.0000000 0 0.0000000\n3 0 0.3977273 0 0.2954545\n4 0 0.0000000 1 0.0000000",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 434,
      "chunk_index": 2
    }
  },
  {
    "text": "4 0 0.0000000 1 0.0000000\n5 0 0.0000000 0 0.0000000\n6 1 0.0000000 0 0.0000000\ninvest_type_num5 invest_type_num6 fund_nation_US fund_state_CAMA\n1 0.3235294 0 1.0000000 1.0000000\n2 0.0000000 1 1.0000000 1.0000000\n3 0.3068182 0 0.6306818 0.0000000\n4 0.0000000 0 0.4052288 0.1503268\n5 0.0000000 1 0.3909145 0.0000000\n6 0.0000000 0 0.6319613 0.1864407\n17.2.3 NCAA teams\nWe revisit our NCAA data set, and form clusters there.\n> ncaa = read. table(\"ncaa. txt\" ,header=TRUE)\n> names(ncaa)\n1",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 434,
      "chunk_index": 3
    }
  },
  {
    "text": "> names(ncaa)\n1\n[ ] \"No\" \"NAME\" \"GMS\" \"PTS\" \"REB\" \"AST\" \"TO\" \"A.T\" \"STL\" \"BLK\"\n11 3\n[ ] \"PF\" \"FG\" \"FT\" \"X P\"",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 434,
      "chunk_index": 4
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 435\n3 14 4\n> fit = kmeans(ncaa[ , : ] , )\n> fit$size\n1 14 17 27 6\n[ ]\n> fit$centers\nGMS PTS REB AST TO A.T STL\n1 3 357143 80 12857 34 15714 16 357143 13 70714 1 2357143 6 821429\n. . . . . . .\n2 1 529412 60 24118 38 76471 9 282353 16 45882 0 5817647 6 882353\n. . . . . . .\n3 1 777778 68 39259 33 17407 13 596296 12 83704 1 1107407 6 822222\n. . . . . . .\n4 1 000000 50 33333 28 83333 10 333333 12 50000 0 9000000 6 666667\n. . . . . . .\n3",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 435,
      "chunk_index": 0
    }
  },
  {
    "text": ". . . . . . .\n3\nBLK PF FG FT X P\n1 2 514286 18 48571 0 4837143 0 7042143 0 4035714\n. . . . .\n2 2 882353 18 51176 0 3838824 0 6683529 0 3091765\n. . . . .\n3 2 918519 18 68519 0 4256296 0 7071852 0 3263704\n. . . . .\n4 2 166667 19 33333 0 3835000 0 6565000 0 2696667\n. . . . .\n> idx = c( 4 , 6 ); plot(ncaa[ ,idx] ,col=fit$cluster )\n174\nSee Figure . . Since there are more than two attributes of each obser-\nvation in the data, we picked two of them {AST, PTS} and plotted the\nclusters against those.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 435,
      "chunk_index": 1
    }
  },
  {
    "text": "clusters against those.\n50 60 70 80\n02\n51\n01\n5\nPTS\nTSA\nFigure17.4: NCAAclusterexample.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 435,
      "chunk_index": 2
    }
  },
  {
    "text": "436 data science: theories, models, algorithms, and analytics\n17.3 Hierarchical Clustering\nHierarchical clustering is both, a top-down (divisive) approach and\nbottom-up (agglomerative) approach. At the top level there is just one\ncluster. A level below, this may be broken down into a few clusters,\nwhich are then further broken down into more sub-clusters a level be-\nlow, and so on. This clustering approach is computationally expensive,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 436,
      "chunk_index": 0
    }
  },
  {
    "text": "and the divisive approach is exponentially expensive in n, the number of\nentities being clustered. In fact, the algorithm is (2n).\nO\nThe function for clustering is hclust and is included in the stats\npackage in the base R distribution.\nWe re-use the NCAA data set one more time.\n3 14\n> d = dist (ncaa[ , : ] , method=\"euclidian\")\n> fit = hclust(d, method=\"ward\")\n> names( fit )\n1\n[ ] \"merge\" \"height\" \"order\" \"labels\" \"method\"\n6\n[ ] \" call \" \"dist .method\"\n> plot( fit ,main=\"NCAA Teams\")\n4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 436,
      "chunk_index": 1
    }
  },
  {
    "text": "> plot( fit ,main=\"NCAA Teams\")\n4\n> groups = cutree( fit , k= )\n> rect . hclust( fit , k= 4 , border=\"blue\")\nWe begin by first computing the distance matrix. Then we call the hclust\nfunction and the plot function applied to object fit gives what is known\nas a “dendrogram” plot, showing the cluster hierarchy. We may pick\nclusters at any level. In this case, we chose a “cut” level such that we get\nfour clusters, and the rect.hclust function allows us to superimpose",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 436,
      "chunk_index": 2
    }
  },
  {
    "text": "boxes on the clusters so we can see the grouping more clearly. The result\n175\nis plotted in Figure . .\nWe can also visualize the clusters loaded on to the top two principal\ncomponents as follows, using the clusplot function that resides in pack-\n176\nage cluster. The result is plotted in Figure . .\n> groups\n[1] 1 1 1 1 1 2 1 1 3 2 1 3 3 1 1 1 2 3 3 2 3 2 1 1 3 3 1 3 2 3 3 3 1 2 2\n[36] 3 3 4 1 2 4 4 4 3 3 2 4 3 1 3 3 4 1 2 4 3 3 3 3 4 4 4 4 3\n> library(cluster)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 436,
      "chunk_index": 3
    }
  },
  {
    "text": "> library(cluster)\n> clusplot(ncaa[,3:14],groups,color=TRUE,shade=TRUE,labels=2,lines=0)\n17.4 Prediction Trees\nPrediction trees are a natural outcome of recursive partitioning of the\ndata. Hence, they are a particular form of clustering at different levels.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 436,
      "chunk_index": 4
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 437\n051\n001\n05\n0\n41 32 1 42 2 7 61 33 5 4 3 72\n94\n93 35 8 11 51 34 74 26 83 14 16 36 25 06 24 55 53 01 04 45 6 71 43 64 92 02 22 73 91 44 95 63 54 85 46 31 05 65 81 15 03 13 52 82\n84\n62 23 12 75 9 21\nNCAA Teams\nd\nhclust (*, \"ward\")\nthgieH\nFigure17.5: NCAAdata,hierarchi-\ncalclusterexample.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 437,
      "chunk_index": 0
    }
  },
  {
    "text": "438 data science: theories, models, algorithms, and analytics\n-4 -2 0 2 4\n3\n2\n1\n0\n1-\n2-\n3-\nCLUSPLOT( ncaa[, 3:14] )\nComponent 1\n2 tnenopmoC\nFigure17.6: NCAAdata,hierarchi-\ncalclusterexamplewithclusterson\n1\n4 3 thetoptwoprincipalcomponents.\n31 39\n62 60\n41\n48 13\n18 7 5 1\n38 5473 47 555296 32 5 5 8 0 2 5 5 1 6 2 64 35 2 2 3 8 0 5 3 3 3 3 6 62 2 1 4 7 6 2 3 3\n37 5 1 1 2 295 4 14 2\n44 8\n42\n4405\n4634\n1151\n1720\n29\n63\n19 1202\n54\n61 49\nThese two components explain 42.57 % of the point variability.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 438,
      "chunk_index": 0
    }
  },
  {
    "text": "Usual cluster analysis results in a “flat” partition, but prediction trees\ndevelop a multi-level cluster of trees. The term used here is CART, which\nstands for classification analysis and regression trees. But prediction\ntrees are different from vanilla clustering in an important way – there is\na dependent variable, i.e., a category or a range of values (e.g., a score)\nthat one is attempting to predict.\nPrediction trees are of two types: (a) Classification trees, where the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 438,
      "chunk_index": 1
    }
  },
  {
    "text": "leaves of the trees are different categories of discrete outcomes. and (b)\nRegression trees, where the leaves are continuous outcomes. We may\nthink of the former as a generalized form of limited dependent variables,\nand the latter as a generalized form of regression analysis.\nTo set ideas, suppose we want to predict the credit score of an individ-\nual using age, income, and education as explanatory variables. Assume\nthat income is the best explanatory variable of the three. Then, at the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 438,
      "chunk_index": 2
    }
  },
  {
    "text": "top of the tree, there will be income as the branching variable, i.e., if in-\ncome is less than some threshold, then we go down the left branch of the\ntree, else we go down the right. At the next level, it may be that we use\neducation to make the next bifurcation, and then at the third level we\nuse age. A variable may even be repeatedly used at more than one level.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 438,
      "chunk_index": 3
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 439\nThis leads us to several leaves at the bottom of the tree that contain the\naverage values of the credit scores that may be reached. For example if\nwe get an individual of young age, low income, and no education, it is\nvery likely that this path down the tree will lead to a low credit score on\naverage. Instead of credit score (an example of a regression tree), con-\nsider credit ratings of companies (an example of a classification tree).",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 439,
      "chunk_index": 0
    }
  },
  {
    "text": "These ideas will become clearer once we present some examples.\nRecursive partitioning is the main algorithmic construct behind pre-\ndiction trees. We take the data and using a single explanatory variable,\nwe try and bifurcate the data into two categories such that the additional\ninformation from categorization results in better “information” than be-\nfore the binary split. For example, suppose we are trying to predict who\nwill make donations and who will not using a single variable – income.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 439,
      "chunk_index": 1
    }
  },
  {
    "text": "If we have a sample of people and have not yet analyzed their incomes,\nwe only have the raw frequency p of how many people made donations,\n0 1\ni.e., and number between and . The “information” of the predicted\nlikelihood p is inversely related to the sum of squared errors (SSE) be-\ntween this value p and the 0 values and 1 values of the observations.\nn\nSSE = ∑ (x p)2\n1 i\n−\ni=1\nwhere x = 0,1 , depending on whether person i made a donation\ni\n{ }",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 439,
      "chunk_index": 2
    }
  },
  {
    "text": "i\n{ }\nor not. Now, if we bifurcate the sample based on income, say to the left\nwe have people with income less than K, and to the right, people with\nincomes greater than or equal to K. If we find that the proportion of\npeople on the left making donations is p < p and on the right is p >\nL R\np, our new information is:\nSSE = ∑ (x p )2+ ∑ (x p )2\n2 i L i R\n− −\ni,Income<K i,Income K\n≥\nBy choosing K correctly, our recursive partitioning algorithm will maxi-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 439,
      "chunk_index": 3
    }
  },
  {
    "text": "mize the gain, i.e., δ = (SSE SSE ). We stop branching further when\n1 2\n−\nat a given tree level δ is less than a pre-specified threshold.\nWe note that as n gets large, the computation of binary splits on any\nvariable is expensive, i.e., of order (2n). But as we go down the tree,\nO\nand use smaller subsamples, the algorithm becomes faster and faster. In\ngeneral, this is quite an efficient algorithm to implement.\nThe motivation of prediction trees is to emulate a decision tree. It also",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 439,
      "chunk_index": 4
    }
  },
  {
    "text": "helps make sense of complicated regression scenarios where there are",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 439,
      "chunk_index": 5
    }
  },
  {
    "text": "440 data science: theories, models, algorithms, and analytics\nlots of variable interactions over many variables, when it becomes dif-\nficult to interpret the meaning and importance of explanatory variables\nin a prediction scenario. By proceeding in a hierarchical manner on a\ntree, the decision analysis becomes transparent, and can also be used in\npractical settings to make decisions.\n17.4.1 Classification Trees\nTo demonstrate this, let’s use a data set that is already in R. We use the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 440,
      "chunk_index": 0
    }
  },
  {
    "text": "kyphosis data set which contains data on children who have had spinal\nsurgery. The model we wish to fit is to predict whether a child has a\npost-operative deformity or not (variable: Kyphosis = {absent, present}).\nThe variables we use are Age in months, number of vertebrae operated\non (Number), and the beginning of the range of vertebrae operated on\n(Start). The package used is called rpart which stands for “recursive\npartitioning”.\n> library(rpart)\n> data(kyphosis)\n> head(kyphosis)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 440,
      "chunk_index": 1
    }
  },
  {
    "text": "> data(kyphosis)\n> head(kyphosis)\nKyphosis Age Number Start\n1 absent 71 3 5\n2 absent 158 3 14\n3 present 128 4 5\n4 absent 2 5 1\n5 absent 1 4 15\n6 absent 1 2 16\n> fit = rpart(Kyphosis~Age+Number+Start , method=\"class\", data=kyphosis)\n>\n> printcp(fit)\nClassification tree:\nrpart(formula = Kyphosis ~ Age + Number + Start , data = kyphosis,\nmethod = \"class\")\nVariables actually used in tree construction:\n[1] Age Start\nRoot node error: 17/81 = 0.20988\nn= 81\nCP nsplit rel error xerror xstd",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 440,
      "chunk_index": 2
    }
  },
  {
    "text": "n= 81\nCP nsplit rel error xerror xstd\n1 0.176471 0 1.00000 1.0000 0.21559\n2 0.019608 1 0.82353 1.1765 0.22829\n3 0.010000 4 0.76471 1.1765 0.22829\nWe can now get a detailed summary of the analysis as follows:\n> summary(fit)\nCall:\nrpart(formula = Kyphosis ~ Age + Number + Start , data = kyphosis,\nmethod = \"class\")\nn= 81",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 440,
      "chunk_index": 3
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 441\nCP nsplit rel error xerror xstd\n1 0.17647059 0 1.0000000 1.000000 0.2155872\n2 0.01960784 1 0.8235294 1.176471 0.2282908\n3 0.01000000 4 0.7647059 1.176471 0.2282908\nNode number 1: 81 observations , complexity param=0.1764706\npredicted class=absent expected loss=0.2098765\nclass counts: 64 17\nprobabilities: 0.790 0.210\nleft son=2 (62 obs) right son=3 (19 obs)\nPrimary splits:\nStart < 8.5 to the right , improve=6.762330, (0 missing)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 441,
      "chunk_index": 0
    }
  },
  {
    "text": "Number < 5.5 to the left , improve=2.866795, (0 missing)\nAge < 39.5 to the left , improve=2.250212, (0 missing)\nSurrogate splits:\nNumber < 6.5 to the left , agree=0.802, adj=0.158, (0 split)\nNode number 2: 62 observations , complexity param=0.01960784\npredicted class=absent expected loss=0.09677419\nclass counts: 56 6\nprobabilities: 0.903 0.097\nleft son=4 (29 obs) right son=5 (33 obs)\nPrimary splits:\nStart < 14.5 to the right , improve=1.0205280, (0 missing)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 441,
      "chunk_index": 1
    }
  },
  {
    "text": "Age < 55 to the left , improve=0.6848635, (0 missing)\nNumber < 4.5 to the left , improve=0.2975332, (0 missing)\nSurrogate splits:\nNumber < 3.5 to the left , agree=0.645, adj=0.241, (0 split)\nAge < 16 to the left , agree=0.597, adj=0.138, (0 split)\nNode number 3: 19 observations\npredicted class=present expected loss=0.4210526\nclass counts: 8 11\nprobabilities: 0.421 0.579\nNode number 4: 29 observations\npredicted class=absent expected loss=0\nclass counts: 29 0\nprobabilities: 1.000 0.000",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 441,
      "chunk_index": 2
    }
  },
  {
    "text": "class counts: 29 0\nprobabilities: 1.000 0.000\nNode number 5: 33 observations , complexity param=0.01960784\npredicted class=absent expected loss=0.1818182\nclass counts: 27 6\nprobabilities: 0.818 0.182\nleft son=10 (12 obs) right son=11 (21 obs)\nPrimary splits:\nAge < 55 to the left , improve=1.2467530, (0 missing)\nStart < 12.5 to the right , improve=0.2887701, (0 missing)\nNumber < 3.5 to the right , improve=0.1753247, (0 missing)\nSurrogate splits:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 441,
      "chunk_index": 3
    }
  },
  {
    "text": "Surrogate splits:\nStart < 9.5 to the left , agree=0.758, adj=0.333, (0 split)\nNumber < 5.5 to the right , agree=0.697, adj=0.167, (0 split)\nNode number 10: 12 observations\npredicted class=absent expected loss=0\nclass counts: 12 0\nprobabilities: 1.000 0.000\nNode number 11: 21 observations , complexity param=0.01960784\npredicted class=absent expected loss=0.2857143",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 441,
      "chunk_index": 4
    }
  },
  {
    "text": "442 data science: theories, models, algorithms, and analytics\nclass counts: 15 6\nprobabilities: 0.714 0.286\nleft son=22 (14 obs) right son=23 (7 obs)\nPrimary splits:\nAge < 111 to the right , improve=1.71428600, (0 missing)\nStart < 12.5 to the right , improve=0.79365080, (0 missing)\nNumber < 3.5 to the right , improve=0.07142857, (0 missing)\nNode number 22: 14 observations\npredicted class=absent expected loss=0.1428571\nclass counts: 12 2\nprobabilities: 0.857 0.143\nNode number 23: 7 observations",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 442,
      "chunk_index": 0
    }
  },
  {
    "text": "Node number 23: 7 observations\npredicted class=present expected loss=0.4285714\nclass counts: 3 4\nprobabilities: 0.429 0.571\n177\nWe can plot the tree as well using the plot command. See Figure . .\nThe dendrogram like tree shows the allocation of the n = 81 cases to\nvarious branches of the tree.\n> plot( fit , uniform=TRUE)\n> text ( fit , use.n=TRUE, all=TRUE, cex= 0 . 8 )\n17.4.2 The C4.5 Classifier\nThis is one of the top algorithms of data science. This classifier also fol-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 442,
      "chunk_index": 1
    }
  },
  {
    "text": "lows recursive partitioning as in the previous case, but instead of min-\nimizing the sum of squared errors between the sample data x and the\ntrue value p at each level, here the goal is to minimize entropy. This im-\nproves the information gain. Natural entropy (H) of the data x is defined\nas\nH = ∑ f(x) ln f(x) ( 17 . 1 )\n− ·\nx\nwhere f(x) is the probability density of x. This is intuitive because\nafter the optimal split in recursing down the tree, the distribution of x",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 442,
      "chunk_index": 2
    }
  },
  {
    "text": "becomes narrower, lowering entropy. This measure is also often known\nas “differential entropy.”\nTo see this let’s do a quick example. We compute entropy for two\ndistributions of varying spread (standard deviation).\n0 001\ndx = .\nx = seq( 5 , 5 ,dx)\n−\nH 2 = sum(dnorm(x,sd= 2 )*log(dnorm(x,sd= 2 ))*dx)\n−\nprint(H 2 )",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 442,
      "chunk_index": 3
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 443\nStart>=8.5 Figure17.7: Classificationtreefor\n|\nabsent\nthekyphosisdataset.\n64/17\nStart>=14.5\nabsent present\n56/6 8/11\nAge< 55\nabsent absent\n29/0 27/6\nAge>=111\nabsent absent\n12/0 15/6\nabsent present\n12/2 3/4",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 443,
      "chunk_index": 0
    }
  },
  {
    "text": "444 data science: theories, models, algorithms, and analytics\nH 3 = sum(dnorm(x,sd= 3 )*log(dnorm(x,sd= 3 ))*dx)\n−\nprint(H 3 )\n1 2 042076\n[ ] .\n1 2 111239\n[ ] .\nTherefore, we see that entropy increases as the normal distribution be-\n45\ncomes wider. Now, let’s use the C . classifier on the iris data set. The\nclassifier resides in the RWeka package.\nlibrary(RWeka)\ndata( iris )\nprint(head( iris ))\nres = J 48 (Species~. ,data=iris )\nprint(res)\nsummary(res)\nThe output is as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 444,
      "chunk_index": 0
    }
  },
  {
    "text": "print(res)\nsummary(res)\nThe output is as follows:\nSepal .Length Sepal .Width Petal .Length Petal .Width Species\n1 5 1 3 5 1 4 0 2\n. . . . setosa\n2 4 9 3 0 1 4 0 2\n. . . . setosa\n3 4 7 3 2 1 3 0 2\n. . . . setosa\n4 4 6 3 1 1 5 0 2\n. . . . setosa\n5 5 0 3 6 1 4 0 2\n. . . . setosa\n6 5 4 3 9 1 7 0 4\n. . . . setosa\n48\nJ pruned tree\n−−−−−−−−−−−−−−−−−−\n0 6 50 0\nPetal .Width <= . : setosa ( . )\n0 6\nPetal .Width > .\n1 7\n| Petal .Width <= .\n| | Petal .Length <= 4 . 9 : versicolor ( 48 . 0 / 1 . 0 )\n4 9",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 444,
      "chunk_index": 1
    }
  },
  {
    "text": "4 9\n| | Petal .Length > .\n1 5 3 0\n| | | Petal .Width <= . : virginica ( . )\n| | | Petal .Width > 1 . 5 : versicolor ( 3 . 0 / 1 . 0 )\n| Petal .Width > 1 . 7 : virginica ( 46 . 0 / 1 . 0 )\n5\nNumber of Leaves :\n9\nSize of the tree :",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 444,
      "chunk_index": 2
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 445\n=== Summary ===\n147 98\nCorrectly Classified Instances %\n3 2\nIncorrectly Classified Instances %\n0 97\nKappa statistic .\n0 0233\nMean absolute error .\nRoot mean squared error 0 . 108\n5 2482\nRelative absolute error . %\n22 9089\nRoot relative squared error . %\n0 95 98 6667\nCoverage of cases ( . level ) . %\n0 95 34\nMean rel . region size ( . level ) %\n150\nTotal Number of Instances\n=== Confusion Matrix ===\na b c < classified as\n−−\n50 0 0",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 445,
      "chunk_index": 0
    }
  },
  {
    "text": "a b c < classified as\n−−\n50 0 0\n| a = setosa\n0 49 1\n| b = versicolor\n0 2 48 | c = virginica\n17.5 Regression Trees\nWe move from classification trees (discrete outcomes) to regression trees\n(scored or continuous outcomes). Again, we use an example that already\nexists in R, i.e., the cars dataset in the cu.summary data frame. Let’s load\nit up.\n> data(cu.summary)\n> names(cu.summary)\n1\n[ ] \"Price\" \"Country\" \" Reliability \" \"Mileage\" \"Type\"\n> head(cu.summary)\nPrice Country Reliability Mileage Type",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 445,
      "chunk_index": 1
    }
  },
  {
    "text": "Price Country Reliability Mileage Type\n4 11950\nAcura Integra Japan Much better NA Small\n4 6851\nDodge Colt Japan <NA> NA Small\n4 6995\nDodge Omni USA Much worse NA Small\n4 8895 33\nEagle Summit USA better Small\n4 7402 33\nFord Escort USA worse Small",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 445,
      "chunk_index": 2
    }
  },
  {
    "text": "446 data science: theories, models, algorithms, and analytics\n4 6319 37\nFord Festiva Korea better Small\n> dim(cu.summary)\n1 117 5\n[ ]\nWe see that the variables are self-explanatory. See that in some cases,\nthere are missing (< NA >) values in the Reliability variable. We will\ntry and predict Mileage using the other variables. (Note: if we tried to\npredict Reliability, then we would be back in the realm of classification\ntrees, here we are looking at regression trees.)\n> library(rpart)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 446,
      "chunk_index": 0
    }
  },
  {
    "text": "> library(rpart)\n> fit < rpart(Mileage~Price + Country + Reliability + Type,\n−\nmethod=\"anova\", data=cu.summary)\n> summary(fit)\nCall:\nrpart(formula = Mileage ~ Price + Country + Reliability + Type,\ndata = cu.summary, method = \"anova\")\nn=60 (57 observations deleted due to missingness)\nCP nsplit rel error xerror xstd\n1 0.62288527 0 1.0000000 1.0322810 0.17522180\n2 0.13206061 1 0.3771147 0.5305328 0.10329174\n3 0.02544094 2 0.2450541 0.3790878 0.08392992\n4 0.01160389 3 0.2196132 0.3738624 0.08489026",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 446,
      "chunk_index": 1
    }
  },
  {
    "text": "4 0.01160389 3 0.2196132 0.3738624 0.08489026\n5 0.01000000 4 0.2080093 0.3985025 0.08895493\nNode number 1: 60 observations , complexity param=0.6228853\nmean=24.58333, MSE=22.57639\nleft son=2 (48 obs) right son=3 (12 obs)\nPrimary splits:\nPrice < 9446.5 to the right , improve=0.6228853, (0 missing)\nType splits as LLLRLL, improve=0.5044405, (0 missing)\nReliability splits as LLLRR, improve=0.1263005, (11 missing)\nCountry splits as LRLRRRLL, improve=0.1243525, (0 missing)\n−−\nSurrogate splits:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 446,
      "chunk_index": 2
    }
  },
  {
    "text": "−−\nSurrogate splits:\nType splits as LLLRLL, agree=0.950, adj=0.750, (0 split)\nCountry splits as LLLLRRLL, agree=0.833, adj=0.167, (0 split)\n−−\nNode number 2: 48 observations , complexity param=0.1320606\nmean=22.70833, MSE=8.498264\nleft son=4 (23 obs) right son=5 (25 obs)\nPrimary splits:\nType splits as RLLRRL, improve=0.43853830, (0 missing)\nPrice < 12154.5 to the right , improve=0.25748500, (0 missing)\nCountry splits as RRLRL LL, improve=0.13345700, (0 missing)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 446,
      "chunk_index": 3
    }
  },
  {
    "text": "Reliability splits as − L − LLRR, − improve=0.01637086, (10 missing)\nSurrogate splits:\nPrice < 12215.5 to the right , agree=0.812, adj=0.609, (0 split)\nCountry splits as RRLRL RL, agree=0.646, adj=0.261, (0 split)\n−− −\nNode number 3: 12 observations\nmean=32.08333, MSE=8.576389\nNode number 4: 23 observations , complexity param=0.02544094\nmean=20.69565, MSE=2.907372\nleft son=8 (10 obs) right son=9 (13 obs)\nPrimary splits:\nType splits as LR L, improve=0.515359600, (0 missing)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 446,
      "chunk_index": 4
    }
  },
  {
    "text": "Price < 14962 t − o t − h − e left , improve=0.131259400, (0 missing)\nCountry splits as L R R, improve=0.007022107, (0 missing)\n−−−− − −−",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 446,
      "chunk_index": 5
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 447\nSurrogate splits:\nPrice < 13572 to the right , agree=0.609, adj=0.1, (0 split)\nNode number 5: 25 observations , complexity param=0.01160389\nmean=24.56, MSE=6.4864\nleft son=10 (14 obs) right son=11 (11 obs)\nPrimary splits:\nPrice < 11484.5 to the right , improve=0.09693168, (0 missing)\nReliability splits as LLRRR, improve=0.07767167, (4 missing)\nType splits as L RR , improve=0.04209834, (0 missing)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 447,
      "chunk_index": 0
    }
  },
  {
    "text": "Country splits as −− LRRR − LL, improve=0.02201687, (0 missing)\n−− −−\nSurrogate splits:\nCountry splits as LLLL LR, agree=0.80, adj=0.545, (0 split)\nType splits as − L − RL − , − agree=0.64, adj=0.182, (0 split)\n−− −\nNode number 8: 10 observations\nmean=19.3, MSE=2.21\nNode number 9: 13 observations\nmean=21.76923, MSE=0.7928994\nNode number 10: 14 observations\nmean=23.85714, MSE=7.693878\nNode number 11: 11 observations\nmean=25.45455, MSE=3.520661\nWe may then plot the results, as follows:",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 447,
      "chunk_index": 1
    }
  },
  {
    "text": "We may then plot the results, as follows:\n> plot( fit , uniform=TRUE)\n> text ( fit , use.n=TRUE, all=TRUE, cex=. 8 )\n178\nThe result is shown in Figure . .\n17.5.1 Example: Califonia Home Data\nThis example is taken from a data set posted by Cosmo Shalizi at CMU.\nWe use a different package here, called tree, though this has been sub-\nsumed in most of its functionality by rpart used earlier. The analysis is\nas follows:\n> library(tree)\n> cahomes = read.table(\"cahomedata.txt\",header=TRUE)",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 447,
      "chunk_index": 2
    }
  },
  {
    "text": "> fit = tree(log(MedianHouseValue)~Longitude+Latitude , data=cahomes)\n> plot(fit)\n> text(fit ,cex=0.8)\nThis predicts housing values from just latitude and longitude coordi-\n179\nnates. The prediction tree is shown in Figure . .\nFurther analysis goes as follows:\n> price.deciles = quantile(cahomes$MedianHouseValue,0:10/10)\n> cut.prices = cut(cahomes$MedianHouseValue,price.deciles ,include.lowest=TRUE)\n> plot(cahomes$Longitude, cahomes$Latitude ,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 447,
      "chunk_index": 3
    }
  },
  {
    "text": "> plot(cahomes$Longitude, cahomes$Latitude ,\ncol=grey(10:2/11)[cut.prices],pch=20,xlab=\"Longitude\",ylab=\"Latitude\")\n> partition.tree(fit ,ordvars=c(\"Longitude\",\"Latitude\"),add=TRUE)\n1710\nThe plot of the output and the partitions is given in Figure . .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 447,
      "chunk_index": 4
    }
  },
  {
    "text": "448 data science: theories, models, algorithms, and analytics\nPrice>=9446 Figure17.8: Predictiontreeforcars\n24|.58\nmileage.\nn=60\nType=bcf\n22.71 32.08\nn=48 n=12\nType=bf Price>=1.148e+04\n20.7 24.56\nn=23 n=25\n19.3 21.77 23.86 25.45\nn=10 n=13 n=14 n=11\nLatitude | < 38.485 Figure17.9: Californiahomeprices\npredictiontree.\nLongitude < -121.655 Latitude < 39.355\n11.73 11.32\nLatitude < 37.925 Latitude < 34.675\n12.48 12.10\nLongitude < -118.315 Longitude < -120.275\n11.75 11.28\nLongitude < -117.545\n12.53",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 448,
      "chunk_index": 0
    }
  },
  {
    "text": "11.75 11.28\nLongitude < -117.545\n12.53\nLatitude < 33.725 Latitude < 33.59\nLongitude < -116.33\n12.54 12.14 11.63\n12.09 11.16",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 448,
      "chunk_index": 1
    }
  },
  {
    "text": "in the same boat: cluster analysis and prediction trees 449\n-124 -122 -120 -118 -116 -114\n24\n04\n83\n63\n43\nLongitude\nedutitaL\nFigure17.10: Californiahomeprices\npartitiondiagram.\n11.3\n11.7\n12.1\n11.8 11.3\n12.5\n12.1 11.6\n12.5\n12.5 12.1 11.2",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 449,
      "chunk_index": 0
    }
  },
  {
    "text": "450 data science: theories, models, algorithms, and analytics",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 450,
      "chunk_index": 0
    }
  },
  {
    "text": "18\nBibliography\n2001\nA. Admati, and P. Pfleiderer ( ). “Noisytalk.com: Broadcasting\nOpinions in a Noisy Environment,” working paper, Stanford Univer-\nsity.\n2006\nAggarwal, Gagan., Ashish Goel, and Rajeev Motwani ( ). “Truth-\nful Auctions for Price Searching Keywords,” Working paper, Stanford\nUniversity.\n2003\nAndersen, Leif., Jakob Sidenius, and Susanta Basu ( ). All your\nhedges in one basket, Risk, November, 67 - 72 .\n2004\nW. Antweiler and M. Frank ( ). “Is all that Talk just Noise? The In-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 451,
      "chunk_index": 0
    }
  },
  {
    "text": "formation Content of Internet Stock Message Boards,” Journal of Finance,\n59 3 1259 1295\nv ( ), - .\n2005\nW. Antweiler and M. Frank ( ). “The Market Impact of Corporate\nNews Stories,” Working paper, University of British Columbia.\n1999\nArtzner, A., F. Delbaen., J-M. Eber., D. Heath, ( ). “Coherent Mea-\nsures of Risk,” Mathematical Finance 9 ( 3 ), 203 – 228 .\n2007\nAshcraft, Adam., and Darrell Duffie ( ). “Systemic Illiquidity in the",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 451,
      "chunk_index": 1
    }
  },
  {
    "text": "Federal Funds Market,” American Economic Review, Papers and Proceed-\nings 97 , 221 - 225 .\n1999\nBarabasi, A.-L.; R. Albert ( ). “Emergence of scaling in random\nnetworks,” Science 286 ( 5439 ), 509 – 512 . arXiv:cond-mat/ 9910332 .\n101126 2865439509 10521342\ndoi: . /science. . . . PMID .\n2003\nBarabasi, Albert-Laszlo., and Eric Bonabeau ( ). “Scale-Free Net-\nworks,” Scientific American May, 50 – 59 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 451,
      "chunk_index": 2
    }
  },
  {
    "text": "452 data science: theories, models, algorithms, and analytics\n1969\nBass, Frank. ( ). “A New Product Growth Model for Consumer\nDurables,” Management Science 16 , 215 – 227 .\n1994\nBass, Frank., Trichy Krishnan, and Dipak Jain ( ). “Why the Bass\nModel Without Decision Variables,” Marketing Science 13 , 204 – 223 .\n2010\nBengtsson, O., Hsu, D. ( ). How do venture capital partners match\nwith startup founders? Working Paper.\n2012\nBillio, M., Getmansky, M., Lo. A., Pelizzon, L. ( ). “Econometric",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 452,
      "chunk_index": 0
    }
  },
  {
    "text": "Measures of Connectedness and Systemic Risk in the Finance and In-\nsurance Sectors,” Journal of Financial Economics 104 ( 3 ), 536 – 559 .\nBillio, M., Getmansky, M., Gray, D., Lo. A., Merton, R., Pelizzon, L.\n2012\n( ). “Sovereign, Bank and Insurance Credit Spreads: Connectedness\nand System Networks,” Working paper, IMF.\n1995\nBishop, C. ( ). “Neural Networks for Pattern Recognition,” Oxford\nUniversity Press, New York.\n2003\nBoatwright, Lee., and Wagner Kamakura ( ). “Bayesian Model for",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 452,
      "chunk_index": 1
    }
  },
  {
    "text": "Prelaunch Sales Forecasting of Recorded Music,” Management Science\n49 2 179 196\n( ), – .\n1972\nP. Bonacich ( ). “Technique for analyzing overlappingmemberships,”\nSociological Methodology 4 , 176 - 185 .\nP. Bonacich ( 1987 ). “Power and centrality: a family of measures,” Ameri-\ncan Journal of Sociology 92 ( 5 ), 1170 - 1182 .\n2011\nBottazzi, L., Da Rin, M., Hellmann, T. ( ). The importance of trust\nfor investment: Evidence from venture capital. Working Paper.\n2002",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 452,
      "chunk_index": 2
    }
  },
  {
    "text": "2002\nBrander, J. A., Amit, R., Antweiler, W. ( ). Venture-capital syndi-\ncation: Improved venture selection vs. the value-added hypothesis,\nJournal of Economics and Management Strategy, v 11 , 423 - 452 .\n1996\nBrowne, Sid., and Ward Whitt ( ). “Portfolio Choice and the\nBayesian Kelly Criterion,” Advances in Applied Probability 28 ( 4 ), 1145 –\n1176\n.\nBurdick, D., Hernandez, M., Ho, H., Koutrika, G., Krishnamurthy,\n2011\nR., Popa, L., Stanoi, I.R., Vaithyanathan, S., Das, S.R. ( ). Extract-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 452,
      "chunk_index": 3
    }
  },
  {
    "text": "ing, linking and integrating data from public sources: A financial case\nstudy, IEEE Data Engineering Bulletin, 34 ( 3 ), 60 - 67 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 452,
      "chunk_index": 4
    }
  },
  {
    "text": "bibliography 453\n2012\nCai, Y., and Sevilir, M. ( ). Board connections and M&A Transac-\ntions, Journal of Financial Economics 103 ( 2 ), 327 - 349 .\n2006\nCestone, Giacinta., Lerner, Josh, White, Lucy ( ). The design of com-\nmunicates in venture capital, Harvard Business School Working Paper.\n1998\nChakrabarti, S., B. Dom, R. Agrawal, and P. Raghavan. ( ). “Scalable\nfeature selection, classification and signature generation for organiz-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 453,
      "chunk_index": 0
    }
  },
  {
    "text": "ing large text databases into hierarchical topic taxonomies,” The VLDB\nJournal, Springer-Verlag.\n2010\nChidambaran, N. K., Kedia, S., Prabhala, N.R. ( ). CEO-Director\nconnections and fraud, University of Maryland Working Paper.\nCochrane, John ( 2005 ). The risk and return of venture capital. Journal of\nFinancial Economics 75 , 3 - 52 .\n2008\nCohen, Lauren, Frazzini, Andrea, Malloy, Christopher ( a). The\nsmall world of investing: Board connections and mutual fund returns,",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 453,
      "chunk_index": 1
    }
  },
  {
    "text": "Journal of Political Economy 116 , 951 – 979 .\n2008\nCohen, Lauren, Frazzini, Andrea, Malloy, Christopher ( b). Sell-Side\nschool ties, forthcoming, Journal of Finance.\n1975\nM. Coleman and T. L. Liau. ( ). A computer readability formula\ndesigned for machine scoring. Journal of Applied Psychology 60 , 283 – 284 .\n2009\nCormen, Thomas., Charles Leiserson, and Ronald Rivest ( ). Intro-\nduction to Algorithms, MIT Press, Cambridge, Massachusetts.\n2003",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 453,
      "chunk_index": 2
    }
  },
  {
    "text": "2003\nCornelli, F., Yosha, O. ( ). Stage financing and the role of convertible\nsecurities, Review of Economic Studies 70 , 1 - 32 .\n2012\nDa Rin, Marco, Hellmann, Thomas, Puri, Manju ( ). A survey of\nventure capital research, Duke University Working Paper.\n2014\nDas, Sanjiv., ( ). “Text and Context: Language Analytics for Fi-\nnance,” Foundations and Trends in Finance v 8 ( 3 ), 145 - 260 .\n2014\nDas, Sanjiv., ( ). “Matrix Math: Network-Based Systemic Risk Scor-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 453,
      "chunk_index": 3
    }
  },
  {
    "text": "ing,” forthcoming Journal of Alternative Investments.\n2003\nDas, Sanjiv., Murali Jagannathan, and Atulya Sarin ( ). Private Eq-\nuity Returns: An Empirical Examination of the Exit of asset-Backed\nCompanies, Journal of Investment Management 1 ( 1 ), 152 - 177 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 453,
      "chunk_index": 4
    }
  },
  {
    "text": "454 data science: theories, models, algorithms, and analytics\nDas, Sanjiv., and Jacob Sisk ( 2005 ). “Financial Communities,” Journal of\nPortfolio Management 31 ( 4 ), 112 - 123 .\n2007\nS. Das and M. Chen ( ). “Yahoo for Amazon! Sentiment Extraction\nfrom Small Talk on the Web,” Management Science 53 , 1375 - 1388 .\n2005\nS. Das, A. Martinez-Jerez, and P. Tufano ( ). “eInformation: A Clini-\ncal Study of Investor Discussion and Sentiment,” Financial Management\n34 5 103 137\n( ), - .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 454,
      "chunk_index": 0
    }
  },
  {
    "text": "34 5 103 137\n( ), - .\nS. Das and J. Sisk ( 2005 ). “Financial Communities,” Journal of Portfolio\nManagement 31 ( 4 ), 112 - 123 .\n1996\nDas, Sanjiv., and Rangarajan Sundaram ( ). “Auction Theory: A\nSummary with Applications and Evidence from the Treasury Markets,”\nFinancial Markets, Institutions and Instruments v 5 ( 5 ), 1 – 36 .\n2011\nDas, Sanjiv., Jo, Hoje, Kim, Yongtae ( ). Polishing diamonds in the\nrough: The sources of communicated venture performance, Journal of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 454,
      "chunk_index": 1
    }
  },
  {
    "text": "Financial Intermediation, 20 ( 2 ), 199 - 230 .\n2004\nDean, Jeffrey., and Sanjay Ghemaway ( ). “MapReduce: Simplified\n04\nData Processing on Large Clusters,” OSDI’ : Sixth Symposium on\nOperating System Design and Implementation.\n2003\nP. DeMarzo, D. Vayanos, and J. Zwiebel ( ). “Persuasion Bias, So-\ncial Influence, and Uni-Dimensional Opinions,” Quarterly Journal of\nEconomics 118 , 909 - 968 .\n2011\nDu, Qianqian ( ). Birds of a feather or celebrating differences?",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 454,
      "chunk_index": 2
    }
  },
  {
    "text": "The formation and impact of venture capital syndication, University\nof British Columbia Working Paper.\n2001\nJ. Edwards., K. McCurley, and J. Tomlin ( ). “An Adaptive Model for\nOptimizing Performance of an Incremental Web Crawler,” Proceedings\n10 106 113\nWWW , Hong Kong, - .\n1995\nG. Ellison, and D. Fudenberg ( ). “Word of Mouth Communication\nand Social Learning,” Quarterly Journal of Economics 110 , 93 - 126 .\n2000\nEngelberg, Joseph., Gao, Pengjie, Parsons, Christopher ( ). The value",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 454,
      "chunk_index": 3
    }
  },
  {
    "text": "of a rolodex: CEO pay and personal networks, Working Paper, Univer-\nsity of North Carolina at Chapel Hill.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 454,
      "chunk_index": 4
    }
  },
  {
    "text": "bibliography 455\nFortunato, S. ( 2009 ). Community detection in graphs, arXiv: 0906 . 0612 v 1\n[physics.soc-ph].\nS. Fortunato ( 2010 ). “Community Detection in Graphs,” Physics Reports\n486 75 174\n, - .\n1995\nGertler, M.S. ( ). Being there: proximity, organization and culture in\nthe development and adoption of advanced manufacturing technolo-\ngies, Economic Geography 7 ( 1 ), 1 - 26 .\n2005\nGhiassi, M., H. Saidane, and D. Zimbra ( ). “A dynamic artificial",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 455,
      "chunk_index": 0
    }
  },
  {
    "text": "neural network model for forecasting time series events,” International\nJournal of Forecasting 21 , 341 – 362 .\nGinsburg, Jeremy., Matthew Mohebbi, Rajan Patel, Lynnette Brammer,\n2009\nMark Smolinski, and Larry Brilliant ( ). “Detecting Influenza Epi-\ndemics using Search Engine Data,” Nature 457 , 1012 – 1014 .\n2002\nGirvan, M., Newman, M. ( ). Community structure in social and\nbiological networks, Proc. of the National Academy of Science 99 ( 12 ), 7821 –\n7826\n.\n2010",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 455,
      "chunk_index": 1
    }
  },
  {
    "text": "7826\n.\n2010\nGlaeser, E., ed., ( ). Agglomeration Economics, University of\nChicago Press.\nD. Godes, D. Mayzlin, Y. Chen, S. Das, C. Dellarocas, B. Pfeieffer, B.\nLibai, S. Sen, M. Shi, and P. Verlegh. “The Firm’s Management of Social\nInteractions,” Marketing Letters v 16 , 415 - 428 .\n2004\nGodes, David., and Dina Mayzlin ( ). “Using Online Conversations\nto Study Word of Mouth Communication” Marketing Science 23 ( 4 ), 545 –\n560\n.\n2009",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 455,
      "chunk_index": 2
    }
  },
  {
    "text": "560\n.\n2009\nGodes, David., and Dina Mayzlin ( ). “Firm-Created Word-of-Mouth\nCommunication: Evidence from a Field Test”, Marketing Science 28 ( 4 ),\n721 739\n– .\n2007\nGoldfarb, Brent, Kirsch, David, Miller, David, ( ). Was there too little\nentry in the dot com era?, Journal of Financial Economics 86 ( 1 ), 100 - 144 .\n2000\nGompers, P., Lerner, J. ( ). Money chasing deals? The impact of fund\ninflows on private equity valuations, Journal of Financial Economics 55 ( 2 ),\n281 325\n- .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 455,
      "chunk_index": 3
    }
  },
  {
    "text": "456 data science: theories, models, algorithms, and analytics\nGompers, P., Lerner, J. ( 2001 ). The venture capital revolution, Journal of\nEconomic Perspectives 15 ( 2 ), 45 - 62 .\n2004\nGompers, P., Lerner, J., ( ). The Venture Capital Cycle, MIT Press.\nGorman, M., Sahlman, W. ( 1989 ). What do venture capitalists do? Jour-\nnal of Business Venturing 4 , 231 - 248 .\n2004\nP. Graham ( ). “Hackers and Painters,” O’Reilly Media, Sebastopol,\nCA.\n1969",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 456,
      "chunk_index": 0
    }
  },
  {
    "text": "CA.\n1969\nGranger, Clive, ( ). “Investigating Causal Relations by Econometric\nModels and Cross-spectral Methods\". Econometrica 37 ( 3 ), 424 – 438 .\n1985\nGranovetter, M. ( ). Economic action and social structure: The prob-\nlem of embeddedness, American Journal of Sociology 91 ( 3 ), 481 - 510 .\n2011 7\nGreene, William ( ). Econometric Analysis, th edition, Prentice-\nHall.\n1986\nGrossman, S., Hart, O. ( ). The costs and benefits of ownership: A",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 456,
      "chunk_index": 1
    }
  },
  {
    "text": "theory of vertical and lateral integration, Journal of Political Economy\n94 4 691 719\n( ), - .\n2005\nGuimera, R., Amaral, L.A.N. ( ). Functional cartography of complex\nmetabolic networks, Nature 433 , 895 - 900 .\n2005\nGuimera, R., Mossa, S., Turtschi, A., Amaral, L.A.N. ( ). The world-\nwide air transportation network: Anomalous centrality, community\nstructure, and cities’ global roles, Proceedings of the National Academy of\nScience 102 , 7794 - 7799 .\n2004",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 456,
      "chunk_index": 2
    }
  },
  {
    "text": "Science 102 , 7794 - 7799 .\n2004\nGuiso, L., Sapienza, P., Zingales, L. ( ). The role of social capital in\nfinancial development, American Economic Review 94 , 526 - 556 .\nR. Gunning. The Technique of Clear Writing. McGraw-Hill, 1952 .\n2009\nHalevy, Alon., Peter Norvig, and Fernando Pereira ( ). “The Unrea-\nsonable Effectiveness of Data,” IEEE Intelligent Systems March-April,\n8 12\n– .\n2007\nHarrison, D., Klein, K. ( ). What’s the difference? Diversity con-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 456,
      "chunk_index": 3
    }
  },
  {
    "text": "structs as separation, variety, or disparity in organization, Academy of\nManagement Review 32 ( 4 ), 1199 - 1228 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 456,
      "chunk_index": 4
    }
  },
  {
    "text": "bibliography 457\n1990\nHart, O., Moore, J. ( ). Property rights and the nature of the firm,\nJournal of Political Economy 98 ( 6 ), 1119 - 1158 .\n2011\nHegde, D., Tumlinson, J. ( ). Can birds of a feather fly together?\nEvidence for the economic payoffs of ethnic homophily, Working Paper.\n2008\nHellmann, T. J., Lindsey, L., Puri, M. ( ). Building relationships\nearly: Banks in venture capital, Review of Financial Studies 21 ( 2 ), 513 - 541 .\n2002",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 457,
      "chunk_index": 0
    }
  },
  {
    "text": "2002\nHellmann, T. J., Puri, M. ( ). Venture capital and the professionaliza-\ntion of start-up firms: Empirical evidence, Journal of Finance 57 , 169 - 197 .\n2010\nHoberg, G., Phillips, G. ( ). Product Market Synergies and Compe-\ntition in Mergers and Acquisitions: A Text-Based Analysis, Review of\nFinancial Studies 23 ( 10 ), 3773 – 3811 .\n2007\nHochberg, Y., Ljungqvist, A., Lu, Y. ( ). Whom You Know Matters:\nVenture Capital Networks and Investment Performance, Journal of Fi-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 457,
      "chunk_index": 1
    }
  },
  {
    "text": "nance 62 ( 1 ), 251 - 301 .\n2011\nHochberg, Y., Lindsey, L., Westerfield, M. ( ). Inter-firm Economic\nTies: Evidence from Venture Capital, Northwestern University Working\nPaper.\n1994\nHuchinson, J., Andrew Lo, and T. Poggio ( ). “A Non Parametric\nApproach to Pricing and Hedging Securities via Learning Networks,”\nJournal of Finance 49 ( 3 ), 851 – 889 .\nHwang, B., Kim, S. ( 2009 ). It pays to have friends, Journal of Financial\nEconomics 93 , 138 - 158 .\n2009",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 457,
      "chunk_index": 2
    }
  },
  {
    "text": "Economics 93 , 138 - 158 .\n2009\nIshii, J.L., Xuan, Y. ( ). Acquirer-Target social ties and merger out-\ncomes, Working Paper, SSRN: http://ssrn.com/abstract= 1361106 .\n1999\nT. Joachims ( ). “Making large-Scale SVM Learning Practical. Ad-\nvances in Kernel Methods - Support Vector Learning,” B. Scholkopf and\nC. Burges and A. Smola (ed.), MIT-Press.\n2003\nKanniainen, Vesa., and Christian Keuschnigg ( ). The optimal port-\nfolio of start-up firms in asset capital finance, Journal of Corporate Fi-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 457,
      "chunk_index": 3
    }
  },
  {
    "text": "nance 9 ( 5 ), 521 - 534 .\n2005\nKaplan, S. N., Schoar, A. ( ). Private equity performance: Returns,\npersistence and capital flows, Journal of Finance 60 , 1791 - 1823 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 457,
      "chunk_index": 4
    }
  },
  {
    "text": "458 data science: theories, models, algorithms, and analytics\n2002\nKaplan, S. N., Sensoy, B., Stromberg, P. ( ). How well do venture\ncapital databases reflect actual investments?, Working paper, University\nof Chicago.\n2003\nKaplan, S. N., Stromberg, P. ( ). Financial contracting theory meets\nthe real world: Evidence from venture capital contracts, Review of Eco-\nnomic Studies 70 , 281 - 316 .\n2004\nKaplan, S. N., Stromberg, P. ( ). Characteristics, contracts and ac-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 458,
      "chunk_index": 0
    }
  },
  {
    "text": "tions: Evidence from venture capital analyses, Journal of Finance 59 ,\n2177 2210\n- .\nKelly, J.L. ( 1956 ). “A New Interpretation of Information Rate,” The Bell\nSystem Technical Journal 35 , 917 – 926 .\n1997\nKoller, D., and M. Sahami ( ). “Hierarchically Classifying Doc-\numents using Very Few Words,” International Conference on Machine\nLearning, v 14 , Morgan-Kaufmann, San Mateo, California.\n2009\nD. Koller ( ). “Probabilistic Graphical Models,” MIT Press.\n2011",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 458,
      "chunk_index": 1
    }
  },
  {
    "text": "2011\nKrishnan, C. N. V., Masulis, R. W. ( ). Venture capital reputation, in\nDouglas J. Cummings, ed., Handbook on Entrepreneurial Finance, Venture\nCapital and Private Equity, Oxford University Press.\n2000\nLavinio, Stefano ( ). “The Hedge Fund Handbook,” Irwin Library of\nInvestment & Finance, McGraw-Hill..\n2010\nD. Leinweber., and J. Sisk ( ). “Relating News Analytics to Stock\nReturns,” mimeo, Leinweber & Co.\nLerner, J. ( 1994 ). The syndication of venture capital investments. Finan-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 458,
      "chunk_index": 2
    }
  },
  {
    "text": "cial Management 23 , 1627 .\n1995\nLerner, J. ( ). Venture capitalists and the oversight of private firms,\nJournal of Finance 50 ( 1 ), 302 - 318\n2010\nLeskovec, J., Kang, K.J., Mahoney, M.W. ( ). Empirical comparison of\nalgorithms for network community detection, ACM WWW International\nConference on World Wide Web.\nS. Levy ( 2010 ). “How Google’s Algorithm Rules the Web,” Wired,\nMarch.\n2006\nF. Li ( ). “Do Stock Market Investors Understand the RiskSentiment",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 458,
      "chunk_index": 3
    }
  },
  {
    "text": "of Corporate Annual Reports?” Working paper, University of Michigan.",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 458,
      "chunk_index": 4
    }
  },
  {
    "text": "bibliography 459\n2008\nLindsey, L. A. ( ). Blurring boundaries: The role of venture capital in\nstrategic alliances, Journal of Finance 63 ( 3 ), 1137 - 1168 .\n2006\nLossen, Ulrich ( ). The Performance of Private Equity Funds: Does\n192 15\nDiversification Matter?, Discussion Papers , SFB/TR , University\nof Munich.\n2014\nT. Loughran and W. McDonald, ( ). Measuring readability in finan-\ncial disclosures, The Journal of Finance 69 , 1643 – 1671 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 459,
      "chunk_index": 0
    }
  },
  {
    "text": "Loukides, Mike ( 2012 ). “What is Data Science?” O’Reilly, Sebastopol,\nCA.\n2003\nLusseau, D. ( ). The emergent properties of a dolphin social net-\nwork, Proceedings of the Royal Society of London B 271 S 6 : 477 – 481 .\n2013\nMayer-Schönberger, Viktor., and Kenneth Cukier ( ). “Big Data:\nA Revolution that will Transform How We Live, Work, and Think,”\nHoughton Mifflin Harcourt, New York.\n1996\nA. McCallum ( ). \"Bow: A toolkit for statistical lan-",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 459,
      "chunk_index": 1
    }
  },
  {
    "text": "guage modeling, text retrieval, classification and clustering,\"\nhttp://www.cs.cmu.edu/ mccallum/bow.\n∼\n2001\nMcPherson, M., Smith-Lovin, L., Cook, J. ( ). Birds of a feather: Ho-\nmophily in social networks, Annual Review of Sociology 27 , 415 - 444 .\n2003\nMezrich, Ben ( ). “Bringing Down the House: The Inside Story of\nSix MIT Students Who Took Vegas for Millions,” Free Press,\n1997\nMitchell, Tom ( ). “Machine Learning,” McGraw-Hill.\n2008",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 459,
      "chunk_index": 2
    }
  },
  {
    "text": "2008\nL. Mitra., G. Mitra., and D. diBartolomeo ( ). “Equity Portfolio\nRisk (Volatility) Estimation using Market Information and Sentiment,”\nWorking paper, Brunel University.\n2005\nP. Morville ( ). “Ambient Findability,” O’Reilly Press, Sebastopol,\nCA.\nNeal, R.( 1996 ). “Bayesian Learning for Neural-Networks,” Lecture Notes\nin Statistics, v 118 , Springer-Verlag.\nNeher, D. V. ( 1999 ). Staged financing: An agency perspective, Review of\nEconomic Studies 66 , 255 - 274 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 459,
      "chunk_index": 3
    }
  },
  {
    "text": "460 data science: theories, models, algorithms, and analytics\n2001\nNewman, M. ( ). Scientific collaboration networks: II. Shortest\npaths, weighted networks, and centrality, Physical Review E 64 , 016132 .\n2006\nNewman, M. ( ). Modularity and community structure in networks,\nProc. of the National Academy of Science 103 ( 23 ), 8577 - 8582 .\n2010\nNewman, M. ( ). Networks: An introduction, Oxford University\nPress.\n2002\nB. Pang., L. Lee., and S. Vaithyanathan ( ). “Thumbs Up? Sentiment",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 460,
      "chunk_index": 0
    }
  },
  {
    "text": "Classification using Machine Learning Techniques,” Proc. Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nPatil, D.J. ( 2012 ). “Data Jujitsu,” O’Reilly, Sebastopol, CA.\nPatil, D.J. ( 2011 ). “Building Data Science Teams,” O’Reilly, Sebastopol,\nCA.\n2006\nP. Pons, M. Latapy ( ). “Computing Communities in Large Net-\nworks Using Random Walks,” Journal of Graph Algorithms Applied, 10 ( 2 ),\n191 218\n- .\nM. Porter, ( 1980 ). “An Algorithm for Suffix Stripping,” Program 14 ( 3 ),",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 460,
      "chunk_index": 1
    }
  },
  {
    "text": "130 137\n? .\n2000\nPorter, M.E. ( ). Location, competition and economic development:\nLocal clusters in a global economy, Economic Development Quarterly\n14 1 15 34\n( ), - .\n2007\nPorter, Mason., Mucha, Peter, Newman, Mark, Friend, A. J. ( ).\nCommunity structure in the United States House of Representatives,\nPhysica A: Statistical Mechanics and its Applications 386 ( 1 ), 413 – 438 .\nRavasz, E., Somera, A.L., Mongru, D.A., Oltvai, N., Barabasi, A.L.\n2002",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 460,
      "chunk_index": 2
    }
  },
  {
    "text": "2002\n( ). Hierarchical organization of modularity in metabolic networks,\nScience 297 ( 5586 ), 1551 .\n2008\nRobinson, D. ( ). Strategic alliances and the boundaries of the firm,\nReview of Financial Studies 21 ( 2 ), 649 - 681 .\n2011 21\nRobinson, D., Sensoy, B. ( ). Private equity in the st century: cash\nflows, performance, and contract terms from 1984 - 2010 , Working Paper,\nOhio State University.\n2007\nRobinson, D., Stuart, T. ( ). Financial contracting in biotech strategic",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 460,
      "chunk_index": 3
    }
  },
  {
    "text": "alliances, Journal of Law and Economics 50 ( 3 ), 559 - 596 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 460,
      "chunk_index": 4
    }
  },
  {
    "text": "bibliography 461\nSeigel, Eric ( 2013 ). “Predictive Analytics,” John-Wiley & Sons, New Jer-\nsey.\n2007\nSegaran, T ( ). “Programming Collective Intelligence,” O’Reilly\nMedia Inc., California.\n1948\nShannon, Claude ( ). “A Mathematical Theory of Communication,”\nThe Bell System Technical Journal 27 , 379 – 423 .\nSimon, Herbert ( 1962 ). The architecture of complexity, Proceedings of the\nAmerican Philosophical Society 106 ( 6 ), 467 – 482 .\n1998",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 461,
      "chunk_index": 0
    }
  },
  {
    "text": "1998\nSmola, A.J., and Scholkopf, B ( ). “A Tutorial on Support Vector\n2\nRegression,” NeuroCOLT Technical Report, ESPIRIT Working Group\nin Neural and Computational Learning II.\n2007\nSorensen, Morten ( ). How smart is smart money? A Two-sided\nmatching model of venture capital, Journal of Finance 62 , 2725 - 2762 .\n2008\nSorensen, Morten ( ). Learning by investing: evidence from venture\ncapital, Columbia University Working Paper.\n1983",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 461,
      "chunk_index": 1
    }
  },
  {
    "text": "capital, Columbia University Working Paper.\n1983\nTarjan, Robert, E. ( ), “Data Structures and Network Algorithmsï£¡\nCBMS-NSF Regional Conference Series in Applied Mathematics.\n2007\nP. Tetlock ( ). “Giving Content to Investor Sentiment: The Role of\nMedia in the Stock Market,” Journal of Finance 62 ( 3 ), 1139 - 1168 .\n2008\nP. Tetlock, P. M. Saar-Tsechansky, and S. Macskassay ( ). “More than\nWords: Quantifying Language to Measure Firm’s Fundamentals,” Jour-\nnal of Finance 63 ( 3 ), 1437 - 1467 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 461,
      "chunk_index": 2
    }
  },
  {
    "text": "nal of Finance 63 ( 3 ), 1437 - 1467 .\n1962\nThorp, Ed. ( ). “Beat the Dealer,” Random House, New York.\n1997\nThorp, Ed ( ). “The Kelly Criterion in Blackjack, Sports Betting, and\nthe Stock Market,” Proc. of The 10th International Conference on Gambling\nand Risk Taking, Montreal, June.\n1963\nVapnik, V, and A. Lerner ( ). “Pattern Recognition using General-\nized Portrait Method,” Automation and Remote Control, v 24 .\n1964\nVapnik, V. and Chervonenkis ( ). “On the Uniform Convergence of",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 461,
      "chunk_index": 3
    }
  },
  {
    "text": "Relative Frequencies of Events to their Probabilities,” Theory of Probabil-\nity and its Applications, v 16 ( 2 ), 264 - 280 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 461,
      "chunk_index": 4
    }
  },
  {
    "text": "462 data science: theories, models, algorithms, and analytics\n1995\nVapnik, V ( ). The Nature of Statistical Learning Theory, Springer-\nVerlag, New York.\n1961\nVickrey, William ( ). “Counterspeculation, Auctions, and Competi-\ntive Sealed Tenders,” Journal of Finance 16 ( 1 ), 8 – 37 .\nXindong Wu, Vipin Kumar, J. Ross Quinlan, Joydeep Ghosh, Qiang\nYang, Hiroshi Motoda, Geoffrey J. McLachlan, Angus Ng, Bing Liu,\nPhilip S. Yu, Zhi-Hua Zhou, Michael Steinbach, David J. Hand and Dan",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 462,
      "chunk_index": 0
    }
  },
  {
    "text": "Steinberg, ( 2008 ). “Top 10 Algorithms in Data Mining,” Knowledge and\nInformation Systems 14 ( 1 ), 1 - 37 .\nWu, K., Taki, Y., Sato, K., Kinomura, S., Goto, R., Okada, K.,\n2011\nKawashima, R., He, Y., Evans, A. C. and Fukuda, H. ( ). Age-related\nchanges in topological organization of structural brain networks in\nhealthy individuals, Human Brain Mapping 32 , doi: 10 . 1002 /hbm. 21232 .\n1977\nZachary, Wayne ( ). An information flow model for conflict and",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 462,
      "chunk_index": 1
    }
  },
  {
    "text": "fission in small groups, Journal of Anthropological Research 33 , 452 – 473 .",
    "metadata": {
      "source": "./data\\DSA_Book.pdf",
      "file_name": "DSA_Book.pdf",
      "page_number": 462,
      "chunk_index": 2
    }
  }
]